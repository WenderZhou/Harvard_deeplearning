{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from resnets_utils import *\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ResNet50 import ResNet50\n",
    "from ResNet18 import ResNet18\n",
    "from CNN50 import CNN50\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# with the per-pixel mean substracted\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train -= X_train_mean\n",
    "X_test -= X_train_mean\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a learning rate scheduler to change the learning rate\n",
    "def my_schedule(epoch):\n",
    "    if epoch > 180:\n",
    "        learning_rate = 5e-7\n",
    "    elif epoch > 160:\n",
    "        learning_rate = 1e-6\n",
    "    elif epoch > 120:\n",
    "        learning_rate = 1e-5\n",
    "    elif epoch > 80:\n",
    "        learning_rate = 1e-4\n",
    "    else:\n",
    "        learning_rate = 1e-3\n",
    "    print('Learning rate: ', learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "scheduler = LearningRateScheduler(my_schedule)\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'Res_50_model.h5' \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "model1 = ResNet50(input_shape = (64, 64, 3), classes = 6)\n",
    "model1.compile(optimizer=optimizers.Adam(learning_rate=my_schedule(0)), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080 samples, validate on 120 samples\n",
      "Learning rate:  0.001\n",
      "Epoch 1/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 2.1208 - accuracy: 0.3672\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.16667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 11s 10ms/sample - loss: 2.0667 - accuracy: 0.3806 - val_loss: 7586.3975 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 2/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7297 - accuracy: 0.7812\n",
      "Epoch 00002: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.7207 - accuracy: 0.7824 - val_loss: 107.8063 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 3/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7553 - accuracy: 0.8277\n",
      "Epoch 00003: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.7677 - accuracy: 0.8269 - val_loss: 21.8235 - val_accuracy: 0.1500\n",
      "Learning rate:  0.001\n",
      "Epoch 4/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5329 - accuracy: 0.8769\n",
      "Epoch 00004: val_accuracy improved from 0.16667 to 0.18333, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.5302 - accuracy: 0.8759 - val_loss: 32.7546 - val_accuracy: 0.1833\n",
      "Learning rate:  0.001\n",
      "Epoch 5/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.8220\n",
      "Epoch 00005: val_accuracy did not improve from 0.18333\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.5983 - accuracy: 0.8213 - val_loss: 99.7791 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 6/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9299 ETA: 1s - loss:\n",
      "Epoch 00006: val_accuracy improved from 0.18333 to 0.34167, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.2036 - accuracy: 0.9296 - val_loss: 7.9614 - val_accuracy: 0.3417\n",
      "Learning rate:  0.001\n",
      "Epoch 7/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9479\n",
      "Epoch 00007: val_accuracy improved from 0.34167 to 0.55000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.1478 - accuracy: 0.9491 - val_loss: 2.1003 - val_accuracy: 0.5500\n",
      "Learning rate:  0.001\n",
      "Epoch 8/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.9725\n",
      "Epoch 00008: val_accuracy improved from 0.55000 to 0.65000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0690 - accuracy: 0.9731 - val_loss: 2.0008 - val_accuracy: 0.6500\n",
      "Learning rate:  0.001\n",
      "Epoch 9/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9792\n",
      "Epoch 00009: val_accuracy improved from 0.65000 to 0.80000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0704 - accuracy: 0.9796 - val_loss: 0.9795 - val_accuracy: 0.8000\n",
      "Learning rate:  0.001\n",
      "Epoch 10/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 0.9877\n",
      "Epoch 00010: val_accuracy improved from 0.80000 to 0.85833, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0406 - accuracy: 0.9880 - val_loss: 0.4412 - val_accuracy: 0.8583\n",
      "Learning rate:  0.001\n",
      "Epoch 11/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0242 - accuracy: 0.9943\n",
      "Epoch 00011: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.0237 - accuracy: 0.9944 - val_loss: 0.4999 - val_accuracy: 0.8500\n",
      "Learning rate:  0.001\n",
      "Epoch 12/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9924\n",
      "Epoch 00012: val_accuracy improved from 0.85833 to 0.90000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0220 - accuracy: 0.9926 - val_loss: 0.4284 - val_accuracy: 0.9000\n",
      "Learning rate:  0.001\n",
      "Epoch 13/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9943\n",
      "Epoch 00013: val_accuracy improved from 0.90000 to 0.91667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0151 - accuracy: 0.9944 - val_loss: 0.3063 - val_accuracy: 0.9167\n",
      "Learning rate:  0.001\n",
      "Epoch 14/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9877\n",
      "Epoch 00014: val_accuracy did not improve from 0.91667\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0537 - accuracy: 0.9880 - val_loss: 0.3343 - val_accuracy: 0.9000\n",
      "Learning rate:  0.001\n",
      "Epoch 15/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9640\n",
      "Epoch 00015: val_accuracy did not improve from 0.91667\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.1463 - accuracy: 0.9648 - val_loss: 1.0290 - val_accuracy: 0.7333\n",
      "Learning rate:  0.001\n",
      "Epoch 16/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9309\n",
      "Epoch 00016: val_accuracy did not improve from 0.91667\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.2155 - accuracy: 0.9324 - val_loss: 47.0399 - val_accuracy: 0.5083\n",
      "Learning rate:  0.001\n",
      "Epoch 17/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9830\n",
      "Epoch 00017: val_accuracy did not improve from 0.91667\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.0649 - accuracy: 0.9824 - val_loss: 1.8004 - val_accuracy: 0.8250\n",
      "Learning rate:  0.001\n",
      "Epoch 18/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 00018: val_accuracy improved from 0.91667 to 0.93333, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0269 - accuracy: 0.9907 - val_loss: 0.2551 - val_accuracy: 0.9333\n",
      "Learning rate:  0.001\n",
      "Epoch 19/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9943\n",
      "Epoch 00019: val_accuracy improved from 0.93333 to 0.95833, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0124 - accuracy: 0.9944 - val_loss: 0.2246 - val_accuracy: 0.9583\n",
      "Learning rate:  0.001\n",
      "Epoch 20/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9972\n",
      "Epoch 00020: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 0.0165 - accuracy: 0.9972 - val_loss: 0.2743 - val_accuracy: 0.9250\n",
      "Learning rate:  0.001\n",
      "Epoch 21/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9953\n",
      "Epoch 00021: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.3357 - val_accuracy: 0.8917\n",
      "Learning rate:  0.001\n",
      "Epoch 22/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9972\n",
      "Epoch 00022: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.2177 - val_accuracy: 0.9417\n",
      "Learning rate:  0.001\n",
      "Epoch 23/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00023: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2001 - val_accuracy: 0.9417\n",
      "Learning rate:  0.001\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00024: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9417\n",
      "Learning rate:  0.001\n",
      "Epoch 25/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9934\n",
      "Epoch 00025: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0347 - accuracy: 0.9935 - val_loss: 0.1728 - val_accuracy: 0.9417\n",
      "Learning rate:  0.001\n",
      "Epoch 26/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0894 - accuracy: 0.9792\n",
      "Epoch 00026: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0909 - accuracy: 0.9787 - val_loss: 13.6910 - val_accuracy: 0.4000\n",
      "Learning rate:  0.001\n",
      "Epoch 27/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8930\n",
      "Epoch 00027: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.3991 - accuracy: 0.8907 - val_loss: 67.5918 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 28/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9309\n",
      "Epoch 00028: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.2026 - accuracy: 0.9306 - val_loss: 3.9028 - val_accuracy: 0.6917\n",
      "Learning rate:  0.001\n",
      "Epoch 29/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.9735\n",
      "Epoch 00029: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0890 - accuracy: 0.9741 - val_loss: 0.1542 - val_accuracy: 0.9500\n",
      "Learning rate:  0.001\n",
      "Epoch 30/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9886\n",
      "Epoch 00030: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0372 - accuracy: 0.9889 - val_loss: 0.2093 - val_accuracy: 0.9583\n",
      "Learning rate:  0.001\n",
      "Epoch 31/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9934\n",
      "Epoch 00031: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0170 - accuracy: 0.9935 - val_loss: 0.3874 - val_accuracy: 0.9000\n",
      "Learning rate:  0.001\n",
      "Epoch 32/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9962\n",
      "Epoch 00032: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0184 - accuracy: 0.9963 - val_loss: 0.2766 - val_accuracy: 0.9500\n",
      "Learning rate:  0.001\n",
      "Epoch 33/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9924\n",
      "Epoch 00033: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.3086 - val_accuracy: 0.9417\n",
      "Learning rate:  0.001\n",
      "Epoch 34/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9962\n",
      "Epoch 00034: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.1004 - val_accuracy: 0.9583\n",
      "Learning rate:  0.001\n",
      "Epoch 35/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 0.0146 - accuracy: 0.9980\n",
      "Epoch 00035: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0146 - accuracy: 0.9981 - val_loss: 0.4560 - val_accuracy: 0.9000\n",
      "Learning rate:  0.001\n",
      "Epoch 36/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9924\n",
      "Epoch 00036: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0140 - accuracy: 0.9926 - val_loss: 0.3777 - val_accuracy: 0.9333\n",
      "Learning rate:  0.001\n",
      "Epoch 37/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00037: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.9583\n",
      "Learning rate:  0.001\n",
      "Epoch 38/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00038: val_accuracy improved from 0.95833 to 0.96667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9667\n",
      "Learning rate:  0.001\n",
      "Epoch 39/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.6836e-04 - accuracy: 1.0000\n",
      "Epoch 00039: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.0283e-04 - accuracy: 1.0000 - val_loss: 0.1768 - val_accuracy: 0.9667\n",
      "Learning rate:  0.001\n",
      "Epoch 40/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.6633e-04 - accuracy: 1.0000\n",
      "Epoch 00040: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 6.5208e-04 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9667\n",
      "Learning rate:  0.001\n",
      "Epoch 41/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 8.6498e-04 - accuracy: 1.0000\n",
      "Epoch 00041: val_accuracy improved from 0.96667 to 0.97500, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\Res_50_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 8.4883e-04 - accuracy: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 42/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0261e-04 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9902e-04 - accuracy: 1.0000 - val_loss: 0.2245 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 43/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.8537e-04 - accuracy: 1.0000\n",
      "Epoch 00043: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.8102e-04 - accuracy: 1.0000 - val_loss: 0.2119 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 44/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1820e-04 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.1685e-04 - accuracy: 1.0000 - val_loss: 0.2106 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 45/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.5245e-05 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 9.3680e-05 - accuracy: 1.0000 - val_loss: 0.2047 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 46/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.0778e-05 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 9.0679e-05 - accuracy: 1.0000 - val_loss: 0.2095 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 47/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6072e-04 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5817e-04 - accuracy: 1.0000 - val_loss: 0.2106 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 48/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.9878e-05 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 7.7005e-05 - accuracy: 1.0000 - val_loss: 0.2100 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 49/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0392e-04 - accuracy: 1.0000\n",
      "Epoch 00049: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.0241e-04 - accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 50/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.3707e-05 - accuracy: 1.0000\n",
      "Epoch 00050: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 9.1767e-05 - accuracy: 1.0000 - val_loss: 0.2042 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 51/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.0427e-05 - accuracy: 1.0000\n",
      "Epoch 00051: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.9707e-05 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 52/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 8.0626e-05 - accuracy: 1.0000\n",
      "Epoch 00052: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 7.9460e-05 - accuracy: 1.0000 - val_loss: 0.2032 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 53/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.0156e-05 - accuracy: 1.0000\n",
      "Epoch 00053: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 5.9037e-05 - accuracy: 1.0000 - val_loss: 0.2017 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 54/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.6042e-05 - accuracy: 1.0000\n",
      "Epoch 00054: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.8433e-05 - accuracy: 1.0000 - val_loss: 0.2016 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 55/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 7.1004e-05 - accuracy: 1.0000\n",
      "Epoch 00055: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 7.0090e-05 - accuracy: 1.0000 - val_loss: 0.1976 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 56/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.9361e-05 - accuracy: 1.0000\n",
      "Epoch 00056: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.8367e-05 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 57/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.8500e-05 - accuracy: 1.0000\n",
      "Epoch 00057: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 6.7677e-05 - accuracy: 1.0000 - val_loss: 0.1969 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 58/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.1008e-05 - accuracy: 1.0000\n",
      "Epoch 00058: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 4.0155e-05 - accuracy: 1.0000 - val_loss: 0.1972 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 59/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.7338e-05 - accuracy: 1.0000\n",
      "Epoch 00059: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 6.6097e-05 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 60/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 7.6134e-05 - accuracy: 1.0000\n",
      "Epoch 00060: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 7.6116e-05 - accuracy: 1.0000 - val_loss: 0.1932 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 61/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4671e-04 - accuracy: 1.0000\n",
      "Epoch 00061: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.4703e-04 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 62/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.9239e-05 - accuracy: 1.0000\n",
      "Epoch 00062: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.6709e-05 - accuracy: 1.0000 - val_loss: 0.1776 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 63/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.5727e-05 - accuracy: 1.0000\n",
      "Epoch 00063: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.5114e-05 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 64/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 8.9136e-05 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 9.1909e-05 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 65/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.0064e-05 - accuracy: 1.0000\n",
      "Epoch 00065: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.9201e-05 - accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 66/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.4508e-05 - accuracy: 1.0000\n",
      "Epoch 00066: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 6.3387e-05 - accuracy: 1.0000 - val_loss: 0.1807 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 67/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.2169e-05 - accuracy: 1.0000\n",
      "Epoch 00067: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 5.1237e-05 - accuracy: 1.0000 - val_loss: 0.1843 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 68/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.1963e-05 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.2924e-05 - accuracy: 1.0000 - val_loss: 0.1829 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 69/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 7.1727e-05 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 7.3152e-05 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 70/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.1068e-05 - accuracy: 1.0000\n",
      "Epoch 00070: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.0274e-05 - accuracy: 1.0000 - val_loss: 0.1839 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 71/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8857e-05 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8640e-05 - accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 72/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.6463e-05 - accuracy: 1.0000\n",
      "Epoch 00072: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 4.6647e-05 - accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 73/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.1951e-05 - accuracy: 1.0000\n",
      "Epoch 00073: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.0891e-05 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 74/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.6731e-05 - accuracy: 1.0000\n",
      "Epoch 00074: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.8640e-05 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 75/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5573e-05 - accuracy: 1.0000\n",
      "Epoch 00075: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5323e-05 - accuracy: 1.0000 - val_loss: 0.1800 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 76/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5410e-05 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5783e-05 - accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 77/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1718e-05 - accuracy: 1.0000\n",
      "Epoch 00077: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.1344e-05 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 78/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8838e-05 - accuracy: 1.0000\n",
      "Epoch 00078: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8617e-05 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 79/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.2546e-05 - accuracy: 1.0000\n",
      "Epoch 00079: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.2325e-05 - accuracy: 1.0000 - val_loss: 0.1820 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 80/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5518e-05 - accuracy: 1.0000\n",
      "Epoch 00080: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5273e-05 - accuracy: 1.0000 - val_loss: 0.1820 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 81/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4705e-05 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.4276e-05 - accuracy: 1.0000 - val_loss: 0.1774 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 1.3794e-05 - accuracy: 1.0000\n",
      "Epoch 00082: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3333e-05 - accuracy: 1.0000 - val_loss: 0.1781 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9870e-05 - accuracy: 1.0000\n",
      "Epoch 00083: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9503e-05 - accuracy: 1.0000 - val_loss: 0.1783 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9956e-05 - accuracy: 1.0000\n",
      "Epoch 00084: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9645e-05 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.7533e-05 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 3.6799e-05 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5407e-05 - accuracy: 1.0000\n",
      "Epoch 00086: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.5445e-05 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5247e-05 - accuracy: 1.0000\n",
      "Epoch 00087: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.5506e-05 - accuracy: 1.0000 - val_loss: 0.1795 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1852e-05 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.1977e-05 - accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.7952e-05 - accuracy: 1.0000\n",
      "Epoch 00089: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.7362e-05 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1952e-05 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.2078e-05 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5058e-05 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.5183e-05 - accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4978e-05 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4699e-05 - accuracy: 1.0000 - val_loss: 0.1795 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.9992e-05 - accuracy: 1.0000\n",
      "Epoch 00093: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.9436e-05 - accuracy: 1.0000 - val_loss: 0.1796 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 6.0149e-05 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 6.1500e-05 - accuracy: 1.0000 - val_loss: 0.1806 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.7680e-05 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.7322e-05 - accuracy: 1.0000 - val_loss: 0.1804 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9274e-05 - accuracy: 1.0000\n",
      "Epoch 00096: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.8913e-05 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7591e-05 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7408e-05 - accuracy: 1.0000 - val_loss: 0.1812 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0704e-05 - accuracy: 1.0000 ETA: 0s - loss: 1.9145e-05 \n",
      "Epoch 00098: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0399e-05 - accuracy: 1.0000 - val_loss: 0.1811 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.1739e-05 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.2218e-05 - accuracy: 1.0000 - val_loss: 0.1816 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1199e-05 - accuracy: 1.0000\n",
      "Epoch 00100: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0905e-05 - accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.6481e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00101: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 5.6469e-05 - accuracy: 1.0000 - val_loss: 0.1842 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.3790e-05 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.3746e-05 - accuracy: 1.0000 - val_loss: 0.1841 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.5828e-05 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.6225e-05 - accuracy: 1.0000 - val_loss: 0.1846 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9692e-05 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9502e-05 - accuracy: 1.0000 - val_loss: 0.1844 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.2524e-05 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.3068e-05 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.4203e-05 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.3484e-05 - accuracy: 1.0000 - val_loss: 0.1838 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2872e-05 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8989e-05 - accuracy: 1.0000 - val_loss: 0.1838 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 1.5979e-05 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5497e-05 - accuracy: 1.0000 - val_loss: 0.1844 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3321e-05 - accuracy: 1.0000\n",
      "Epoch 00109: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3333e-05 - accuracy: 1.0000 - val_loss: 0.1842 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4170e-05 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.9278e-05 - accuracy: 1.0000 - val_loss: 0.1835 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3052e-05 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3231e-05 - accuracy: 1.0000 - val_loss: 0.1841 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.6746e-05 - accuracy: 1.0000\n",
      "Epoch 00112: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.6340e-05 - accuracy: 1.0000 - val_loss: 0.1839 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.7191e-05 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.6809e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2376e-05 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2883e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7648e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7330e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.3756e-05 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.3283e-05 - accuracy: 1.0000 - val_loss: 0.1873 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8510e-05 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8847e-05 - accuracy: 1.0000 - val_loss: 0.1865 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1056e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0775e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.6073e-04 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.5590e-04 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.1961e-05 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.1878e-05 - accuracy: 1.0000 - val_loss: 0.1871 - val_accuracy: 0.9750\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.4520e-05 - accuracy: 1.0000\n",
      "Epoch 00121: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 4.3712e-05 - accuracy: 1.0000 - val_loss: 0.1870 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.3905e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.3619e-05 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 2.3085e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.2606e-05 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6457e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6285e-05 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.4091e-06 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 9.3171e-06 - accuracy: 1.0000 - val_loss: 0.1865 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7506e-05 - accuracy: 1.0000 ETA: 1s - loss: 3.1\n",
      "Epoch 00126: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7225e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-05\n",
      "Epoch 127/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7722e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7392e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1085e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.1085e-05 - accuracy: 1.0000 - val_loss: 0.1868 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8238e-05 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9574e-05 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3963e-05 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3749e-05 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 00131: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.1882 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.5066e-05 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.4124e-05 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.2160e-05 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.1798e-05 - accuracy: 1.0000 - val_loss: 0.1891 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.1217e-05 - accuracy: 1.0000\n",
      "Epoch 00134: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.0791e-05 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.9595e-05 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.9362e-05 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.2727e-05 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.2289e-05 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6704e-05 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6402e-05 - accuracy: 1.0000 - val_loss: 0.1891 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1386e-05 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.1231e-05 - accuracy: 1.0000 - val_loss: 0.1883 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1977e-05 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.9614e-05 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7838e-05 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7631e-05 - accuracy: 1.0000 - val_loss: 0.1880 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0567e-05 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0366e-05 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6816e-05 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7218e-05 - accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1519e-05 - accuracy: 1.0000\n",
      "Epoch 00143: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1577e-05 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8345e-05 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.8065e-05 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9637e-05 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.9676e-05 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5062e-05 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6820e-05 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8273e-05 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8028e-05 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4401e-05 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4184e-05 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9515e-05 - accuracy: 1.0000\n",
      "Epoch 00149: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9331e-05 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3838e-05 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5253e-05 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.0374e-05 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.9440e-05 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 152/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.2740e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00152: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.2245e-04 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 153/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8990e-05 - accuracy: 1.0000\n",
      "Epoch 00153: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9071e-05 - accuracy: 1.0000 - val_loss: 0.1846 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 154/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3818e-05 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4242e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 155/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8804e-05 - accuracy: 1.0000\n",
      "Epoch 00155: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8431e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 156/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5926e-05 - accuracy: 1.0000\n",
      "Epoch 00156: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5771e-05 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 157/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.9786e-05 - accuracy: 1.0000\n",
      "Epoch 00157: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.9707e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 158/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1584e-05 - accuracy: 1.0000\n",
      "Epoch 00158: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1359e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 159/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5313e-05 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.4850e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 160/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2246e-05 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2055e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-05\n",
      "Epoch 161/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.2732e-05 - accuracy: 1.0000\n",
      "Epoch 00161: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.2228e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 162/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.6487e-05 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.6199e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 163/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1415e-05 - accuracy: 1.0000\n",
      "Epoch 00163: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.1088e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 164/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2447e-05 - accuracy: 1.0000\n",
      "Epoch 00164: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2268e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 165/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0734e-05 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.0882e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 166/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0751e-05 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0355e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 167/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0448e-05 - accuracy: 1.0000\n",
      "Epoch 00167: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0119e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 168/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3955e-05 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3690e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 169/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2703e-05 - accuracy: 1.0000\n",
      "Epoch 00169: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3577e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 170/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7975e-05 - accuracy: 1.0000\n",
      "Epoch 00170: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8057e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 171/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0828e-05 - accuracy: 1.0000\n",
      "Epoch 00171: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0403e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 172/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.3162e-05 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.6992e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 173/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3613e-05 - accuracy: 1.0000\n",
      "Epoch 00173: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3432e-05 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 174/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9113e-05 - accuracy: 1.0000\n",
      "Epoch 00174: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8832e-05 - accuracy: 1.0000 - val_loss: 0.1846 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 175/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0362e-05 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.1927e-05 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 176/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 8.1497e-05 - accuracy: 1.0000\n",
      "Epoch 00176: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 8.1349e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 177/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5427e-05 - accuracy: 1.0000\n",
      "Epoch 00177: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.6948e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-06\n",
      "Epoch 178/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1353e-05 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.5815e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 179/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4730e-05 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4607e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 180/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2692e-05 - accuracy: 1.0000\n",
      "Epoch 00180: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2540e-05 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9750\n",
      "Learning rate:  1e-06\n",
      "Epoch 181/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.6999e-05 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.6497e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5986e-05 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6371e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4019e-05 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4741e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6099e-05 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7257e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0639e-05 - accuracy: 1.0000\n",
      "Epoch 00185: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.0643e-05 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9631e-05 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9265e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.2103e-05 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.1046e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5160e-05 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5306e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5035e-05 - accuracy: 1.0000\n",
      "Epoch 00189: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5657e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4410e-05 - accuracy: 1.0000\n",
      "Epoch 00190: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.4170e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.8782e-05 - accuracy: 1.0000\n",
      "Epoch 00191: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.7967e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4738e-05 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7896e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1064e-05 - accuracy: 1.0000\n",
      "Epoch 00193: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.6110e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6825e-05 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6621e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.8487e-06 - accuracy: 1.0000\n",
      "Epoch 00195: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1320e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.0509e-05 - accuracy: 1.0000\n",
      "Epoch 00196: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1182e-04 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4676e-05 - accuracy: 1.0000\n",
      "Epoch 00197: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5071e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6016e-05 - accuracy: 1.0000\n",
      "Epoch 00198: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5766e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5112e-05 - accuracy: 1.0000\n",
      "Epoch 00199: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.4895e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.3027e-05 - accuracy: 1.0000\n",
      "Epoch 00200: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.2698e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 201/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6958e-05 - accuracy: 1.0000\n",
      "Epoch 00201: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.6625e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 202/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 5.5751e-05 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 5.7418e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 203/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5187e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00203: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6010e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 204/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.6826e-05 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.6928e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 205/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6278e-05 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5943e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 206/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6429e-05 - accuracy: 1.0000\n",
      "Epoch 00206: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6173e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 207/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9592e-05 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9592e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 208/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6148e-05 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5996e-05 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 209/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0358e-05 - accuracy: 1.0000\n",
      "Epoch 00209: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.0493e-05 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 210/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4491e-05 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.4066e-05 - accuracy: 1.0000 - val_loss: 0.1865 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 211/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 1.2527e-05 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.3190e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 212/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.3995e-05 - accuracy: 1.0000\n",
      "Epoch 00212: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.7099e-05 - accuracy: 1.0000 - val_loss: 0.1865 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 213/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0623e-05 - accuracy: 1.0000\n",
      "Epoch 00213: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.0616e-05 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 214/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0298e-05 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.0120e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 215/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6432e-05 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6199e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 216/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4827e-05 - accuracy: 1.0000\n",
      "Epoch 00216: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5077e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 217/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3809e-05 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.3624e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 218/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0337e-05 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.0218e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 219/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9487e-05 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.9375e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 220/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7260e-05 - accuracy: 1.0000\n",
      "Epoch 00220: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7132e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 221/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5994e-05 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5700e-05 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 222/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0081e-05 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.0083e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 223/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7198e-05 - accuracy: 1.0000\n",
      "Epoch 00223: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.6982e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 224/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0900e-05 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.0681e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 225/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0107e-05 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.9765e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 226/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1287e-05 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.1048e-05 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 227/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.8993e-05 - accuracy: 1.0000\n",
      "Epoch 00227: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.8237e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 228/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 7.1590e-05 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 7.0396e-05 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 229/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8983e-05 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.8607e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 230/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3838e-05 - accuracy: 1.0000\n",
      "Epoch 00230: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.3709e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 231/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.6766e-05 - accuracy: 1.0000\n",
      "Epoch 00231: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.6179e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 232/300\n",
      "1024/1080 [===========================>..] - ETA: 0s - loss: 1.5382e-05 - accuracy: 1.0000\n",
      "Epoch 00232: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4960e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 233/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0575e-05 - accuracy: 1.0000\n",
      "Epoch 00233: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.1054e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 234/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1539e-05 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.1358e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 235/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0511e-05 - accuracy: 1.0000\n",
      "Epoch 00235: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.0338e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 236/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7886e-05 - accuracy: 1.0000\n",
      "Epoch 00236: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7676e-05 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 237/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9185e-05 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9065e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 238/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5102e-05 - accuracy: 1.0000\n",
      "Epoch 00238: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5263e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 239/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5679e-04 - accuracy: 1.0000\n",
      "Epoch 00239: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.5142e-04 - accuracy: 1.0000 - val_loss: 0.1843 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 240/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1044e-05 - accuracy: 1.0000\n",
      "Epoch 00240: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.1228e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 241/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.3930e-05 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.3657e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 242/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3171e-05 - accuracy: 1.0000\n",
      "Epoch 00242: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.8272e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 243/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6018e-05 - accuracy: 1.0000\n",
      "Epoch 00243: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.5710e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 244/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4057e-05 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.4388e-05 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 245/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6131e-05 - accuracy: 1.0000\n",
      "Epoch 00245: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7250e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 246/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4847e-05 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.4643e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 247/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.6139e-05 - accuracy: 1.0000\n",
      "Epoch 00247: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.5739e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 248/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.9282e-05 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 3.8546e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 249/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6377e-05 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.6902e-05 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 250/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5905e-05 - accuracy: 1.0000\n",
      "Epoch 00250: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.5494e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 251/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2830e-04 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.2552e-04 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 252/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8801e-05 - accuracy: 1.0000\n",
      "Epoch 00252: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.8498e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 253/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9003e-05 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.8689e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 254/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4516e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00254: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.4040e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 255/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0959e-05 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.1084e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 256/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4255e-05 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.6160e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 257/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5700e-05 - accuracy: 1.0000\n",
      "Epoch 00257: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.6243e-05 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 258/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8124e-05 - accuracy: 1.0000\n",
      "Epoch 00258: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7841e-05 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 259/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.4099e-05 - accuracy: 1.0000\n",
      "Epoch 00259: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 9.3463e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 260/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2918e-05 - accuracy: 1.0000\n",
      "Epoch 00260: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.4127e-05 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 261/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7659e-05 - accuracy: 1.0000\n",
      "Epoch 00261: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7632e-05 - accuracy: 1.0000 - val_loss: 0.1844 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 262/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1777e-05 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.1576e-05 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 263/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.9947e-05 - accuracy: 1.0000\n",
      "Epoch 00263: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 4.6363e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 264/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7356e-05 - accuracy: 1.0000\n",
      "Epoch 00264: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.7034e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 265/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4995e-05 - accuracy: 1.0000\n",
      "Epoch 00265: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 2.4859e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 266/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.5872e-05 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6294e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 267/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4636e-05 - accuracy: 1.0000\n",
      "Epoch 00267: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.5142e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 268/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.3587e-05 - accuracy: 1.0000\n",
      "Epoch 00268: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.3013e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 269/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8216e-05 - accuracy: 1.0000\n",
      "Epoch 00269: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0807e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 270/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4690e-05 - accuracy: 1.0000\n",
      "Epoch 00270: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.4902e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 271/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7095e-05 - accuracy: 1.0000\n",
      "Epoch 00271: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.9089e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 272/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4442e-05 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4217e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 273/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0893e-05 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.0716e-05 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 274/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.4500e-05 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.4267e-05 - accuracy: 1.0000 - val_loss: 0.1867 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 275/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.5088e-05 - accuracy: 1.0000\n",
      "Epoch 00275: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.4429e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 276/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4156e-05 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4016e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 277/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2005e-05 - accuracy: 1.0000\n",
      "Epoch 00277: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1884e-05 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 278/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5772e-05 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.6504e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 279/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2919e-05 - accuracy: 1.0000\n",
      "Epoch 00279: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2712e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 280/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1986e-05 - accuracy: 1.0000\n",
      "Epoch 00280: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1907e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 281/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.4909e-06 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 9.4249e-06 - accuracy: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 282/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.4833e-05 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.4125e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 283/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.3513e-05 - accuracy: 1.0000\n",
      "Epoch 00283: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.2582e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 284/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 4.1093e-05 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 4.7471e-05 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 285/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5461e-05 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.5602e-05 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 286/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2654e-05 - accuracy: 1.0000\n",
      "Epoch 00286: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2394e-05 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 287/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.1918e-05 - accuracy: 1.0000\n",
      "Epoch 00287: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.3216e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 288/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1328e-05 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1474e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 289/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5114e-05 - accuracy: 1.0000\n",
      "Epoch 00289: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.6988e-05 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 290/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.5185e-05 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.4894e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 291/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.2506e-05 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 3.1856e-05 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 292/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 3.0115e-05 - accuracy: 1.0000\n",
      "Epoch 00292: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.9622e-05 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 293/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8550e-05 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 1ms/sample - loss: 1.8717e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 294/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3095e-05 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.2861e-05 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 295/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6601e-05 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7496e-05 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 296/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.9179e-05 - accuracy: 1.0000\n",
      "Epoch 00296: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 2.3528e-05 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 297/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6428e-05 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.6230e-05 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 298/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.7127e-05 - accuracy: 1.0000\n",
      "Epoch 00298: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.7908e-05 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 299/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4710e-05 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.4441e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9750\n",
      "Learning rate:  5e-07\n",
      "Epoch 300/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 9.3492e-06 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 0.97500\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 9.9242e-06 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, Y_train, validation_data = (X_test,Y_test), epochs = 300, batch_size = 32, callbacks=[scheduler, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc1 = history1.history['accuracy']\n",
    "val_acc1 = history1.history['val_accuracy']\n",
    "\n",
    "fig1,ax1 = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "ax1.plot(acc1, label='Training Accuracy')\n",
    "ax1.plot(val_acc1, label='Validation Accuracy')\n",
    "\n",
    "ax1.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax1.set_ylabel(r'Acc', fontsize=20)\n",
    "ax1.set_title('ResNet50', fontsize=24)\n",
    "ax1.tick_params(labelsize=20)\n",
    "\n",
    "ax1.legend(loc=4, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'Res_18_model.h5' \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "model2 = ResNet18(input_shape = (64, 64, 3), classes = 6)\n",
    "model2.compile(optimizer=optimizers.Adam(learning_rate=my_schedule(0)), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080 samples, validate on 120 samples\n",
      "Learning rate:  0.001\n",
      "Epoch 1/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0661 - accuracy: 0.6310\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.19167, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 3s 3ms/sample - loss: 1.0212 - accuracy: 0.6463 - val_loss: 2.7318 - val_accuracy: 0.1917\n",
      "Learning rate:  0.001\n",
      "Epoch 2/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.2528 - accuracy: 0.9073\n",
      "Epoch 00002: val_accuracy did not improve from 0.19167\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 0.2548 - accuracy: 0.9083 - val_loss: 11.7258 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 3/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.1881 - accuracy: 0.9405\n",
      "Epoch 00003: val_accuracy did not improve from 0.19167\n",
      "1080/1080 [==============================] - 1s 696us/sample - loss: 0.1874 - accuracy: 0.9398 - val_loss: 15.1838 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 4/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.1576 - accuracy: 0.9446\n",
      "Epoch 00004: val_accuracy did not improve from 0.19167\n",
      "1080/1080 [==============================] - 1s 696us/sample - loss: 0.1583 - accuracy: 0.9444 - val_loss: 7.8253 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 5/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.1135 - accuracy: 0.9607\n",
      "Epoch 00005: val_accuracy improved from 0.19167 to 0.20000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 957us/sample - loss: 0.1089 - accuracy: 0.9611 - val_loss: 5.0502 - val_accuracy: 0.2000\n",
      "Learning rate:  0.001\n",
      "Epoch 6/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 0.9859\n",
      "Epoch 00006: val_accuracy did not improve from 0.20000\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 0.0393 - accuracy: 0.9852 - val_loss: 17.7583 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 7/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0492 - accuracy: 0.9829\n",
      "Epoch 00007: val_accuracy did not improve from 0.20000\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 0.0469 - accuracy: 0.9843 - val_loss: 13.6046 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 8/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0235 - accuracy: 0.9950\n",
      "Epoch 00008: val_accuracy did not improve from 0.20000\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 0.0240 - accuracy: 0.9935 - val_loss: 10.5426 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 9/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0259 - accuracy: 0.9940\n",
      "Epoch 00009: val_accuracy improved from 0.20000 to 0.21667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 974us/sample - loss: 0.0249 - accuracy: 0.9944 - val_loss: 7.0552 - val_accuracy: 0.2167\n",
      "Learning rate:  0.001\n",
      "Epoch 10/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0757 - accuracy: 0.9819\n",
      "Epoch 00010: val_accuracy improved from 0.21667 to 0.27500, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 965us/sample - loss: 0.0827 - accuracy: 0.9796 - val_loss: 8.8766 - val_accuracy: 0.2750\n",
      "Learning rate:  0.001\n",
      "Epoch 11/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.1770 - accuracy: 0.9466\n",
      "Epoch 00011: val_accuracy improved from 0.27500 to 0.32500, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 969us/sample - loss: 0.1881 - accuracy: 0.9463 - val_loss: 5.6518 - val_accuracy: 0.3250\n",
      "Learning rate:  0.001\n",
      "Epoch 12/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.1024 - accuracy: 0.9587\n",
      "Epoch 00012: val_accuracy improved from 0.32500 to 0.59167, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 956us/sample - loss: 0.1050 - accuracy: 0.9565 - val_loss: 2.4983 - val_accuracy: 0.5917\n",
      "Learning rate:  0.001\n",
      "Epoch 13/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0359 - accuracy: 0.9899\n",
      "Epoch 00013: val_accuracy did not improve from 0.59167\n",
      "1080/1080 [==============================] - 1s 717us/sample - loss: 0.0332 - accuracy: 0.9907 - val_loss: 2.8441 - val_accuracy: 0.4750\n",
      "Learning rate:  0.001\n",
      "Epoch 14/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0055 - accuracy: 0.9980\n",
      "Epoch 00014: val_accuracy improved from 0.59167 to 0.74167, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 967us/sample - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.8567 - val_accuracy: 0.7417\n",
      "Learning rate:  0.001\n",
      "Epoch 15/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 00015: val_accuracy improved from 0.74167 to 0.92500, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 952us/sample - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.2668 - val_accuracy: 0.9250\n",
      "Learning rate:  0.001\n",
      "Epoch 16/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0087 - accuracy: 0.9980\n",
      "Epoch 00016: val_accuracy improved from 0.92500 to 0.97500, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 983us/sample - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.1292 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 17/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 00017: val_accuracy improved from 0.97500 to 0.98333, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\ResNet_18_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 1s 956us/sample - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0659 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 18/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0027 - accuracy: 0.9990\n",
      "Epoch 00018: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0853 - val_accuracy: 0.9750\n",
      "Learning rate:  0.001\n",
      "Epoch 19/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 0.0019 - accuracy: 0.9990\n",
      "Epoch 00019: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 0.0019 - accuracy: 0.9991 - val_loss: 0.0846 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 20/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.8902e-04 - accuracy: 1.0000\n",
      "Epoch 00020: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 717us/sample - loss: 4.1150e-04 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 21/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.4541e-04 - accuracy: 1.0000\n",
      "Epoch 00021: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 2.3400e-04 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 22/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.1218e-04 - accuracy: 1.0000\n",
      "Epoch 00022: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 2.0881e-04 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 23/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.5523e-04 - accuracy: 1.0000\n",
      "Epoch 00023: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 1.6249e-04 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 24/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.5968e-04 - accuracy: 1.0000\n",
      "Epoch 00024: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.5706e-04 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 25/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4827e-04 - accuracy: 1.0000\n",
      "Epoch 00025: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 696us/sample - loss: 1.5155e-04 - accuracy: 1.0000 - val_loss: 0.0806 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 26/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3463e-04 - accuracy: 1.0000\n",
      "Epoch 00026: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 699us/sample - loss: 1.4736e-04 - accuracy: 1.0000 - val_loss: 0.0801 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 27/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.4291e-04 - accuracy: 1.0000\n",
      "Epoch 00027: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 2.3835e-04 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 28/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4456e-05 - accuracy: 1.0000\n",
      "Epoch 00028: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 8.2689e-05 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 29/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.5047e-05 - accuracy: 1.0000\n",
      "Epoch 00029: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 9.2177e-05 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 30/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2413e-04 - accuracy: 1.0000\n",
      "Epoch 00030: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 1.1831e-04 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 31/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0133e-04 - accuracy: 1.0000\n",
      "Epoch 00031: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 1.0002e-04 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 32/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.0574e-05 - accuracy: 1.0000\n",
      "Epoch 00032: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 9.0956e-05 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 33/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.2523e-05 - accuracy: 1.0000\n",
      "Epoch 00033: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 6.1189e-05 - accuracy: 1.0000 - val_loss: 0.0806 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 34/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0696e-04 - accuracy: 1.0000\n",
      "Epoch 00034: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 1.0017e-04 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 35/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.9455e-05 - accuracy: 1.0000\n",
      "Epoch 00035: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 4.7342e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 36/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.5239e-05 - accuracy: 1.0000\n",
      "Epoch 00036: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 5.9964e-05 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 37/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.1569e-05 - accuracy: 1.0000\n",
      "Epoch 00037: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 5.0770e-05 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 38/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.9522e-05 - accuracy: 1.0000\n",
      "Epoch 00038: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 5.7902e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 39/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.9681e-05 - accuracy: 1.0000\n",
      "Epoch 00039: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 4.6743e-05 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 40/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.2808e-05 - accuracy: 1.0000\n",
      "Epoch 00040: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 5.8795e-05 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 41/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.4453e-05 - accuracy: 1.0000\n",
      "Epoch 00041: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 719us/sample - loss: 4.2345e-05 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 42/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.4562e-05 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 6.0672e-05 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 43/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.9302e-05 - accuracy: 1.0000\n",
      "Epoch 00043: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 716us/sample - loss: 5.6244e-05 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 44/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.9743e-05 - accuracy: 1.0000\n",
      "Epoch 00044: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 717us/sample - loss: 4.6985e-05 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 45/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.8836e-05 - accuracy: 1.0000\n",
      "Epoch 00045: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 2.7924e-05 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 46/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.2436e-05 - accuracy: 1.0000\n",
      "Epoch 00046: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 4.0759e-05 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 47/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.0927e-05 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 3.0191e-05 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 48/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.3108e-05 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 2.3512e-05 - accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 49/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.7619e-05 - accuracy: 1.0000\n",
      "Epoch 00049: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 2.1574e-05 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 50/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.3722e-05 - accuracy: 1.0000\n",
      "Epoch 00050: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 3.5136e-05 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 51/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.7161e-05 - accuracy: 1.0000\n",
      "Epoch 00051: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 2.7035e-05 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 52/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.8137e-05 - accuracy: 1.0000\n",
      "Epoch 00052: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 3.7024e-05 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 53/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.7281e-05 - accuracy: 1.0000\n",
      "Epoch 00053: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 2.1267e-05 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 54/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.3235e-05 - accuracy: 1.0000\n",
      "Epoch 00054: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 2.2414e-05 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 55/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.2876e-05 - accuracy: 1.0000\n",
      "Epoch 00055: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 3.2094e-05 - accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 56/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.1662e-05 - accuracy: 1.0000\n",
      "Epoch 00056: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 697us/sample - loss: 3.1051e-05 - accuracy: 1.0000 - val_loss: 0.0878 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 57/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.3385e-05 - accuracy: 1.0000\n",
      "Epoch 00057: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 3.1374e-05 - accuracy: 1.0000 - val_loss: 0.0881 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 58/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.8686e-05 - accuracy: 1.0000\n",
      "Epoch 00058: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 2.9146e-05 - accuracy: 1.0000 - val_loss: 0.0884 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 59/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.2344e-05 - accuracy: 1.0000\n",
      "Epoch 00059: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 5.8734e-05 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 60/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 3.0616e-05 - accuracy: 1.0000\n",
      "Epoch 00060: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 3.3460e-05 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 61/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.0903e-05 - accuracy: 1.0000\n",
      "Epoch 00061: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 2.0645e-05 - accuracy: 1.0000 - val_loss: 0.0880 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 62/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.7162e-05 - accuracy: 1.0000\n",
      "Epoch 00062: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 1.8670e-05 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 63/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6914e-05 - accuracy: 1.0000\n",
      "Epoch 00063: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 1.6077e-05 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 64/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.3692e-05 - accuracy: 1.0000\n",
      "Epoch 00064: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 2.2273e-05 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 65/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4610e-05 - accuracy: 1.0000\n",
      "Epoch 00065: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 1.4176e-05 - accuracy: 1.0000 - val_loss: 0.0922 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 66/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.9746e-05 - accuracy: 1.0000\n",
      "Epoch 00066: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 2.1037e-05 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 67/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6965e-05 - accuracy: 1.0000\n",
      "Epoch 00067: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 1.7257e-05 - accuracy: 1.0000 - val_loss: 0.0942 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 68/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6407e-05 - accuracy: 1.0000\n",
      "Epoch 00068: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 1.6658e-05 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 69/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2621e-05 - accuracy: 1.0000\n",
      "Epoch 00069: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 1.6589e-05 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 70/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2401e-05 - accuracy: 1.0000\n",
      "Epoch 00070: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 698us/sample - loss: 1.2705e-05 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 71/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2926e-05 - accuracy: 1.0000\n",
      "Epoch 00071: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.4112e-05 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 72/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0448e-05 - accuracy: 1.0000\n",
      "Epoch 00072: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 1.0515e-05 - accuracy: 1.0000 - val_loss: 0.0943 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 73/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.4281e-05 - accuracy: 1.0000\n",
      "Epoch 00073: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 2.4007e-05 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 74/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4782e-05 - accuracy: 1.0000\n",
      "Epoch 00074: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 1.5069e-05 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 75/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3078e-05 - accuracy: 1.0000\n",
      "Epoch 00075: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.2623e-05 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 76/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.2946e-06 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 8.9912e-06 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 77/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.3992e-06 - accuracy: 1.0000\n",
      "Epoch 00077: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 9.2641e-06 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 78/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6005e-05 - accuracy: 1.0000\n",
      "Epoch 00078: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 1.5582e-05 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 79/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.3018e-05 - accuracy: 1.0000\n",
      "Epoch 00079: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 2.2281e-05 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 80/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2071e-05 - accuracy: 1.0000\n",
      "Epoch 00080: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 1.1772e-05 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9833\n",
      "Learning rate:  0.001\n",
      "Epoch 81/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3397e-05 - accuracy: 1.0000\n",
      "Epoch 00081: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 1.2691e-05 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.9938e-06 - accuracy: 1.0000\n",
      "Epoch 00082: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 8.9981e-06 - accuracy: 1.0000 - val_loss: 0.0972 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1194e-05 - accuracy: 1.0000\n",
      "Epoch 00083: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.2248e-05 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2467e-05 - accuracy: 1.0000\n",
      "Epoch 00084: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.5931e-05 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.1828e-06 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 8.0002e-06 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.7437e-06 - accuracy: 1.0000\n",
      "Epoch 00086: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 9.4929e-06 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1037e-05 - accuracy: 1.0000\n",
      "Epoch 00087: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 1.0830e-05 - accuracy: 1.0000 - val_loss: 0.0972 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0009e-05 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 699us/sample - loss: 9.3796e-06 - accuracy: 1.0000 - val_loss: 0.0971 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.4014e-06 - accuracy: 1.0000\n",
      "Epoch 00089: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 9.8646e-06 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.9467e-06 - accuracy: 1.0000\n",
      "Epoch 00090: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 9.0058e-06 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1393e-05 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 1.1330e-05 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.3205e-06 - accuracy: 1.0000\n",
      "Epoch 00092: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 7.1339e-06 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2099e-05 - accuracy: 1.0000\n",
      "Epoch 00093: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.2491e-05 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1541e-05 - accuracy: 1.0000\n",
      "Epoch 00094: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.1018e-05 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.4182e-06 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 9.3179e-06 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1122e-05 - accuracy: 1.0000\n",
      "Epoch 00096: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.2792e-05 - accuracy: 1.0000 - val_loss: 0.0972 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.8095e-06 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 1.1427e-05 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.8082e-06 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 7.5059e-06 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.1216e-06 - accuracy: 1.0000\n",
      "Epoch 00099: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 9.1071e-06 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.3862e-06 - accuracy: 1.0000\n",
      "Epoch 00100: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 7.7484e-06 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.7795e-06 - accuracy: 1.0000\n",
      "Epoch 00101: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 8.3903e-06 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.8672e-06 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 8.6285e-06 - accuracy: 1.0000 - val_loss: 0.0979 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0815e-05 - accuracy: 1.0000\n",
      "Epoch 00103: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 699us/sample - loss: 1.0578e-05 - accuracy: 1.0000 - val_loss: 0.0979 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.9056e-06 - accuracy: 1.0000\n",
      "Epoch 00104: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 697us/sample - loss: 6.9281e-06 - accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.5637e-06 - accuracy: 1.0000\n",
      "Epoch 00105: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 8.5882e-06 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.9772e-05 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 1.8950e-05 - accuracy: 1.0000 - val_loss: 0.0981 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0378e-05 - accuracy: 1.0000\n",
      "Epoch 00107: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 1.4602e-05 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0544e-05 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 1.0069e-05 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5928e-06 - accuracy: 1.0000\n",
      "Epoch 00109: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 720us/sample - loss: 8.4019e-06 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5004e-06 - accuracy: 1.0000\n",
      "Epoch 00110: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 8.0579e-06 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0388e-05 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 9.8891e-06 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.1148e-06 - accuracy: 1.0000\n",
      "Epoch 00112: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 720us/sample - loss: 8.2522e-06 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.1275e-06 - accuracy: 1.0000\n",
      "Epoch 00113: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 726us/sample - loss: 7.7658e-06 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.5027e-06 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 722us/sample - loss: 1.0735e-05 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3214e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 724us/sample - loss: 1.3200e-05 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4810e-06 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 9.1470e-06 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.8692e-06 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 9.3473e-06 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1156e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 1.2005e-05 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.5840e-06 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 7.5016e-06 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.9878e-05 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.8513e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.8490e-06 - accuracy: 1.0000\n",
      "Epoch 00121: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 7.8474e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0474e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 719us/sample - loss: 1.0007e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3585e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 1.2798e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1433e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 1.1084e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.3098e-06 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 6.9820e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.6152e-06 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 8.1278e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-05\n",
      "Epoch 127/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0848e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 1.2619e-05 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2049e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 1.1335e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4438e-06 - accuracy: 1.0000\n",
      "Epoch 00129: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 8.0486e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.0026e-06 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.0840e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.8037e-06 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 8.3051e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.1712e-06 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 716us/sample - loss: 8.9771e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1375e-05 - accuracy: 1.0000\n",
      "Epoch 00133: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.0836e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.5791e-06 - accuracy: 1.0000\n",
      "Epoch 00134: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 7.5262e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3186e-05 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 1.2846e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.9613e-06 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 1.0552e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.2161e-06 - accuracy: 1.0000\n",
      "Epoch 00137: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 9.3057e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.9165e-06 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 8.9063e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1366e-05 - accuracy: 1.0000\n",
      "Epoch 00139: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 1.1182e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.8830e-05 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 720us/sample - loss: 1.8530e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5695e-06 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 8.1717e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.5024e-06 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 6.8990e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.4640e-06 - accuracy: 1.0000\n",
      "Epoch 00143: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 9.1989e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.5104e-05 - accuracy: 1.0000\n",
      "Epoch 00144: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.4711e-05 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.3411e-06 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 7.9984e-06 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.1550e-06 - accuracy: 1.0000\n",
      "Epoch 00146: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 716us/sample - loss: 7.1150e-06 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.3227e-06 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 1.0187e-05 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1504e-05 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 1.0712e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.7693e-06 - accuracy: 1.0000\n",
      "Epoch 00149: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 6.7031e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5456e-06 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 8.1759e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.6719e-06 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 6.8315e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 152/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.9854e-06 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 716us/sample - loss: 8.8292e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 153/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.2089e-06 - accuracy: 1.0000\n",
      "Epoch 00153: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 8.0460e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 154/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.5128e-05 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 720us/sample - loss: 1.5229e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 155/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.2089e-06 - accuracy: 1.0000\n",
      "Epoch 00155: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 699us/sample - loss: 8.3117e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 156/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1779e-05 - accuracy: 1.0000\n",
      "Epoch 00156: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 1.2959e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 157/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.4221e-06 - accuracy: 1.0000\n",
      "Epoch 00157: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 7.1040e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 158/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.3935e-06 - accuracy: 1.0000\n",
      "Epoch 00158: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 8.9400e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 159/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.7123e-06 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 6.9685e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 160/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.1244e-06 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 7.2738e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-05\n",
      "Epoch 161/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.8618e-05 - accuracy: 1.0000\n",
      "Epoch 00161: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 2.1841e-05 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 162/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3877e-05 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 1.3285e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 163/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.5250e-05 - accuracy: 1.0000\n",
      "Epoch 00163: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.6372e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 164/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.0414e-06 - accuracy: 1.0000\n",
      "Epoch 00164: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 8.5719e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 165/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.8053e-06 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 7.6323e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 166/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.4507e-06 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 1.1734e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 167/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.7888e-05 - accuracy: 1.0000\n",
      "Epoch 00167: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 1.7606e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 168/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.9163e-06 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 9.5829e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 169/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.0804e-06 - accuracy: 1.0000\n",
      "Epoch 00169: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 700us/sample - loss: 1.0216e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 170/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.0994e-06 - accuracy: 1.0000\n",
      "Epoch 00170: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 6.7953e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 171/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.8897e-06 - accuracy: 1.0000\n",
      "Epoch 00171: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 8.5000e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 172/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1772e-05 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 1.1697e-05 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 173/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.7963e-06 - accuracy: 1.0000\n",
      "Epoch 00173: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 7.4921e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 174/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.0963e-06 - accuracy: 1.0000\n",
      "Epoch 00174: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 6.4203e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 175/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4581e-06 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 8.3482e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 176/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4451e-05 - accuracy: 1.0000\n",
      "Epoch 00176: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 1.4016e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 177/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.2343e-06 - accuracy: 1.0000\n",
      "Epoch 00177: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 5.9133e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-06\n",
      "Epoch 178/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4204e-05 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 1.3749e-05 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 179/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.3020e-06 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 6.9976e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 180/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2920e-05 - accuracy: 1.0000\n",
      "Epoch 00180: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 1.2368e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  1e-06\n",
      "Epoch 181/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.0130e-06 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 9.3104e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.1733e-06 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 1.6899e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.8064e-06 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 6.4970e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.3205e-05 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 1.2764e-05 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.7803e-05 - accuracy: 1.0000\n",
      "Epoch 00185: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 703us/sample - loss: 1.6777e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.2841e-06 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 9.1563e-06 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.7810e-06 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 4.7607e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.6981e-06 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 1.1441e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.6774e-06 - accuracy: 1.0000\n",
      "Epoch 00189: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 7.8616e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.0978e-06 - accuracy: 1.0000\n",
      "Epoch 00190: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 737us/sample - loss: 8.2399e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.7985e-06 - accuracy: 1.0000\n",
      "Epoch 00191: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 7.5813e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.2804e-06 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 6.9375e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.0421e-06 - accuracy: 1.0000\n",
      "Epoch 00193: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 5.9336e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4530e-05 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 1.3851e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.8577e-06 - accuracy: 1.0000\n",
      "Epoch 00195: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 8.6522e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.9104e-06 - accuracy: 1.0000\n",
      "Epoch 00196: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 6.3308e-06 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2582e-05 - accuracy: 1.0000\n",
      "Epoch 00197: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 1.4040e-05 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.3515e-06 - accuracy: 1.0000\n",
      "Epoch 00198: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 716us/sample - loss: 8.4432e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.2562e-06 - accuracy: 1.0000\n",
      "Epoch 00199: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 8.9567e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.1022e-05 - accuracy: 1.0000\n",
      "Epoch 00200: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 1.9596e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 201/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4780e-06 - accuracy: 1.0000\n",
      "Epoch 00201: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 8.6029e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 202/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.0124e-06 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 8.9358e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 203/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6771e-05 - accuracy: 1.0000\n",
      "Epoch 00203: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 1.6502e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 204/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.5819e-06 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 699us/sample - loss: 9.1518e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 205/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.0390e-06 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 6.7788e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 206/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.2414e-06 - accuracy: 1.0000\n",
      "Epoch 00206: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 7.3931e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 207/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2954e-05 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 1.2229e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 208/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.4621e-06 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 725us/sample - loss: 7.5748e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 209/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.9654e-06 - accuracy: 1.0000\n",
      "Epoch 00209: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 6.8788e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 210/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.4780e-06 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 1.2177e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 211/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.4393e-06 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 720us/sample - loss: 6.3264e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 212/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.6791e-06 - accuracy: 1.0000\n",
      "Epoch 00212: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 1.1539e-05 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 213/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.8153e-06 - accuracy: 1.0000\n",
      "Epoch 00213: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 8.3961e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 214/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.5932e-06 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 9.4593e-06 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 215/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1031e-05 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 1.0992e-05 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 216/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.9680e-06 - accuracy: 1.0000\n",
      "Epoch 00216: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 6.9326e-06 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 217/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.5781e-06 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 6.5531e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 218/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1552e-05 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 2.7254e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 219/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.5552e-06 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 9.0370e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 220/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.9157e-06 - accuracy: 1.0000\n",
      "Epoch 00220: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 8.3568e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 221/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.9481e-06 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 720us/sample - loss: 7.1166e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 222/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.1684e-06 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 731us/sample - loss: 8.7350e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 223/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6195e-05 - accuracy: 1.0000\n",
      "Epoch 00223: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 725us/sample - loss: 1.5761e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 224/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.2876e-06 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 6.9884e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 225/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4747e-06 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 9.3489e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 226/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.0850e-05 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 1.9625e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 227/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.0942e-05 - accuracy: 1.0000\n",
      "Epoch 00227: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.9413e-05 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 228/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.4706e-06 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 7.2609e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 229/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.2632e-05 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 721us/sample - loss: 2.0967e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 230/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.4363e-06 - accuracy: 1.0000\n",
      "Epoch 00230: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 729us/sample - loss: 7.0393e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 231/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.2082e-06 - accuracy: 1.0000\n",
      "Epoch 00231: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 6.9070e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 232/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 4.7466e-06 - accuracy: 1.0000\n",
      "Epoch 00232: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 5.3108e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 233/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0091e-05 - accuracy: 1.0000\n",
      "Epoch 00233: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 9.6667e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 234/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.9336e-06 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 702us/sample - loss: 7.0070e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 235/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5687e-06 - accuracy: 1.0000\n",
      "Epoch 00235: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 8.2568e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 236/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.7335e-06 - accuracy: 1.0000\n",
      "Epoch 00236: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 8.5574e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 237/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.9074e-06 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 9.7622e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 238/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.1556e-06 - accuracy: 1.0000\n",
      "Epoch 00238: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 7.5097e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 239/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.7571e-06 - accuracy: 1.0000\n",
      "Epoch 00239: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 701us/sample - loss: 8.1918e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 240/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.8656e-05 - accuracy: 1.0000\n",
      "Epoch 00240: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 1.7317e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 241/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 2.1483e-05 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 714us/sample - loss: 2.0377e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 242/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.5689e-06 - accuracy: 1.0000\n",
      "Epoch 00242: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 724us/sample - loss: 7.8414e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 243/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0036e-05 - accuracy: 1.0000\n",
      "Epoch 00243: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 724us/sample - loss: 9.4145e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 244/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.2871e-06 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 5.9600e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 245/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.7172e-06 - accuracy: 1.0000\n",
      "Epoch 00245: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 716us/sample - loss: 6.6120e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 246/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4381e-06 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 719us/sample - loss: 7.9778e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 247/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.0570e-06 - accuracy: 1.0000\n",
      "Epoch 00247: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 717us/sample - loss: 7.6289e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 248/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1093e-05 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 1.1290e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 249/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.1238e-06 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 6.0944e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 250/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.6811e-06 - accuracy: 1.0000\n",
      "Epoch 00250: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 7.2730e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 251/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.0148e-06 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 7.1012e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 252/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0783e-05 - accuracy: 1.0000\n",
      "Epoch 00252: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 1.0703e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 253/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.0465e-06 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 7.7718e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 254/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.4763e-06 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 6.8592e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 255/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.7511e-06 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 8.7416e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 256/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4061e-06 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 8.3853e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 257/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.4066e-06 - accuracy: 1.0000\n",
      "Epoch 00257: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 8.7599e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 258/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.5438e-06 - accuracy: 1.0000\n",
      "Epoch 00258: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 6.0984e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 259/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.5585e-06 - accuracy: 1.0000\n",
      "Epoch 00259: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 6.4206e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 260/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.7929e-06 - accuracy: 1.0000\n",
      "Epoch 00260: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 7.4813e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 261/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.1384e-06 - accuracy: 1.0000\n",
      "Epoch 00261: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 5.0644e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 262/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5984e-06 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 8.7978e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 263/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.3941e-06 - accuracy: 1.0000\n",
      "Epoch 00263: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 8.1272e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 264/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.3590e-06 - accuracy: 1.0000\n",
      "Epoch 00264: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 707us/sample - loss: 8.2076e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 265/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1910e-05 - accuracy: 1.0000\n",
      "Epoch 00265: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 1.2087e-05 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 266/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.8118e-06 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 6.4784e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 267/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.4309e-06 - accuracy: 1.0000\n",
      "Epoch 00267: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 7.4162e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 268/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.4175e-05 - accuracy: 1.0000\n",
      "Epoch 00268: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 1.3398e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 269/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.1131e-06 - accuracy: 1.0000\n",
      "Epoch 00269: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 711us/sample - loss: 5.9956e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 270/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.9481e-06 - accuracy: 1.0000\n",
      "Epoch 00270: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 729us/sample - loss: 6.6668e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 271/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.8642e-06 - accuracy: 1.0000\n",
      "Epoch 00271: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 722us/sample - loss: 8.5818e-06 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 272/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.0830e-06 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 6.3041e-06 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 273/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.2376e-06 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 719us/sample - loss: 8.8813e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 274/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0825e-05 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 1.0414e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 275/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.7929e-06 - accuracy: 1.0000\n",
      "Epoch 00275: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 1.0605e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 276/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.8797e-06 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 5.8684e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 277/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.2527e-05 - accuracy: 1.0000\n",
      "Epoch 00277: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 1.1727e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 278/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0801e-05 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 719us/sample - loss: 1.0204e-05 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 279/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.4506e-06 - accuracy: 1.0000\n",
      "Epoch 00279: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 1.0945e-05 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 280/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.1622e-06 - accuracy: 1.0000\n",
      "Epoch 00280: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 718us/sample - loss: 8.7126e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 281/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.9501e-06 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 704us/sample - loss: 7.8648e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 282/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0190e-05 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 9.7120e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 283/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.9162e-06 - accuracy: 1.0000\n",
      "Epoch 00283: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 708us/sample - loss: 7.7317e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 284/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.8284e-06 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 8.4874e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 285/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.6137e-05 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 725us/sample - loss: 1.5367e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 286/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0420e-05 - accuracy: 1.0000\n",
      "Epoch 00286: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 715us/sample - loss: 1.0147e-05 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 287/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.1861e-06 - accuracy: 1.0000\n",
      "Epoch 00287: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 723us/sample - loss: 7.1495e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 288/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.0425e-05 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 724us/sample - loss: 1.0038e-05 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 289/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.1959e-05 - accuracy: 1.0000\n",
      "Epoch 00289: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 717us/sample - loss: 1.1637e-05 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 290/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.2426e-06 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 728us/sample - loss: 7.9100e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 291/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 1.5476e-05 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 1.4642e-05 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 292/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 5.1680e-06 - accuracy: 1.0000\n",
      "Epoch 00292: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 713us/sample - loss: 5.0572e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 293/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 8.5675e-06 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 717us/sample - loss: 9.3753e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 294/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 7.5067e-06 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 7.5434e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 295/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.5262e-06 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 709us/sample - loss: 9.6345e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 296/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.6641e-06 - accuracy: 1.0000\n",
      "Epoch 00296: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 710us/sample - loss: 9.6134e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 297/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.8475e-06 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 705us/sample - loss: 6.7236e-06 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 298/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.5675e-06 - accuracy: 1.0000\n",
      "Epoch 00298: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 706us/sample - loss: 6.3857e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 299/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 6.3300e-06 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 6.3670e-06 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9833\n",
      "Learning rate:  5e-07\n",
      "Epoch 300/300\n",
      " 992/1080 [==========================>...] - ETA: 0s - loss: 9.0183e-06 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 0.98333\n",
      "1080/1080 [==============================] - 1s 712us/sample - loss: 8.6053e-06 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train, Y_train, validation_data = (X_test,Y_test), epochs = 300, batch_size = 32, callbacks=[scheduler, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc2 = history2.history['accuracy']\n",
    "val_acc2 = history2.history['val_accuracy']\n",
    "\n",
    "fig2,ax2 = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "ax2.plot(acc1, label='Training Accuracy')\n",
    "ax2.plot(val_acc1, label='Validation Accuracy')\n",
    "\n",
    "ax2.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax2.set_ylabel(r'Acc', fontsize=20)\n",
    "ax2.set_title('ResNet18', fontsize=24)\n",
    "ax2.tick_params(labelsize=20)\n",
    "\n",
    "ax2.legend(loc=4, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'CNN_50_model.h5' \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "model3 = CNN50(input_shape = (64, 64, 3), classes = 6)\n",
    "model3.compile(optimizer=optimizers.Adam(learning_rate=my_schedule(0)), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080 samples, validate on 120 samples\n",
      "Learning rate:  0.001\n",
      "Epoch 1/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 2.0676 - accuracy: 0.1941\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.16667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 7s 6ms/sample - loss: 2.0620 - accuracy: 0.1935 - val_loss: 1.7961 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 2/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8242 - accuracy: 0.1903\n",
      "Epoch 00002: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.8270 - accuracy: 0.1917 - val_loss: 1.7946 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 3/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8027 - accuracy: 0.2188\n",
      "Epoch 00003: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.8012 - accuracy: 0.2204 - val_loss: 1.8183 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 4/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.8353 - accuracy: 0.2472\n",
      "Epoch 00004: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.8285 - accuracy: 0.2481 - val_loss: 1.8670 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 5/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.6383 - accuracy: 0.2718\n",
      "Epoch 00005: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.6417 - accuracy: 0.2667 - val_loss: 2.2352 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 6/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.4733 - accuracy: 0.3456\n",
      "Epoch 00006: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.4748 - accuracy: 0.3426 - val_loss: 2.5462 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 7/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.3106 - accuracy: 0.4148\n",
      "Epoch 00007: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.3080 - accuracy: 0.4167 - val_loss: 3.5817 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 8/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1906 - accuracy: 0.4801\n",
      "Epoch 00008: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.2037 - accuracy: 0.4759 - val_loss: 2.0124 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 9/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.2845 - accuracy: 0.4138\n",
      "Epoch 00009: val_accuracy did not improve from 0.16667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.2812 - accuracy: 0.4148 - val_loss: 7.4898 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 10/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.1656 - accuracy: 0.4612\n",
      "Epoch 00010: val_accuracy improved from 0.16667 to 0.25000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 1.1614 - accuracy: 0.4648 - val_loss: 2.2794 - val_accuracy: 0.2500\n",
      "Learning rate:  0.001\n",
      "Epoch 11/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0033 - accuracy: 0.5312\n",
      "Epoch 00011: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.0042 - accuracy: 0.5306 - val_loss: 2.9505 - val_accuracy: 0.2250\n",
      "Learning rate:  0.001\n",
      "Epoch 12/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 1.0106 - accuracy: 0.5398\n",
      "Epoch 00012: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 1.0163 - accuracy: 0.5407 - val_loss: 3.6231 - val_accuracy: 0.1417\n",
      "Learning rate:  0.001\n",
      "Epoch 13/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.9289 - accuracy: 0.6032\n",
      "Epoch 00013: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.9310 - accuracy: 0.6009 - val_loss: 7.1835 - val_accuracy: 0.1917\n",
      "Learning rate:  0.001\n",
      "Epoch 14/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.8837 - accuracy: 0.6165\n",
      "Epoch 00014: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.8827 - accuracy: 0.6167 - val_loss: 6.3234 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 15/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.8541 - accuracy: 0.6297\n",
      "Epoch 00015: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.8544 - accuracy: 0.6306 - val_loss: 5.3492 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 16/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.8442 - accuracy: 0.6638\n",
      "Epoch 00016: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.8430 - accuracy: 0.6639 - val_loss: 6.1295 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 17/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.8467 - accuracy: 0.6458\n",
      "Epoch 00017: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.8865 - accuracy: 0.6370 - val_loss: 15.8980 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 18/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.9003 - accuracy: 0.6335\n",
      "Epoch 00018: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.8983 - accuracy: 0.6324 - val_loss: 12.9154 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 19/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.8182 - accuracy: 0.6809\n",
      "Epoch 00019: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.8181 - accuracy: 0.6824 - val_loss: 56.4722 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 20/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7729 - accuracy: 0.7055\n",
      "Epoch 00020: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7676 - accuracy: 0.7083 - val_loss: 10.5214 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 21/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7378 - accuracy: 0.7064\n",
      "Epoch 00021: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7319 - accuracy: 0.7093 - val_loss: 12.8345 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 22/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7727 - accuracy: 0.7263\n",
      "Epoch 00022: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7731 - accuracy: 0.7259 - val_loss: 4.2370 - val_accuracy: 0.2000\n",
      "Learning rate:  0.001\n",
      "Epoch 23/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7737 - accuracy: 0.7083\n",
      "Epoch 00023: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7741 - accuracy: 0.7083 - val_loss: 7.3707 - val_accuracy: 0.1417\n",
      "Learning rate:  0.001\n",
      "Epoch 24/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7551 - accuracy: 0.7472\n",
      "Epoch 00024: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7514 - accuracy: 0.7472 - val_loss: 8.7232 - val_accuracy: 0.1833\n",
      "Learning rate:  0.001\n",
      "Epoch 25/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.6750 - accuracy: 0.7415\n",
      "Epoch 00025: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.6732 - accuracy: 0.7407 - val_loss: 10.7644 - val_accuracy: 0.1583\n",
      "Learning rate:  0.001\n",
      "Epoch 26/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5326 - accuracy: 0.8201\n",
      "Epoch 00026: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.5364 - accuracy: 0.8194 - val_loss: 11.0695 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 27/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7042 - accuracy: 0.7396\n",
      "Epoch 00027: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7114 - accuracy: 0.7370 - val_loss: 16.1964 - val_accuracy: 0.1667\n",
      "Learning rate:  0.001\n",
      "Epoch 28/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7838 - accuracy: 0.6970\n",
      "Epoch 00028: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7999 - accuracy: 0.7028 - val_loss: 9.2194 - val_accuracy: 0.1583\n",
      "Learning rate:  0.001\n",
      "Epoch 29/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.7860\n",
      "Epoch 00029: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7078 - accuracy: 0.7787 - val_loss: 34.7796 - val_accuracy: 0.2500\n",
      "Learning rate:  0.001\n",
      "Epoch 30/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.7809 - accuracy: 0.6922\n",
      "Epoch 00030: val_accuracy did not improve from 0.25000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.7915 - accuracy: 0.6833 - val_loss: 6.9784 - val_accuracy: 0.1917\n",
      "Learning rate:  0.001\n",
      "Epoch 31/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5894 - accuracy: 0.8040\n",
      "Epoch 00031: val_accuracy improved from 0.25000 to 0.47500, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.5826 - accuracy: 0.8074 - val_loss: 5.1152 - val_accuracy: 0.4750\n",
      "Learning rate:  0.001\n",
      "Epoch 32/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7794\n",
      "Epoch 00032: val_accuracy improved from 0.47500 to 0.49167, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.5588 - accuracy: 0.7796 - val_loss: 6.2812 - val_accuracy: 0.4917\n",
      "Learning rate:  0.001\n",
      "Epoch 33/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3840 - accuracy: 0.8731\n",
      "Epoch 00033: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3781 - accuracy: 0.8759 - val_loss: 12.1598 - val_accuracy: 0.3333\n",
      "Learning rate:  0.001\n",
      "Epoch 34/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4393 - accuracy: 0.8570\n",
      "Epoch 00034: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4413 - accuracy: 0.8565 - val_loss: 7.6428 - val_accuracy: 0.3167\n",
      "Learning rate:  0.001\n",
      "Epoch 35/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8636\n",
      "Epoch 00035: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3916 - accuracy: 0.8639 - val_loss: 9.8936 - val_accuracy: 0.3333\n",
      "Learning rate:  0.001\n",
      "Epoch 36/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3791 - accuracy: 0.8750\n",
      "Epoch 00036: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3752 - accuracy: 0.8759 - val_loss: 8.0252 - val_accuracy: 0.3417\n",
      "Learning rate:  0.001\n",
      "Epoch 37/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.9110\n",
      "Epoch 00037: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3231 - accuracy: 0.9102 - val_loss: 3.6444 - val_accuracy: 0.4917\n",
      "Learning rate:  0.001\n",
      "Epoch 38/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.8864\n",
      "Epoch 00038: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3909 - accuracy: 0.8796 - val_loss: 7.9980 - val_accuracy: 0.4750\n",
      "Learning rate:  0.001\n",
      "Epoch 39/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.9034\n",
      "Epoch 00039: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3353 - accuracy: 0.9046 - val_loss: 8.5470 - val_accuracy: 0.4667\n",
      "Learning rate:  0.001\n",
      "Epoch 40/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.8949\n",
      "Epoch 00040: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3728 - accuracy: 0.8944 - val_loss: 7.6990 - val_accuracy: 0.4083\n",
      "Learning rate:  0.001\n",
      "Epoch 41/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4994 - accuracy: 0.8655\n",
      "Epoch 00041: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.5149 - accuracy: 0.8620 - val_loss: 8.2985 - val_accuracy: 0.2750\n",
      "Learning rate:  0.001\n",
      "Epoch 42/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.8390\n",
      "Epoch 00042: val_accuracy did not improve from 0.49167\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.5706 - accuracy: 0.8380 - val_loss: 5.5161 - val_accuracy: 0.3417\n",
      "Learning rate:  0.001\n",
      "Epoch 43/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.9138\n",
      "Epoch 00043: val_accuracy improved from 0.49167 to 0.56667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.3299 - accuracy: 0.9139 - val_loss: 2.1616 - val_accuracy: 0.5667\n",
      "Learning rate:  0.001\n",
      "Epoch 44/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8968\n",
      "Epoch 00044: val_accuracy improved from 0.56667 to 0.85833, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.3901 - accuracy: 0.8926 - val_loss: 0.4308 - val_accuracy: 0.8583\n",
      "Learning rate:  0.001\n",
      "Epoch 45/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.9100\n",
      "Epoch 00045: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3145 - accuracy: 0.9102 - val_loss: 9.1143 - val_accuracy: 0.3750\n",
      "Learning rate:  0.001\n",
      "Epoch 46/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9432\n",
      "Epoch 00046: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2167 - accuracy: 0.9435 - val_loss: 3.6917 - val_accuracy: 0.4917\n",
      "Learning rate:  0.001\n",
      "Epoch 47/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.9555\n",
      "Epoch 00047: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1764 - accuracy: 0.9509 - val_loss: 4.6949 - val_accuracy: 0.5167\n",
      "Learning rate:  0.001\n",
      "Epoch 48/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9564\n",
      "Epoch 00048: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2007 - accuracy: 0.9519 - val_loss: 2.8039 - val_accuracy: 0.6500\n",
      "Learning rate:  0.001\n",
      "Epoch 49/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9261\n",
      "Epoch 00049: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2583 - accuracy: 0.9231 - val_loss: 3.9261 - val_accuracy: 0.3750\n",
      "Learning rate:  0.001\n",
      "Epoch 50/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9347\n",
      "Epoch 00050: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2391 - accuracy: 0.9352 - val_loss: 2.5510 - val_accuracy: 0.4167\n",
      "Learning rate:  0.001\n",
      "Epoch 51/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9233\n",
      "Epoch 00051: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2685 - accuracy: 0.9194 - val_loss: 6.2780 - val_accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 52/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.9072\n",
      "Epoch 00052: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3243 - accuracy: 0.9083 - val_loss: 3.7400 - val_accuracy: 0.4583\n",
      "Learning rate:  0.001\n",
      "Epoch 53/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9650\n",
      "Epoch 00053: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1593 - accuracy: 0.9657 - val_loss: 3.0690 - val_accuracy: 0.4583\n",
      "Learning rate:  0.001\n",
      "Epoch 54/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1884 - accuracy: 0.9517\n",
      "Epoch 00054: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1911 - accuracy: 0.9481 - val_loss: 2.4284 - val_accuracy: 0.5000\n",
      "Learning rate:  0.001\n",
      "Epoch 55/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5191 - accuracy: 0.8835\n",
      "Epoch 00055: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.5140 - accuracy: 0.8843 - val_loss: 7.6323 - val_accuracy: 0.3917\n",
      "Learning rate:  0.001\n",
      "Epoch 56/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.8958\n",
      "Epoch 00056: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3937 - accuracy: 0.8917 - val_loss: 3.2951 - val_accuracy: 0.5333\n",
      "Learning rate:  0.001\n",
      "Epoch 57/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.8778\n",
      "Epoch 00057: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4485 - accuracy: 0.8778 - val_loss: 12.1450 - val_accuracy: 0.2833\n",
      "Learning rate:  0.001\n",
      "Epoch 58/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.9205\n",
      "Epoch 00058: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3061 - accuracy: 0.9194 - val_loss: 10.9686 - val_accuracy: 0.3583\n",
      "Learning rate:  0.001\n",
      "Epoch 59/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4284 - accuracy: 0.8826\n",
      "Epoch 00059: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4334 - accuracy: 0.8796 - val_loss: 0.9301 - val_accuracy: 0.6750\n",
      "Learning rate:  0.001\n",
      "Epoch 60/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4180 - accuracy: 0.8750\n",
      "Epoch 00060: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4143 - accuracy: 0.8769 - val_loss: 25.9995 - val_accuracy: 0.4667\n",
      "Learning rate:  0.001\n",
      "Epoch 61/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.9432\n",
      "Epoch 00061: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2441 - accuracy: 0.9444 - val_loss: 16.5610 - val_accuracy: 0.5083\n",
      "Learning rate:  0.001\n",
      "Epoch 62/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.9186\n",
      "Epoch 00062: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3214 - accuracy: 0.9167 - val_loss: 6.9441 - val_accuracy: 0.3750\n",
      "Learning rate:  0.001\n",
      "Epoch 63/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4002 - accuracy: 0.8845\n",
      "Epoch 00063: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4525 - accuracy: 0.8750 - val_loss: 3.5373 - val_accuracy: 0.4667\n",
      "Learning rate:  0.001\n",
      "Epoch 64/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4945 - accuracy: 0.8580\n",
      "Epoch 00064: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4911 - accuracy: 0.8565 - val_loss: 6.3067 - val_accuracy: 0.5083\n",
      "Learning rate:  0.001\n",
      "Epoch 65/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.8930\n",
      "Epoch 00065: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3395 - accuracy: 0.8944 - val_loss: 9.3239 - val_accuracy: 0.5333\n",
      "Learning rate:  0.001\n",
      "Epoch 66/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9233\n",
      "Epoch 00066: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2543 - accuracy: 0.9204 - val_loss: 2.4315 - val_accuracy: 0.7000\n",
      "Learning rate:  0.001\n",
      "Epoch 67/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9413\n",
      "Epoch 00067: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2364 - accuracy: 0.9407 - val_loss: 5.4284 - val_accuracy: 0.5250\n",
      "Learning rate:  0.001\n",
      "Epoch 68/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.9669\n",
      "Epoch 00068: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1539 - accuracy: 0.9639 - val_loss: 2.3116 - val_accuracy: 0.5917\n",
      "Learning rate:  0.001\n",
      "Epoch 69/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.9583\n",
      "Epoch 00069: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1694 - accuracy: 0.9565 - val_loss: 1.5297 - val_accuracy: 0.6000\n",
      "Learning rate:  0.001\n",
      "Epoch 70/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.9214\n",
      "Epoch 00070: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2439 - accuracy: 0.9213 - val_loss: 2.1607 - val_accuracy: 0.5083\n",
      "Learning rate:  0.001\n",
      "Epoch 71/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4057 - accuracy: 0.8930\n",
      "Epoch 00071: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.3980 - accuracy: 0.8954 - val_loss: 5.8604 - val_accuracy: 0.6417\n",
      "Learning rate:  0.001\n",
      "Epoch 72/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9356\n",
      "Epoch 00072: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2568 - accuracy: 0.9352 - val_loss: 9.8896 - val_accuracy: 0.4333\n",
      "Learning rate:  0.001\n",
      "Epoch 73/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.9290\n",
      "Epoch 00073: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2850 - accuracy: 0.9259 - val_loss: 4.9603 - val_accuracy: 0.5500\n",
      "Learning rate:  0.001\n",
      "Epoch 74/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9754\n",
      "Epoch 00074: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1178 - accuracy: 0.9759 - val_loss: 3.8296 - val_accuracy: 0.6500\n",
      "Learning rate:  0.001\n",
      "Epoch 75/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1117 - accuracy: 0.9716\n",
      "Epoch 00075: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1146 - accuracy: 0.9713 - val_loss: 2.2670 - val_accuracy: 0.6167\n",
      "Learning rate:  0.001\n",
      "Epoch 76/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9650\n",
      "Epoch 00076: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2047 - accuracy: 0.9620 - val_loss: 3.5348 - val_accuracy: 0.4917\n",
      "Learning rate:  0.001\n",
      "Epoch 77/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.4407 - accuracy: 0.9006\n",
      "Epoch 00077: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.4493 - accuracy: 0.8991 - val_loss: 2.7328 - val_accuracy: 0.4167\n",
      "Learning rate:  0.001\n",
      "Epoch 78/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.5159 - accuracy: 0.8570\n",
      "Epoch 00078: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.5123 - accuracy: 0.8574 - val_loss: 2.0451 - val_accuracy: 0.6083\n",
      "Learning rate:  0.001\n",
      "Epoch 79/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9384\n",
      "Epoch 00079: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2655 - accuracy: 0.9361 - val_loss: 1.7540 - val_accuracy: 0.7583\n",
      "Learning rate:  0.001\n",
      "Epoch 80/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9384\n",
      "Epoch 00080: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.2312 - accuracy: 0.9398 - val_loss: 1.9503 - val_accuracy: 0.6417\n",
      "Learning rate:  0.001\n",
      "Epoch 81/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.9555\n",
      "Epoch 00081: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.1740 - accuracy: 0.9565 - val_loss: 3.4736 - val_accuracy: 0.6583\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9839\n",
      "Epoch 00082: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0789 - accuracy: 0.9843 - val_loss: 1.9063 - val_accuracy: 0.6750\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9848\n",
      "Epoch 00083: val_accuracy did not improve from 0.85833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0701 - accuracy: 0.9852 - val_loss: 0.6514 - val_accuracy: 0.8000\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9924\n",
      "Epoch 00084: val_accuracy improved from 0.85833 to 0.95000, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0525 - accuracy: 0.9889 - val_loss: 0.3021 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9896\n",
      "Epoch 00085: val_accuracy did not improve from 0.95000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0403 - accuracy: 0.9898 - val_loss: 0.1771 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9972\n",
      "Epoch 00086: val_accuracy did not improve from 0.95000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0331 - accuracy: 0.9963 - val_loss: 0.1860 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0331 - accuracy: 0.9943\n",
      "Epoch 00087: val_accuracy did not improve from 0.95000\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0326 - accuracy: 0.9944 - val_loss: 0.1897 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9981\n",
      "Epoch 00088: val_accuracy improved from 0.95000 to 0.95833, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0230 - accuracy: 0.9981 - val_loss: 0.2074 - val_accuracy: 0.9583\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0242 - accuracy: 0.9981\n",
      "Epoch 00089: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0240 - accuracy: 0.9981 - val_loss: 0.2194 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9877\n",
      "Epoch 00090: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0490 - accuracy: 0.9880 - val_loss: 0.2308 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9981\n",
      "Epoch 00091: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0208 - accuracy: 0.9981 - val_loss: 0.2623 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9981\n",
      "Epoch 00092: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0206 - accuracy: 0.9981 - val_loss: 0.2688 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9991\n",
      "Epoch 00093: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0155 - accuracy: 0.9991 - val_loss: 0.2615 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0211 - accuracy: 0.9972\n",
      "Epoch 00094: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0207 - accuracy: 0.9972 - val_loss: 0.2816 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9934\n",
      "Epoch 00095: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0360 - accuracy: 0.9935 - val_loss: 0.4992 - val_accuracy: 0.8500\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9991\n",
      "Epoch 00096: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9991 - val_loss: 0.3610 - val_accuracy: 0.9083\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9981\n",
      "Epoch 00097: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0176 - accuracy: 0.9981 - val_loss: 0.2626 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9981\n",
      "Epoch 00098: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0215 - accuracy: 0.9981 - val_loss: 0.2758 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9981\n",
      "Epoch 00099: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0185 - accuracy: 0.9981 - val_loss: 0.2976 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9991\n",
      "Epoch 00100: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0146 - accuracy: 0.9991 - val_loss: 0.2722 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9962\n",
      "Epoch 00101: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0520 - accuracy: 0.9963 - val_loss: 0.2801 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9905\n",
      "Epoch 00102: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0223 - accuracy: 0.9907 - val_loss: 0.3121 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9991\n",
      "Epoch 00103: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0134 - accuracy: 0.9991 - val_loss: 0.2912 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 104/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9981\n",
      "Epoch 00104: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0127 - accuracy: 0.9981 - val_loss: 0.2765 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9981\n",
      "Epoch 00105: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0125 - accuracy: 0.9981 - val_loss: 0.2562 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9991\n",
      "Epoch 00106: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0106 - accuracy: 0.9991 - val_loss: 0.2457 - val_accuracy: 0.9583\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9962\n",
      "Epoch 00107: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0155 - accuracy: 0.9963 - val_loss: 0.2710 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9981\n",
      "Epoch 00108: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0136 - accuracy: 0.9981 - val_loss: 0.3162 - val_accuracy: 0.9250\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9991\n",
      "Epoch 00109: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0081 - accuracy: 0.9991 - val_loss: 0.2914 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9991\n",
      "Epoch 00110: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0091 - accuracy: 0.9991 - val_loss: 0.2811 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0444 - accuracy: 0.9943\n",
      "Epoch 00111: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0435 - accuracy: 0.9944 - val_loss: 0.2328 - val_accuracy: 0.9500\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9924\n",
      "Epoch 00112: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0361 - accuracy: 0.9926 - val_loss: 0.4558 - val_accuracy: 0.8333\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9830\n",
      "Epoch 00113: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0768 - accuracy: 0.9815 - val_loss: 0.2333 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9905\n",
      "Epoch 00114: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0504 - accuracy: 0.9907 - val_loss: 0.3166 - val_accuracy: 0.9000\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9972\n",
      "Epoch 00115: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0215 - accuracy: 0.9972 - val_loss: 0.3324 - val_accuracy: 0.8917\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9962\n",
      "Epoch 00116: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0161 - accuracy: 0.9963 - val_loss: 0.3107 - val_accuracy: 0.9333\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9953\n",
      "Epoch 00117: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0390 - accuracy: 0.9954 - val_loss: 0.2401 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9981\n",
      "Epoch 00118: val_accuracy did not improve from 0.95833\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0127 - accuracy: 0.9981 - val_loss: 0.2468 - val_accuracy: 0.9417\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9981\n",
      "Epoch 00119: val_accuracy improved from 0.95833 to 0.96667, saving model to C:\\Users\\Wender\\Notebook\\proj\\saved_models\\CNN_50_without_augmentation_model.h5\n",
      "1080/1080 [==============================] - 2s 2ms/sample - loss: 0.0118 - accuracy: 0.9981 - val_loss: 0.1797 - val_accuracy: 0.9667\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9972\n",
      "Epoch 00120: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0194 - accuracy: 0.9963 - val_loss: 0.3858 - val_accuracy: 0.8917\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9991\n",
      "Epoch 00121: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0118 - accuracy: 0.9991 - val_loss: 0.3641 - val_accuracy: 0.9083\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9981\n",
      "Epoch 00122: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.3273 - val_accuracy: 0.9333\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9981\n",
      "Epoch 00123: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.2973 - val_accuracy: 0.9500\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9981\n",
      "Epoch 00124: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0087 - accuracy: 0.9981 - val_loss: 0.2944 - val_accuracy: 0.9500\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9991\n",
      "Epoch 00125: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0092 - accuracy: 0.9991 - val_loss: 0.2950 - val_accuracy: 0.9500\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9981\n",
      "Epoch 00126: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0114 - accuracy: 0.9981 - val_loss: 0.3006 - val_accuracy: 0.9500\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9981\n",
      "Epoch 00127: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.3044 - val_accuracy: 0.9500\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9981\n",
      "Epoch 00128: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0119 - accuracy: 0.9981 - val_loss: 0.2957 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9991\n",
      "Epoch 00129: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0086 - accuracy: 0.9991 - val_loss: 0.2919 - val_accuracy: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-05\n",
      "Epoch 130/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9972\n",
      "Epoch 00130: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0247 - accuracy: 0.9972 - val_loss: 0.2887 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9991\n",
      "Epoch 00131: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0399 - accuracy: 0.9926 - val_loss: 0.2875 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9991\n",
      "Epoch 00132: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.2840 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9991\n",
      "Epoch 00133: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0101 - accuracy: 0.9991 - val_loss: 0.2895 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9991\n",
      "Epoch 00134: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0081 - accuracy: 0.9991 - val_loss: 0.2988 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9981\n",
      "Epoch 00135: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0103 - accuracy: 0.9981 - val_loss: 0.2986 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9991\n",
      "Epoch 00136: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0096 - accuracy: 0.9991 - val_loss: 0.2990 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9981\n",
      "Epoch 00137: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.3015 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9981\n",
      "Epoch 00138: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.3022 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9981\n",
      "Epoch 00139: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.3037 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9981\n",
      "Epoch 00140: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.2970 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9991\n",
      "Epoch 00141: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0072 - accuracy: 0.9991 - val_loss: 0.2988 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9991\n",
      "Epoch 00142: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0072 - accuracy: 0.9991 - val_loss: 0.3028 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9991\n",
      "Epoch 00143: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0097 - accuracy: 0.9991 - val_loss: 0.3026 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9991\n",
      "Epoch 00144: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.2975 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9991\n",
      "Epoch 00145: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0066 - accuracy: 0.9991 - val_loss: 0.2912 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9981\n",
      "Epoch 00146: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0110 - accuracy: 0.9981 - val_loss: 0.2944 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9991\n",
      "Epoch 00148: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0074 - accuracy: 0.9991 - val_loss: 0.3048 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9981\n",
      "Epoch 00149: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.2803 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 00150: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.2766 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 152/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2769 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 153/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9896\n",
      "Epoch 00153: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0465 - accuracy: 0.9898 - val_loss: 0.2778 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 154/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 00154: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.2756 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 155/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0307 - accuracy: 0.9943\n",
      "Epoch 00155: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0301 - accuracy: 0.9944 - val_loss: 0.2658 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 156/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9981\n",
      "Epoch 00156: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0113 - accuracy: 0.9981 - val_loss: 0.2745 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 157/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9991\n",
      "Epoch 00157: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0075 - accuracy: 0.9991 - val_loss: 0.2878 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 158/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0855 - accuracy: 0.9915\n",
      "Epoch 00158: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0837 - accuracy: 0.9917 - val_loss: 0.2626 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 159/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9972\n",
      "Epoch 00159: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.2548 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 160/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2592 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-05\n",
      "Epoch 161/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 00161: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 162/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2711 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 163/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0203 - accuracy: 0.9981\n",
      "Epoch 00163: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0207 - accuracy: 0.9972 - val_loss: 0.2701 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 164/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0310 - accuracy: 0.9943\n",
      "Epoch 00164: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0303 - accuracy: 0.9944 - val_loss: 0.2679 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 165/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00165: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2686 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 166/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 00166: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.2714 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 167/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9981\n",
      "Epoch 00167: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.2741 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 168/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2742 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 169/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 00169: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.2768 - val_accuracy: 0.9667\n",
      "Learning rate:  1e-06\n",
      "Epoch 170/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 00170: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2736 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 171/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00171: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 172/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 00172: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.2715 - val_accuracy: 0.9667\n",
      "Learning rate:  1e-06\n",
      "Epoch 173/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00173: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 174/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9962\n",
      "Epoch 00174: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0381 - accuracy: 0.9963 - val_loss: 0.2779 - val_accuracy: 0.9667\n",
      "Learning rate:  1e-06\n",
      "Epoch 175/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9962\n",
      "Epoch 00175: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0099 - accuracy: 0.9963 - val_loss: 0.2783 - val_accuracy: 0.9667\n",
      "Learning rate:  1e-06\n",
      "Epoch 176/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00176: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.9667\n",
      "Learning rate:  1e-06\n",
      "Epoch 177/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9972\n",
      "Epoch 00177: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0309 - accuracy: 0.9972 - val_loss: 0.2737 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 178/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 179/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2727 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 180/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00180: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0248 - accuracy: 0.9954 - val_loss: 0.2693 - val_accuracy: 0.9583\n",
      "Learning rate:  1e-06\n",
      "Epoch 181/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2681 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9981\n",
      "Epoch 00182: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0081 - accuracy: 0.9981 - val_loss: 0.2719 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2717 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00185: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2713 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2696 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9991\n",
      "Epoch 00187: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0072 - accuracy: 0.9991 - val_loss: 0.2709 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 00188: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.2710 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00189: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9991\n",
      "Epoch 00190: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.2717 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9981\n",
      "Epoch 00191: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.2747 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9991\n",
      "Epoch 00193: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0089 - accuracy: 0.9991 - val_loss: 0.2776 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9953\n",
      "Epoch 00195: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0452 - accuracy: 0.9954 - val_loss: 0.2772 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0424 - accuracy: 0.9962\n",
      "Epoch 00196: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0415 - accuracy: 0.9963 - val_loss: 0.2814 - val_accuracy: 0.9500\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 00197: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2774 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 00198: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.2772 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00199: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00200: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 201/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00201: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2735 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 202/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 203/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00203: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 204/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9981\n",
      "Epoch 00204: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0185 - accuracy: 0.9981 - val_loss: 0.2718 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 205/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2699 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 206/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00206: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2676 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 207/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 208/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 00208: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0337 - accuracy: 0.9907 - val_loss: 0.2673 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 209/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0724 - accuracy: 0.9896\n",
      "Epoch 00209: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0708 - accuracy: 0.9898 - val_loss: 0.2644 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 210/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 211/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 212/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 00212: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.2715 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 213/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9991\n",
      "Epoch 00213: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.2718 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 214/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 215/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 216/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 00216: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2716 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 217/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 218/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 219/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 220/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00220: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2694 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 221/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0365 - accuracy: 0.9877\n",
      "Epoch 00221: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0358 - accuracy: 0.9880 - val_loss: 0.2676 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 222/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9972\n",
      "Epoch 00222: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.2676 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 223/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 00223: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0275 - accuracy: 0.9972 - val_loss: 0.2670 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 224/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2661 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 225/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0215 - accuracy: 0.9972 - val_loss: 0.2634 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 226/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 227/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00227: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2679 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 228/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9924\n",
      "Epoch 00228: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0329 - accuracy: 0.9926 - val_loss: 0.2657 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 229/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 230/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00230: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 231/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0306 - accuracy: 0.9934\n",
      "Epoch 00231: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0300 - accuracy: 0.9935 - val_loss: 0.2642 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 232/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00232: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2651 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 233/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00233: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 235/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00235: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 236/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00236: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 237/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2685 - val_accuracy: 0.9667\n",
      "Learning rate:  5e-07\n",
      "Epoch 238/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9991\n",
      "Epoch 00238: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.2668 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 239/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9991\n",
      "Epoch 00239: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.2654 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 240/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00240: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2673 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 241/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9991\n",
      "Epoch 00241: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0076 - accuracy: 0.9991 - val_loss: 0.2779 - val_accuracy: 0.9500\n",
      "Learning rate:  5e-07\n",
      "Epoch 242/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9991\n",
      "Epoch 00242: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0085 - accuracy: 0.9991 - val_loss: 0.2760 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 243/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9991\n",
      "Epoch 00243: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0310 - accuracy: 0.9944 - val_loss: 0.2706 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 244/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 245/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00245: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 246/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2699 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 247/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9962\n",
      "Epoch 00247: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0593 - accuracy: 0.9963 - val_loss: 0.2668 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 248/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 249/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 250/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9991\n",
      "Epoch 00250: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.2736 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 251/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 252/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00252: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2692 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 253/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2696 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 254/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 255/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0211 - accuracy: 0.9954 - val_loss: 0.2642 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 256/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2606 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 257/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 00257: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.2653 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 258/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9991\n",
      "Epoch 00258: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0063 - accuracy: 0.9991 - val_loss: 0.2663 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 259/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00259: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2676 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 260/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9991\n",
      "Epoch 00260: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2691 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 261/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 00261: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.2676 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 262/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 263/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00263: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2692 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 264/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00264: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2691 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 265/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9953\n",
      "Epoch 00265: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0453 - accuracy: 0.9954 - val_loss: 0.2714 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 266/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2685 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 267/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9991\n",
      "Epoch 00267: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.2728 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 268/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9972\n",
      "Epoch 00268: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0340 - accuracy: 0.9972 - val_loss: 0.2733 - val_accuracy: 0.9500\n",
      "Learning rate:  5e-07\n",
      "Epoch 269/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00269: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 270/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00270: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 271/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00271: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 272/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 273/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9934\n",
      "Epoch 00273: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0388 - accuracy: 0.9935 - val_loss: 0.2574 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 274/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 275/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00275: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2618 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 276/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 00276: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2613 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 277/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00277: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2609 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 278/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 279/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 00279: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.2630 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 280/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 00280: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 281/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2682 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 282/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 283/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9991\n",
      "Epoch 00283: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0062 - accuracy: 0.9991 - val_loss: 0.2711 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 284/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 285/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 00286: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 287/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00287: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 288/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2762 - val_accuracy: 0.9500\n",
      "Learning rate:  5e-07\n",
      "Epoch 289/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n",
      "Epoch 00289: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0273 - accuracy: 0.9963 - val_loss: 0.2740 - val_accuracy: 0.9500\n",
      "Learning rate:  5e-07\n",
      "Epoch 290/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2692 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 291/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9905\n",
      "Epoch 00291: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0754 - accuracy: 0.9907 - val_loss: 0.2567 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 292/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00292: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2557 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 293/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 294/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2577 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 295/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2592 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 296/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00296: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2603 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 297/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 298/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 00298: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.2662 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 299/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9991\n",
      "Epoch 00299: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0084 - accuracy: 0.9991 - val_loss: 0.2655 - val_accuracy: 0.9583\n",
      "Learning rate:  5e-07\n",
      "Epoch 300/300\n",
      "1056/1080 [============================>.] - ETA: 0s - loss: 0.0479 - accuracy: 0.9943\n",
      "Epoch 00300: val_accuracy did not improve from 0.96667\n",
      "1080/1080 [==============================] - 1s 1ms/sample - loss: 0.0476 - accuracy: 0.9944 - val_loss: 0.2616 - val_accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(X_train, Y_train, validation_data = (X_test,Y_test), epochs = 300, batch_size = 32, callbacks=[scheduler, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc3 = history3.history['accuracy']\n",
    "val_acc3 = history3.history['val_accuracy']\n",
    "\n",
    "fig3,ax3 = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "ax3.plot(acc1, label='Training Accuracy')\n",
    "ax3.plot(val_acc1, label='Validation Accuracy')\n",
    "\n",
    "ax3.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax3.set_ylabel(r'Acc', fontsize=20)\n",
    "ax3.set_title('CNN50', fontsize=24)\n",
    "ax3.tick_params(labelsize=20)\n",
    "\n",
    "ax3.legend(loc=4, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAGeCAYAAAADl6wFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOy9d7xcdZ3//3xPvTWFFBJCCYGEgAQQgoA0AUEE7KDrDwsqKpaVta1+FQQURNfGyorK7toVKasiRWlBQXqC9AAB0kivt0875/P743POzJm5M/dOn8vwfj4e93Eyp35mJuG+eL2bGGNQFEVRFEVR2o9QqxegKIqiKIqiNAYVeoqiKIqiKG2KCj1FURRFUZQ2RYWeoiiKoihKm6JCT1EURVEUpU1RoacoiqIoitKmqNBTFEVR8hCRc0TEiMjfWr0WRVFqQ4WeoiiKoihKm6JCT1EURSmkD3gOWNPqhSiKUhuikzEURVEURVHaE3X0FEVRFEVR2hQVeoqi1AUR2V9EfiIiz4vIkIjsFJEnReSHInJYkfNfKyK/EZG1IpIUka0icpuIvGuMZ6zyigTeICKzveetFZEREVkuIp8VkVDg/LNE5F5vLf0icouIHFji3r/w7n2xiHSIyCUi8qx3780ico2ILBhjbUeIyOUi8qCIrBORlHfdX0XkzDGuCz43LiJfFZEnRGTA2z/FO6/Hez+/FZGnvPc0IiIviMjVIjJ/jGf0isiFIrLMu29KRNaLyFIR+U7hZ1JOMYaIvNN7b1u87+9lb22Hljh/rndP470+UER+LyIbRSThfdYXikis1DMVRakCY4z+6I/+6E9NP8C/AhnAeD+DwHDg9d8Kzv8Y4ASO7yi4/tdAuMhzVnnHPwRs8P7cV3Dtld653/JeZ4D+gmfNL3LvX3jHLwce8P6c9O7vXzsEHFfk2p7AOQZIFTzTAD8t8dn5z/0W8FDg+p3en6d453264H793vqCn/kbi9x/MvB04DwH2F7w+X+r4Jpzin1v3rEQ8MvAtRnvMw3e/xNFrpsbOOeUwN+PnQVr+VOr/z7rj/600486eoqi1ISInAX8EAgDNwAHGGN6gG5gN+B9wLLA+a8HfowVDDcAexhjpgJTgK9if9m/D/h/Yzz2B8BK4GBjzGRgEnChd+xTIvIV4HPAvwGTjTGTgEXYAoMpwGVj3PsTwEHAB4Ee7/6vBR4FuoDrRGRqwTUucCvwXmAO0OE9cypWBA8CH/M+q1J8ClgA/Iv33ClYcTTkHd8GXAm8Hiv+JgEdwP7Ab7Gf9+9EpLvgvucDBwBbgDOAuDFmF+/aBcCXgRfHWFch/w58APs9XQhM9b6/3YHrsd/rf4nIcWPc41rgJmBv731Own7fBnibiJxWwXoURRmLVitN/dEf/Xnl/gBRYC32F/TvyrzmLu/8f1Dctfumd3wAmFRwbJV3bDue01Xi3gb4WpHjx3rHEkCs4NgvAteeXeTa6cBW7/gFFX5O7/euu7vIseBzT6nyexDgDu8eHyw4dqu3/0sV3O8cijux3eQczsuLXBcG7vWO31NwbG7gfd6OVwxYcM5N3vGftfrvtv7oT7v8qKOnKEotnIR1chzgi+OdLCK7ACd4Ly83xjhFTvs2Voj1AKWcnZ8YY3YW2X+nt00B3y9y/D7v3nFg3xL3Xg38rnCnMWYr8FPvZcmcuxLc5G2PFJFwiXOeMMbcXuF9/bUZ4Bbv5dEFh/u97exq7l3AKVj3LQX8R5F1OMA3vJfHisisEvf5lrfmQv7kbYvmUSqKUjkq9BRFqYUjve3jxph1ZZz/Wqz7ZIC/FzvBGNNHLtRbNLEfeLLE/s3edpUxZrDIvV2sKwc2rFqMv5cQIZBb84GFRQMiEhGRj3gFChu8AgW/+GCHd1rHGM99oMT+4DN2F5Fve0UVO0XECTzjB95puxVcdqu3/YyI/FpE3iwiveM9qwT+9/G4MWZHiXPuwebtBc8v5JES+/2/Q6U+I0VRKiTS6gUoivKKZldvW25j3Rnetq+YEAvwcsH5hWwosd8Z53jwnGiJ42MJVv9YGCtGNoGtiAVuw+bP+Yxg8+Jc77X/WXWTE5tBtozxXETkeOBmrNPp04d1KAE6sW5bXo6eMeZXInI0tgDmfd6PKyJPYJ3GHxtjxvq8gvjfR8nPyBiTEJFt2Pdb9PszxgyUuNx/L6W+G0VRKkQdPUVRakGqvC5e11U0j1Lv90KsyNuKLeLY1RjTZYyZaYyZhS3QGO8excLY9gKRKPAbrMi7EzgO6DTGTDHGzPKe8blS9zfGfBwbDv068Ddste4h3rpXiMjJpZ5dglfq96corzrU0VMUpRY2etu9yjzfd606RWSGMaaUi7V7wfnNpDD0GcTPc3PIhWMB/GrafzXG/L7IdbsW2VcJR2E/k+3A24wxw5U+wxjzNHARgBd2PgVb+LII+KWI7GWMSY+zDv/7KPl9i0gHMK3gfEVRWoQ6eoqi1MKD3vYgEZkz5pmWf2Lz8yBXlJGHiEwG/AbLj9a2vKo4voxjTxljUoH9vjD9Z4nr3ljjmvz7P19C5FX0DGNMyhhzMzmBOhso2XA5gP99zB/j+z6OnInQiu9PUZQAKvQURamFu7D5WmHgO+OdbIzZDtztvfxScIpFgC9hixYGyRUSNJO5IvLewp1exfDHvJfXFxzu87aLilzXg+0PWAv+/ed7jlnhM06htHAea9LESODP5YRjb8dW8UYpUmXtVRT7/QzvNcZsLDxHUZTmokJPUZSq8UJ9n/devldErhORhf5xsWPKPioiPwxcdiG2QOFQ4Pcisrt3bo/X6PjL3nnfMsb003z6gP8WkfeJSMRb20HYYosZ2MreqwquucPbfl9EjhcR8a47HCuGp9e4pvuwkySmAb8Skdne/TtF5MPA/2EbKhfjTrFj6I4TkU5/p4i8BtvDD2zxSqlK5izGmCFsuBdsFe9XPSGL5/BdAxyD/X4vqOwtKorSCFToKYpSE8aYa7Fiz8WGApd781SHgfXA1dhJE/759wOfDJy/RkS2Y0dhXYYtJvgtdiRYK/gxVvT8GhgUkT7gcWAxVmydVaS1yAXYQow9sMUOwyIyCDyMdflGOYSV4PUM9CeFnAWsF5GdWHftf4EXgEtKXD4JO53j79772S4iI8BTWBdwGHi/MSZT4vpCvgv8Cvs9XQrs9L6/td7aXGyu4j2VvUtFURqBCj1FUWrGGPN9bI+8n2OnV0SxrTKeAP4T+GzB+T8FDsc2Jt6ArSbtwzpjZxlj3leimXIzSGIF0NexzZNj2KKC3wOHFhMwxpiXgNdhK2M3Y0PZO7GC9fBqGyEXPOOHwDvJuXsR4FlsgcXrsZNEinGud87d2DY4vqv3LPBfwIHGmLsqWIdjjPkgtmn07dj32YP9Hq8BXmeMKXQ8FUVpEVK6L6iiKMqrBxH5BbY1yiXGmItbuxpFUZT6oI6eoiiKoihKm6JCT1EURVEUpU1RoacoiqIoitKmqNBTFEVRFEVpU7QYQ1EURVEUpU3RWbdFmD59upk7d26rl6EoiqIoijIuy5Yt22qMmVHsmAq9IsydO5elS5e2ehmKoiiKoijjIiKrSx3THD1FURRFUZQ2RYWeoiiKoihKm6JCT1EURVEUpU1RoacoiqIoitKmtFzoiciZInKliNwrIv0iYkTkN1Xea3cR+ZmIrBeRpIisEpErRGRqvdetKIqiKIoy0ZkIVbcXAAcDg8DLwMJqbiIi+wD3AzOBG4FngdcB5wOnisjRxphtdVmxoiiKoijKK4CWO3rAZ4EFwCTgEzXc5yqsyPuMMebtxpgvG2NOBH4A7AdcVvNKFUVRFEVRXkG0XOgZY+42xqwwNYzoEJF5wCnAKuBHBYcvAoaA94tId9ULVRRFURRFeYXRcqFXJ070trcbY9zgAWPMAHAf0AUc2eyFKYqiKIqitIp2EXr7edvnSxxf4W0XNGEtiqIoiqIoE4J2EXqTvW1fieP+/imlbiAiHxORpSKydMuWLXVdnKIoiqIoSiuYCFW3zUC8bck8QGPM1cDVAIsXL646X1BRlDqw8SkY2ACTd4eZ+1d2bf962PR08WPhKMw9FkLh/P3GwNqHIdlf3XoVRVFKMW1f2GXvlj2+XYSe79hNLnF8UsF5iqJMVF64E37zrtzrc26FuUeXd+3gFvjx62FkR+lzDvsQvOWK/H0P/QT++uXK16ooijIeb7wEjvm3lj2+XYTec962VA7efG9bKodPUZSJQGoYbv4cTF8Ab/sR/N9H4Kbz4RP3QSQ+/vW3fQWSg/Dea6F7+ujj//wNLPs5HPQe2Osou2/nWrjrG7DPiXDCV+v7fhRFUSbNaenj20Xo3e1tTxGRULDyVkR6gaOBEeDBViyulSTSDr9+YDXvOHQO03vi3LV8E0ue3dzqZdWNkMlwzOZreHj620mEe2u61/TEGvYaeoJl086o0+pevUxKb+Hozb8napIVXTctuZZ9B1fz3/v8F6uWdjJ/6mc4Z+UXeP4/z2BHbLcxr426SQ7d8ReW7HoOdz1T/D+sMee9fCZ6K+a3H2JF7xEA7D78DDMch/+MfYKdj5QhJhVFUSrgTa+JcNyk8c9rFK8ooSciUWAfIG2MedHfb4x5UURux/bS+xRwZeCyS4Bu4KfGmKFmrncicOWSFfzo7hd5ZNV2/v3UhXziN48Si4ToiLZHHc6B5nne5PyU5zYPcWPo7TXd63LnSk40D/HNDYeByPgXKMUxhqvcSzjULKePnoov/285k5+ungVs5DbmEpK3cvrA35mWNe5L84AczAVbTyG1bWPJcza453Gh+2P23Wb//9AlxOWhD3PTCgFKX6coilINC3bt5bgFM1r2fKmhT3F9FiDydsD/DT0LeBPwEnCvt2+rMeYL3rlzgZXAamPM3IL7FI5AWw4cAZyADdm+vtwRaIsXLzZLly6t+j1NFJ7bOMDpP7yXGb1xNvQlmDOlk4FEmrs+/wZm9LaJc/H0n+D6D8Luh8O5d1Z/n3QCvrMPpAbhq5sg2lG/Nb7aeOwa+NN5cPr34fCPtHo1iqIobY+ILDPGLC52bCLYOocAH/R+3uTtmxfYd2Y5N/EcvsXAL7AC7/NY9++HwFGvxjm337j5GXo7Itz46aNZOKuXdTtH+Mpp+79yRd7OtfA/b4T+Dbl9A96fX34EBmpwY1b+3Yo8gPRwedc8cBVc/yFw3fHPLeTlZXDFIvjeQrjl85VfXyt96+Dnp8Oqf9T3vkPbbJ7cHkfYogdFURSlpbRc6BljLjbGyBg/cwPnrircV3CvtcaYDxljZhtjYsaYvYwx5xtjtjfr/UwkHn95J285eDdm9nZw1dmHcsHp+/PuxXu0elnVs/YhK+hevCu3r3997s/P3lL9vZfflPtzemT8842BB6+Cp/8Aj/6y8ue9uAR2roFYDzz318qvr5W//Dus/gf86ZO2AKJe3H6BbVFyxhUQavl/XhRFUV716H+J25ThVIaBRIZZk20Ict6MHs49dh6hUIW5Z8PbbVhzIuC7d+uW5e+bshfsMq8yoZdO2PcG4Drw3F8g2uUdK0P4bHgM+tZCfDLccdHYbqIxuWf5bH0OJu8BC0+Doc32nHpR+KxClt8Mz94MB7wddq6Gv3+7+HmZFCQCfeW2vQjr/1n654nr4PHfwdHnw64H1PQWto1swzVVOKVKSXYmdo65byg9xNPbns7+bBra1MzlKRXQn+rHcZ1WL0N5haBCr03Z2GfF2axJNeSaDW2F/zoc7ry4Pouqlf5iQm8jTNoNFp4BK++BRBmtEo2B694PPz3Oiry1D8HwVlh4uj1ejtB79haQEJx9PaSH4KGflj73tq/C9/eHzctz+7Y+b1uIdM8EJ1Xeusvh4f+2uYYr7ih+PNEPt34Rdj0Q3vU/8Nr3wf1X2gbFQVwXfvNOuOoou7bHr4UrD4Wr31D65w8ftYL7uC/W9BY2Dm3k5BtO5u41d49/slIWf1zxR4679jjuWpNzw/+29m8cf93x3PD8DQynhznrprP4l5v/Jftz6h9O5emtJRpPKy1jy/AWTv/D6Xzxntr+nSmvHl5RVbdK+Wzsr4PQu+2rVgBteKxOq6qRAS9Mu+lp68hFO2zodvbBVujd/0N4/nY46Kyx7/PU/8GK2+2fX15qRVs4Bvu/BZ68vrzQ7fKbYa+jYc8jbI+kvpeLn/fyMhvixcBN/wYf+ovdv3UFHPp66JlpXw9tgc6SE/rKo2+dFeXGtb3oPvUgxLrzz1lyqXVB3/NrOyXi5G/Y0PFNn4GP3JGbGPHPX8Mqrx7qli/YJsZzFsNxXxh7DXscAdHOmt7GY1seI+2meXmwxGeqVMTWka18Z+l3MBi++dA3OWLWEYgIlz54Ka5x+f7S7/PY5sdYO7CWC4+8kJldM3GNy6UPXsrFD1zMNadfQySkvyomCt9+5NvsTO7kjtV3sGTNEk7c88RWL0mZ4Oi/3jZlkyf0dp1cQug9eQPsexJ0Ti1+/MW74YnfQ7Tbuk8Tgf4N1kVzM7DxSdh9sRUtC061VbfdM21I8sB3WTG3z4nQPS13/eO/h+0vwdKfwa6LYMuz8OxNNj9v3hugZ1d73niO3rYXYctyOMwLefbMtOHXIC8ugTUP2qrg3llw9L/BX79k8/nmn2yfMX0+dHsl94Ob7etycF0rxPrXAfB0ajsJN8Nh65dbh/LtP7FVr3+7HE65NHfdumXw8NVw+Ln2swPo2gVOvdy6cY/8LxzxMbuWOy6EvY6xIdiHr4ZQBN56ZV5I1jUuf3rhT7x57zfTGalN3AXxXaS+ZHUu5wPrH+DRzY/WbT2vdJZuXEoik+Cbx3yTr/7jq3zxni8iCJuHN/PNY77JxfdfzI0v3si75r+Ld+/37ux1GTfD5//+eb7yj6+w16S9mr7urkgXZy04i55YD49uepQHNjzQ9DVMNPqT/dy26jbOO/g8lqxZwmUPXcby7blIwRv3fCP77bIfW0e28ocVfyDtplu42vKISIR3zH8HM7tm8uLOF/nrqhbkLAPxcJx3zX8XUztyvxNvfelWVvavrPneR80+ikN3PbTm+1SLCr02ZWOfbVRb1NEb3GInDpx0ERz7ueI3eOBHMGl3WPwhWPINW00ZFE2tYGC9ddFW3WtFy/T5VjBNmm0T/xeeZgXsI/8Df/kizH8T/H/X2p54O9fAHz9u79M1Hd5+lXW/lv3SFg8c+7mcEzWeo/fS3+x2gVck3j0TdqzKHd/0NPz2LCtIo91w1s9h/ilWfN5/JUzZ0543Y7+c0C4UimPx6C/g5s8CsD0U4uO7zyYpwh83bWX3N10Gh7wX1jxgq4IXvRtmH2Sve+Aq6xqe9LX8+y06Cx6/Bu76Oux/Btx+of0M3nKFFalrH7LiuSDv7h/r/sFF91+Ea1zOXFBWcXxZPLn1SaA6obdixwo+eecnyZhM3dbzSicsYT6/+PO8ZZ+3sLp/NVc/cTUA5y46l7fs8xb6U/3834r/47OHfTbvupP3Opm37fM2bnzxxlYsG4CX+l7ivIPP47w7z2MkU4bT/irg0JmH8tFFH+W4Ocfxybs+yU8e/0n22A3P38CNb7+RL9/zZR7a+FALV1kZ962/j6tOuorz7jyPjUOt62X52ObHuPLEKxER7l5zN1+690t1uW9npFOFnlJ/NvUn6I1H6I4X+Yr9BOxSTl2i37Ybed3HYJYnErataK3QM8bm473mHbDtBVj/KMw73h7rnW23C98Cy35hnbPOqbDiNnjmT/Yav1DjXx+FafvYP+9/hlfBK7Dfabk8ufGE3rpHrVicOte+7pkBLz9s/+y6dmRXx2T49FLrmPkc9G649Qu5Ct/pC+yzwYrvchjYCHdcDHOPhQ/exHf+8RWGVv2VaCjKpa87lh8v/oi948mX2AKTmz4D595lnb4Vt8MBb4WOghbtInD692w+3m/eBZufgTf8v5zD+PF7ii5lyZolADy19am6CT3HdXhm2zMA9KUqE3qucfn6A1+nJ9bDn9/+57z/M1csn37tp/n0az+dt+/s/c/m7P3PHnWuiHDpMZdy6TGXjjrWDH6w7Af87Kmf8dTWpxCEO868g1nds1qylonIohmLuPdf7s2+fmbbM7z3lvdyzl/PYcWOFVxwxAW8Z+F7WrjC8vjTC3/iwvsu5AN//QCbhjbx6zf/mkNmHtL0dfzy6V/y3aXf5Y7Vd3D0nKO57KHL2HfKvlz3luuIhqJNX089UaHXpmzqT5QO2ya9Skpf6C37pQ3XHe8l975why0QWHiGdcv8c/c8srGLHovh7XZNvbvBbofa3Dq/tcokbzTW3sdCrBecpM2F++PH4S9fgnkn2Jy6GfvnRB5YcXfz52xeWY9XFAHjh27XLYM5gekZ3TNheJsVU4/9zraAecdP80Ue2GKPW78Aj/3WCsHuGTafTkLjOnoPb3iY7y37Hu6O1TC9CyaHcG86k+d3PM/HD/o4U+JT+PYj3+adf34nU+JTuOKEK5h86uXWuX34apg2n7vCae6NJbi42AN2mQfHfwnuusQK0GM+W+ysLI7rcPdaWyzhO3DFuO656+hP9XPuonPHvJ/PS30vZZ2bYo7eSztf4sL7LiTlpkYdSzpJVvat5NKjL1WR1wacd/B53LbqNl7Y+QJfOvxLKvLG4YBpB3D2/mfz62d+zcEzDuas/cbJVZ4gvG2ft/HnF//MIxsf4T37vaclIg/s//Dc8tItfO3+rzElPoXNw5v57vHffcWLPFCh17Zs7E+w66QSjZH9lhlbnrdO2UM/taHNYz9vQ6DLb7YiZI/X2fMiHbBl/PFTDcUvxOidBXOPgedugdX35/aBHXr/psvsdub+8JYfwn+fYB22Nffb9xekdxa88SLY7bX2dba9yhiOXnLA5va95h25fT0zrWAb3mYbEPfOhoOK/J/0pN1sQcO6pTD7ECsUJWzdwcGxhd6DGx5k+bblHJ8y0DkdJtucqcN2PYyPHvRRIhJh8/Bmnt/5PPetu487Vt/BmQe+y4Zkl1zK1r2O5MIZ0xjY/DBfMy4hKVJw//p/ta7mojPtZzgGj295nO2J7cybPI8Xdr7AcHqYLv/z83CNy08e/wm9sd6yhd5TW2317569exYVen9Y8Qee2f4Mx8w5puj1p+x1Cm/d561lPUuZ2HRGOvne8d/jzjV38t6F7231cl4RfPqQTxMixLv3e3fxf+MTEBHhG0d/g98t/x3nHXxey9YRCUX41nHf4qrHriLpJPnwgR9umeisNyr02pRNfQmO3KdEqNV39FIDtjhhy3IrVLatsOHIFXfAge/IVWBO29dWibYSv7XKpN2sMLvt/+UaFfuhW4DDPpj7826HwBGfgAd/ZF/77VOCBJ2rbI7eGI7e+scAA3MC+RbZgopNVjBP3bv0rNyFp1uhN31Bbl/PTFt1OwYJJ0FnpJMr16yEI94GJ44Op31u8ecwxvDmP7yZJWuW2HDq6d+DHx3Jf+z8JwM9tgJ3JDNCdzRXjbt1ZCsZN8OMzhmET74EgB2JHSSdJNM6phEN5/6P1hjDlpEt3LryVqKhKB876GN8+d4v8+z2Z0floDy59Um2jGwpu2Jzy/AWlm5aSm+0l0UzFvHY5vxqb2MMS9Yu4YjZR3DliVeWuIvSTrxm+mt4zfTXtHoZrxi6ol184fBxKuMnIHN65vDFw1vfLmbe5Hl89/jvtnoZdeeVIfmVinBdw+aBZOnWKsmB3J+fvN6KPLC5ZyvvsQJw4Vty50xf0PrK26yjNxum7gWzFnktSaaO3c7jhK/YxsST97Au2lhEvM9rLEdvvVfNuVtA1PgtUgY3W6HnF1sUY3/vc525f25f94xxHb1kJklHOGbD0r27lTxPRDhpz5N4cMODDKYGYepc/vG69/GXnm72jNtQ8qA/6g248YUbOeG6Ezj5hpP58G0fxnEd/rjijxx/7fGcfMPJvPvmd5PI5BpmX/bQZZx0/Ulc+9y1HDH7CI6YfQRQPHzr92wbzozfl/CKZVdw4vUn8ucX/8yB0w9kanzqKEdvxc4VrB1Yy0l7njTu/RRFURSLCr02ZOtQkoxrslMxRhGcdvD4NXYbjtvcs2dvtmO5/EIHsEJv5+rWTsjo3wBILkzrC9ExRA8A8R74wI22sXEpl81HxIZvx3L01i2zkziChSndntDrX28F6dQxWlFMnw/n3AqHnZPbV6w9SwEJJ0FcPGds0uwxzz1xzxNJu2n+sf4fDKeHubT/SfbunMlHDzsfgKHMUPbcJ7Y8QU+0h48c+BEe3fwoV/7zSr7zyHc4eMbBnH/o+byw8wV++oRtBv3Ixke49rlrOX3e6Vx81MV87civMb1zOrO7Z2dDrj7GmGyxxsg4xS1Pb32anz/9c07e62QuPupiLjzyQibFJjGYHiTj5qpnl6xZgiCcsMcJY95PURRFyaGh2zZkc79trbJrSUfPE3qxHtsWZPKeVpy8/LAVVPNPzs/Rmj7fun7bX4RdWxRGGVhvnS8/jLj/GfC3b44reoD8AozxiHaO7eite9T27AvS44Vu1//Tfk5jOXoAc4/Of909w1bd+pWxC04dJUqTTpK4X6E7jrg9ZMYh7NKxC9csv4Ylq5ewbmg9P3/TzxlKW4E3lMoJvZX9K9lnyj6cf+j5PLvjWf73qf8lFopx6TGXstekvVjZt5JfPPULpsSncP3z17N7z+5cfNTFdERyf7cOnH5gVugZY7h33b08v+N5VvevZmbXTDYPb84KtvvX38+xc45FvPeXcTNc8sAlTOuYxiWvv4TeWC8Ak+K2MnggNZAtrFiyZgkHzziY6Z3Tx/58FUVRlCzq6LUh444/Sw7Y/m5+ntic19q8tw2PW2dp4Rn550/d2253rm3Qisugf0O+qJt5gJ2IMfvg+j4n2gWpEo5ectDOt521KH9/fJLniC61r8cTeoX0zITMiG1YfM2/2O+h8NGZJB2+0BtH3IZDYU7b+zQe3fwof1n1F87e/2wWz1qczcsLOnor+1ay9+S9EREuOOICdunYhc8c+plsg9wvLP4CM7pm8N2l32Xj0Ea+dtTX8kQe2IKQlwdf5v7193PH6jv41F2f4j8f/U86I52cvrfNixzJjPDA+gf41F2f4vEtuff31NanWL59Of922L9lRR7A5KcX9NIAACAASURBVPhkIFd5uz2xneXbl3P8HgGnWVEURRkXdfTakOz4s5Kh2z7bS236AptzNucwG44ECEVtc98gflGGaeEQ7YGNMHlO7rUIfPTu3NrqRbSzdOh2ZIfddhc4SiJWrPnzYisVen7od+nP7DaTHHVKwkkQ93Mpe8ZvM/Hvh/875y46l5CEso5YVuh5zl5/qp+tI1uZN3keALv37s5dZ92VVzwxtWMqN7/jZgZSA3REOvKKOHzOXHAm1zx7Dd944BsknSQLd1nIj9/4Y7oiXdy68lYAhtPD7Eza/o0v7HwhW822dsD+z8NB0w/Ku+fkmCf0vF56vmN4yIz2qIJTFEVpFurotSGb+hOEBKb3lGiRkeyHeG+uIe6cw+wP2Ny8UQ11vb8mxjRmweUwsD6/uhbqL/LAy9ErEbr1hV6xsXHdM6wQlrCdKFIJfuh3iz/KaPTnnHSSdDgZ+5xIbNxbigjTOqfl9ZPrifYAOaG3ss+O9tl78t7Zc4pVyMbCMaZ1Tisq8sCODrrwyAt5efBltiW2cfFRFzO9czpd0a7saLSRzMio5wKsG7Rj3Gb35H+3hY7eU1ufIiQhDpiWP51DURRFGRt19NqQvpE0kzqjhEMlig+SAzbceMDbbE+4OYttTt7iD+f3h/PJCj23cYseC9e1DZP9NiaNZKxijLGEnl95O2kOhCv8Z+U7ej5FPudEJkGvkx4tdivA73M3ltCrliNmH8H5h55PV6Qrrx2GL/SGM8MMpm2170t9L2WPrx9cz4zOGcTD+f9TUij0ntz6JPMmzxvVq09RFEUZGxV6bUgq4xKPjGHWJvq90O18eNf/5Paf8YPi57da6KUGATPaaWwE0c7cKLRCxnP0oPKwLeREok8R5zTpJImnk9Bbxf09CkO3L/W9RDQUZU7PnLEuK5tiTZF9YTaSGWHYE9BBR2/94Hp26xldXOKHbvtT/RhjeHrr07xhjzfUZZ2KoiivJjR024akMi6xsYSeH7otF78CtFVCz68SrmTN1TJW1W05jl41Qq9rOoQittcfFP2ck06SjvRIeVXGJYiH40Qkkufo7TVpr7IbGldD1tFLD2efu35wfbY337rBdUWFnl+Y0Z/sZ93gOnYkd3Dg9AMbtk5FUZR2RYVeG5LMuMTCYwk9L3RbLtlROi3K0fP7/lWy5mqpNnTbXYPQC0fgX66BN17s7Rj9OScyCeLpxPh9A8dAROiKdmUF16q+VXUJ245FV8Q6esHQrcGwun81juuwcWgju/eMzmkMh8L0xnrpS/VlCzFU6CmKolSOCr02JJlxiUfGKFRI9EPH5PJv2OpiDH+SR7NCt2M5epGO4pM4/IKKsZolj8WCU3L5d8UcvcwIHcbU5OiBLcgYSg+RdtKsHVjbcKEXLMYYTg8T8Zo+r+xbafvrmUxRRw9s+LYvaYVeLBRj/tT5DV2roihKO6JCrw1JOWOEbp0MpIcqDN22OEcv2WxHbwyhV8zNA5i+n/2cdq3BdRpDUCedFHFjanL0gKyjt25wHY5xsv3yGoWfozecto7evlP3RRBW9q3MVtyWEnqT4pPoS/axbNMyDph2ANFQtOh5iqIoSmlU6LUhybRTWuilPHesGtHUKqHnF0c0ReiN00evlNCbdSD8+0sw+6Dix8uhRC6k4zqkTcYKvTo4eoPpQbaObAVgRmdjK5kLHb1dOnZhTs+cPKFXqhhkcmwyL+x8gae2PaWNkhVFUapEhV4bknKKVN1uegbu+W4g360aR6/M0O3ITrj7cjvSqx40NXTbBW4anPToYyM7Sws9GPtYOZT4nJOObaDcYUxN7VXAVt4Op4fZltgGwLTOaeNcURsd4Q4EYThjizG6o93sPXlvXux7kfWD6xGE2d3F39Pk+GQ2DG0A7PxeRVEUpXJU6LUho9qrZFJww4dgyTdg6wq7rxLRVGno9qW74e/fsj366kGzq26hePh2ZAd0TGngw/2+h8WFXhypWUx2RbsYTA+ybcQTeh2NFXoiQmekk5HMCIPpQboiXRw+63Ce3/E8t6y8hRldM4iFizeA9nvpzZ00Nzu9Q1EURakMFXptyKj2Kvf/Z050rVtmt9VU3ZYr9DKpys4fj+SAXUOspz73G4us0CsSvk2M4+jVSonPOevohTty4d0q8YsxtiW2EZIQU+KNFK6WrmgXw+lhhtPD9MR6OHv/s5k7aS6r+1eP2cNvUsz+HT1pz5MavkZFUZR2RYVeG5LXXmVoK/z9O7DX0fb1uqV2W5HQq7CPnlNnoZfw+v7VKHLKwp+8UEzojeyAzgYKo6yhl+/o+T3nYpESs4srIBu6HdnG1PhUwo0YI1dAZ6TThm4zQ3RFuoiFY1x01EVA6UIMICtCNWyrKIpSPToZow1JBdurbF8JThKOPh82PAEve0KvmtBtuX30PAeqfo5ef3MKMaB06DadsOKvlY5epPbxX93RbuvojWxreH6eT1ekix2JHbjGpcdzZRfPWszlx17O/CmlW6acNu804uE4i6Yvaso6FUVR2hEVem1IXnuVbJPfXezIs/WP2teNDN36hQz1DN02Tej5jl6B0EvstNtmCL0CQZ1wrKMX90aY1UJ3tBuD4eXBlxtecevTGenMVvl2R3Lv4Yx5Z4x53fTO6bxn4XsaujZFUZR2R0O3bUhee5XgNIfpC3InNbKPXjZ0W/4jxiTR15yKW4BYidDtWFMx6kbxEHky4zl6dRJ6AGsH1jbP0Yt2sWVki31+rPb3oCiKopSPCr02JK+9Sp7Q88JkoUjx6Q4l8QVIuaHbehdjVDibtxZKhW6bIfRKCOqsoxevvRjFF3pJJ9nwilufzkgnfUnbCzHo6CmKoiiNR4Vem+G6hrRjco6eH3LsmJxz9OKTKitsqLSPXluEblvg6ElxQZ1trxKrXez2RHNisZk5ej7ddXAlFUVRlPJRoddmpBwrrvJCt/HJEI4EhF6FgqHSqttMnYsxEv3NC91OREfPq7rtqIPY9UeSQfOEnj8dAzR0qyiK0mxU6LUZyYwVCdmq22BLkF3mgYQrF00tL8ZoZuh2AuToFTZM9iaDxOvQrDnoqO3SsUvN9yuHoLjU0K2iKEpzUaHXZqQyRRw9X5xEYrDL3tbhq4RW9tHLJO39Wt1eZWSHFcmNFJylRqB5+W0dlX5vRcgL3TYxRy/7/GY0vVYURVGyaHuVNiOZsfNl4+EiQg/gpIug0sa7FffRS1V2/lj4s3k7ahc5ZREZQ+h1Tmls0+YSOXqJlP0M4nUItbYidBvM0euqQy9ARVEUpXxU6LUZvqMXjwaE3uQ9cicc8NbKb9rs0G1qCBPuYMRN0tXMObdgcxnDseKh24aGbSndMNkP3dbh+UFHb2pHg9+Ph+/ohSSU5+4piqIojUdDt21GthijlKNXDRULvRqKMdIJ+P7+PPTg9zj298eyY2Cd3d+s0C3Y8G2qQOgl+hrvKkrxHL1EapCYawjV4fnxcJywhJkSn0I0FK35fuXgu4jdkW6kGWPsFEVRlCwq9NqMZDqQo+e6dZrP2sQcvdQQJPrY0L+alJuib3Cj3d+sqluwBRmFjp6ThnC8wQ8u0TA5PUTcmLqIXRGhO9rdtPw8yDl6wbCxoiiK0hxU6LUZKcdlFtuYufVhSA1Y0VA3R6/SPnpV5Oi59tqMydiXKRu2bFroFqyjV5ij5zoQCjf2uaVCt+lhOoxbN7HbHe1uWn4e5PLygmFjRVEUpTmo0GszUhmXj0Vu4TV/+wgMb7M7my70anD0PJHouLaoxEkO2v3NDN1GOsDrXZfFOIGilAZRqmFyJuE5evURu6+Z9hoOnnFwXe5VDp1eJbM2S1YURWk+WozRZiQzDlNlgJCThA2P251Nz9Grh6NnhZ7rFSI0reoWbBuVwrW30tFzRuioU+gW4Acn/KAu9ykX39FToacoitJ81NFrM1IZl0l4+WVrHrLbmoVeEydjODZkm3X0WhG6FRm9djdjZwQ39sHetqAYw0kSJ9R4odkg/Bw9FXqKoijNR4Vem5HMuEySIftibZ2FXqV99KoRep6jl/YcPZMatL3tws2pEAWss1a4duNYp6/Rz4Uijl6KeKPDxg3EL8LQYgxFUZTm88r97aEUJc/R2/iE3daj/1sx8VOKWvroOfmhWyc12Fw3D4q/V9dtYui20NFL0SGv3CwL39HTYgxFUZTmo0KvzbCOnif0XBsGpQ4zUisTerU4evmhWzc1PDGEXlOLMQocPZMh3qSed42gM9JJNBRlSrwOfw8VRVGUinjl2gRKUayjN5TbEe2CaIUjz4pSJG+tFHWous06eiZtJ1U0k6KOXhOLMQpIGoeO0Ct3okRIQlx98tXsM2WfVi9FURTlVYcKvTYjk07SLUlM5y7IyPb6je2SUOXtVaqZdevl6DlYoeU2I2RaSKty9Eo0TE4Yh3ik0c2aG8viWYtbvQRFUZRXJRq6bTNMwqtS3f1wu61H2BaqDN1WIfQ8Ry/tekLPuI0PmRbSMkevROgW84oXeoqiKEprUKHXZoSSOwAQX+jV1dFrQjGGl6OXC902QWAVUsy9dJ3Gt1cp0TA5AcTD9Qi/K4qiKK82VOi1GeFkv/3Drq+xbUlqnnPrIdLcyRh+w+SmhEwLKNZHrynFGKPbqxgnQ1IgHtHWJIqiKErlTBihJyK7i8jPRGS9iCRFZJWIXCEiFVlSInKMiNzoXZ8QkTUicquInNqotU8kwilP6HVOhSM/AQe8vT43lhDN7KOXMYHQbdMdvWINk5tZjJH7nDMj23FF6Iip0FMURVEqZ0IUY4jIPsD9wEzgRuBZ4HXA+cCpInK0MWZbGff5BHAVMAT8EXgZ2B14J/BmEbnAGHNZY97FxCCSDowMe+NF9btxMfFTDCeTO6+WyRgEc/Sa3FpkAhVjJIa3AhDXqRKKoihKFUwIoYcVZzOBzxhjrvR3isj3gc8ClwHnjXUDEYkCl2NTmg4zxjwXOPZN4J/AV0Xku8aYZP3fwsQg6jt69Z4NW26OXrbilppm3aa9ZzmtqrotdC+bOus29+zkyHYAOmLabFhRFEWpnJaHbkVkHnAKsAr4UcHhi7Du3PtFZDxLYxdgMvB8UOQBGGOWA88DnUBb/8aMZgKOXl0p19ELCr1a+uh5jh5NyI0rpKij5zZhBFoRRy89CGiOnqIoilIdLRd6wIne9nZj8n+7GmMGgPuALuDIce6zGdgCLBCR+cEDIrIAmA88Vk4I+JVMLD2AQwhidQ71ldtHz6+4hZpy9Bzf0WtJjl6x9iqZJjp6uWcn0yMAdIS1vYqiKIpSORNB6O3nbZ8vcXyFt10w1k2MMQb4FPY9LRORX4rI5SLyK2AZ8DRwVqnrReRjIrJURJZu2bKlojcwkYg7AwxJT84dqhdVhW6rz9HLeDl6phlOWiGt6qPn5+gFwsaJjB1nF4++cidjKIqiKK1jIuTo+THGvhLH/f3j9gkxxlwvIuuBa4APBA5tAn4OvDTGtVcDVwMsXry4iuSyiUFHZpDhUDeT6n3jsoVeIP2xJkfPeNsJ4ug1oxijSI5eImMdvc6ICj1FURSlciaCozceo22OUieKvA+4E7gX2B8b8t0fuAv4L+D3DVrjhKHDHWQ41Fv/G5dddRsM3VY/GSOTV3Xbihy9VhRjjG6YPJL2hZ5W3SqKoiiVMxGEnu/YlaoemFRwXlG8PLyfYUO07zfGPGuMGTHGPAu8Hxu+PUtE3lD7kicu3e4AiXAD6k3K7aMXDN1WM+vWH4HmO3qYFgi9AlFrDGCa6Ojlnj3iOXodEZ2MoSiKolTORBB6foVsqRw8v7CiVA6fzylAFPh7kaIOF7jHe3lYNYt8pdDlDpGINEjolVWMUWOOnh+6DTp6rQ7dunZKR9McvYBAHnGs0OvS9iqKoihKFUwEoXe3tz1FJN+6EZFe4GhgBHhwnPv4ZYkzShz396dKHG8Lus0gychECd3W0l7Fih13IhRjmCYJPfvw/PYqmQQAndowWVEURamClgs9Y8yLwO3AXGzVbJBLgG7gV8aYIX+niCwUkYUF597rbc8UkYOCB0TkEOBMrFWypH6rn3j0mCFSjRB65fbRy9RajOFPxggIvZY7ehlvfxPWUeCcjnifZ2dUHT1FURSlciZC1S3AJ7Ej0H4oIicBy4EjgBOwIduvFpy/3Ntme4gYYx4WkZ8DHwIeEZE/AquxAvLtQAy4whjzdAPfR2vJJOkgRTpS95rb5vXRKyjGcJqRG1dIq0K3MMo5HXaso6ezbhVFUZRqmBBCzxjzoogsBr4OnAqcBmwAfghcYozZXuatPoLNxTsHeBPQC/QD/wD+2xjT3lW3CTv+LB1rROi2SX30XF/oTSBHzw/dNs3RCxRjOEliriGsDZMVRVGUKpgQQg/AGLMW68aVc27RbsBe0+RfeD+vPhK2MDkTq/f4M5o36zaboweI10+v1SPQXO/PzcrRCzZMdpJ0GhdC0SY8W1EURWk3Wp6jp9QPd2QnAE6sEaHbJhVjuP5kDCt2TMscvYBIzTp6TfjnUujouSk6jYGwCj1FURSlclTotRGZHWsASHdOr//Nq+mjV4Oj5xdjtCZHT0rk6DXBAC8sxnBSdLim+WJXURRFaQtU6LUR5uVlJE2Ewcn7jX9ypTR5BNqEytHzq26bVowREHq+o6ehW0VRFKUKVOi1EbLhnyw3exGJNSBxv2lVt37o1ns5Eapum1CMcemDl/LwhodHOacJN+3l6E2YdFpFURTlFYT+9mgXXIfIxsd4zD2arkgj9Hu5OXr1rbo1pgVhyya3V3GNy7XPXUtHuIPXFXzOI26aSa7m6CmKoijVoY5eu7D1eULpIZ5w5xFvhNArCCmWpE6zbvMdvRZX3fp/bpCjl/TC3SOZkSKh2zSdhsB4NEVRFEUpHxV67cK6ZQA8bvYhFm6E0Cs3Ry9d2fmFZCdjeC8ngtDLOnqNWUfSm34xnBkeVQgyYjJ0NuSpiqIoyqsBFXrtwrpHcaK9vGRmE4+2UOhlkrZwQMJVT8ZwAdczsBwmQOjWNLbqNuFNv7COXmi00CveNlJRFEVRxkWFXruwbhlD0xdhCBELN0AYld1HLwXhWA2OXjobtgVwTauKMQJh5wbPuvVDt8Pp4VHFGCOuQwcq9BRFUZTqUKHXLmxdweDkBQBEww0QBmX30UvbwoFqhZ6TJhPIR3OF1jt6DS7GSGQCjl6gGMMYQwKHTv1nqiiKolSJ/gZpF5wUabFtVaINKcaoYARaJF5+O5ZC3AyZgE51ofUNk5tUjGFz9HKfW8JJYIDOZucoKoqiKG2D/gZpB4wBN40jNocs2oiigUqKMbKh2+qqbp1AqNJBGlYEUZJmF2OMqrp1c69BHT1FURSlavQ3SDvgCQPH+zojjQjdUm57laQXui0zp68QN53v6AnNd/QKewY2uGGyH7otzNHz96ujpyiKolSL/gZpB7K95zxHr1E5euX20QvHqhd6ToZMwNFzoUU5esFijMZW3eY5egFBnXP0dM6toiiKUh0q9NoBf5qE5zhFGhK6LbfqtsZijFGOnkycEWiNKsYItFcxAZGZFXo6/kxRFEWpEhV67YDX/iNjPKHXMEevCe1VCqpuHZgAVbcNbq/iNUw2GBLC6By9poeuFUVRlHZBhV474HhCzwvxRRsyGaOSPnrxmoSeUxi6bcVkDEwufOt676PBjh7AcGh0MUaHqKOnKIqiVIcKvXbAd/T8YoxQC/voZVK50G01s24LQreOSGscPcgJvWwxRmP+uaQC84FHAp9bLnQbbchzFUVRlPZHhV474OXopY11fiItnXXrhW4LK1fLxUmPLsZoRY4e5Nbf6IbJQUevWHsVFXqKoihKlajQaweyVbf262xc1W2lffQqFHrGgHFGN0xuuqPnLcBff4Nn3fo5egAjodFVtx0q9BRFUZQqUaHXDniOk5+j15Cq23IdOicQuq20YbIvWPNGoEmLcvQY7eg1eDIGwHCgGMPvo9cVijXkuYqiKEr7o0KvHfBCtynjF2O0uI9epMpiDO99OAHnzIFXVeh2RITCHD119BRFUZRqUaHXDmTbq4QIhwSRVjdM9idjVOnohXMOVssaJsOo0K2D4cO3fZj71t1X18cFQ7eFOXodBkJhFXqKoihKdajQawe89iopE25MxS1U2F6lyhw9X7AGhM1ECt0Ou2ke2fgIT219qq6PSzgJeqO99hlCXo5eh6FhuYGKoihK+6NCrx0IhG4b0kMPqhB6VVTdZh09K/QiEm5dw2QY5eglPcEXzKmrB0knyZSOKYAXug0IvU6wDqmiKIqiVIEKvXbAc8LSJtSYqRhQfl+8WkaguflCLxaK4AoTJkcvZQPJpL111otkJsnU+FQAhsXkhW47DaA5eoqiKEqVqNBrB5yco9eYilsq7KNXZTGG9z4cT+jFQ1FcJk7D5KRpjKOXcBJ0RbuIhWKMCASLMTpd0/z3ryiKorQNKvTaAc9xSrvSmIpbKE+4ua51F+uUoxeTSIsaJhf00fNGoCX9EHlgkkU9SDpJOsIddEY7GQ481+bouRq6VRRFUapGhV47EMjRa1jotpw+eqkBu411VTcCzc/R8xysWCiMI0CjXMpSjArdWgGaNF7RS52FXiKTIB6J0xXpso5eMEfPdbUYQ1EURakaFXrtgCeQkiZEtKGh23GEW/8Gu+2dXV3D5GyOnhU2MQnb0G2rc/S8kG3KF3wNKMaIh+N0RjoZCeToJTIJOo3RHD1FURSlalTotQOeAEm5DS7GGM/RG1hvt72zqWrWrdcmJhMKCj0amqN2+UOXc/uq2/N3lijGSLqNcfT80G1XpMuGbj0ndCg9RKfrQFgdPUVRFKU6VOi1A26wj94EcPQmza6uvUp2MoYXuhUvdNtAR+/Wlbdy//r783eWaq+SdfbqH7qNhWN0RnOO3rrBdWwa3sS+yZSGbhVFUZSqUaHXDvghRTfUwGKMMoRb0NGroeo26+gRanjVreM6o9uljHL07DblndeI0G1HxDp6I95z715zNwAnDQ1p6FZRFEWpGhV67YCfo+dKYxsmj1dc0b8BOqdCtLO2PnqeKxkTaXjVbcZkRodiSzZMrn/oNu2mcYxDPBz3QrcGjOGuNXex75R92TOTUUdPURRFqRoVeu2AJ5ASbgOrbsvK0dsAvbsFzq+06tbP0fNCt/ih28b9Nc24mTIcvQwg2ZBtPYWeP+fWb68ygmGHyfDo5kc5cc5x9iTN0VMURVGqRIVeO+BPbmioo1em0Js0u/zzC/EdPQkjxhABL3TbuL+mjhkrdOsJVdeBUDgbsq1n6Na/l99eZRiXvzOCa1xO2u0Ye5KGbhVFUZQqUaHXDmRDt2EioUb20SsjdNvrC73qZ906oRARIIxpaOjWNS6ucYuEbgsaJhsHJCf06uroOQFHL2IdvbtkmNnds9l/8jx7koZuFUVRlCpRodcOeLljCTdEpFWOnpOBoc0BoVfDZAwRIsYQMthZtw0qxnCyE0XGC906EIpkBV49q24TTgLA5uhFu3AE7iPBiXueiHi5gRq6VRRFUapFhV47kK0GbeEItMFN9nhe6LbKyRgiRAyEjItTpGHyA+sf4EeP/aiyexch4026SDvjCD3j1hS6fXrb01z64KW4RT4/P0cvHrENkwHSAifucWL281BHT1EURakWFXrtgJNrr9LYPnpjCL0BfyrGbuWdXwzPMctIiDCGEMUdvTtW38Fvn/ltZfcu9riSjp4vlgM5ehLKOXoVhm7vefkern3uWjYMbRh1LBi67Yp0ATDZCIfuemjW4dQcPUVRFKVaVOi1A27GChGXBlbdjtNepd/roTcpkKNX6azbvNAthI3BgVFVt4lMIuvG1YLjN0Aer72Km8lz9FJOClOBWzmUGgLgpZ0vjToWDN36jt7xboxIKJITemEVeoqiKEp1qNBrB9w0hKJkHNO6Wbd1cfS8YgwRIhhCxrVSscDRSzgJMm7tQs938sbN0SsoxjCYks//3fLfcdZNZ+XtG8pYobeyb+Wo87PtVSIdTIpNAuAkN2YPZkO3TZ71qyiKorQNKvTaAa9YIOO6reuj17/ehhi7pvkXVN9eBQj7OXoyOkdvJDNSF6HnuCVGmhUtxgjnOX+l8vRe6ntplHPnO3or+0cLvaCj97rZr+OK0O6c4Hg5eRq6VRRFUWpEhV474KQhHCHtmMZOxhgvR693Vq7nXS0j0ICoMYSM116l0NHLJDCYrFCrFj90W14xRiRP3JWqvB3JjJBy80O7vqNXLHQbzNGLhCKcFJ6EZPv3aTGGoiiKUhsq9NqBbOjWbV0fvaGt0D0jcHr17VUcsT30bI7eaEcvkbEuWK15euO3V8kvxsgTeiUKMoqtbShthd6q/lUlz49H4oFnB54LmqOnKIqiVI0KvXbAtfNQ065pXR89N5MvSKp19CRE2hjbXsV1MUWqbkcyIwA1h2/TplSOXpGGyYFiDCgduvVDsUGX0Bd62xPb2ZnYmXd+djJGOO4/PCcwtb2KoiiKUiMq9NoBx4qstOO2ro+ecfOdt2r66HnOpIPJFmMUrbr1xFStQi/r6I0XunVtMUbQxRvP0QuKx6H0ULbQojBPLxu6jXR4zw4IPQ3dKoqiKDWiQq8dcDOYUBhjaF0fPa9gIf/8ShsmW8GaMQ5hA2HjFs3R8x29UU5chfg5ehmTyW9mPEZ7lbAnZisVegdOPxAYXXnrnx8LxXLPDj4XNHSrKIqiVI0KvXbATWO8ysyW9dEzTr7zVs2sWzdtq4eNm3P0RIr20YP6OXpQIBqLFWN4jl5vrBcoHbodzgwD+UJwKD3E/CnziYVio4Re0kkSD8cRP1wc/NwcrbpVFEVRamPCCD0R2V1EfiYi60UkKSKrROQKEZlaxb0WicivRGStd6/NIvJ3EflAI9becpw0xgvvtSx0O8rRq0LoOWkIR3GMmx2B5kJeSNgYkw3d+o5ctQTFXZ5DV6K9StJJZoVeqarb3Q64NAAAIABJREFUQkcv7aaz1+01ea+ijl4uP4+CYgzto6coiqLUxoRI/hGRfYD7gZnAjcCzwOuA84FTReRoY8y2Mu91DvA/wDBwM7AKmAIcCJwG/KrOy289roPxxFBDQ7dgw7FSREx6TYXzzq/K0YuSMS4dxhB2Hdyw5AmdtJvOhllrdvRMuY5eTuhNiU8BxgjdeiLUPz6ctg5fd7SbXeK70J/qzzs/6STpCHcE9oiGbhVFUZS6MSGEHnAVVuR9xhhzpb9TRL4PfBa4DDhvvJuIyJFYkfcUcKoxZmPB8fb8jRkI3TbU0YPRRRc+xi0I3VZTdZuBcIQMLhFs1S1hcJGs9ezn50F9Q7fjOnoSJuUMZ4sqSlbdFjh6fsVtd7SbzkgnfcN9+ec7iVxrFf/ZWnWrKIqi1ImWh25FZB5wCtZ5+1HB4YuAIeD9ItJdxu3+AwgD7ysUeQDGmNqy9ycqbgYjVgw0rL0KfsuREnl6rju6GKPiWbeeo+c6hI0h5DluTkC7+kIKahd6wV535Tp62dBtmcUYg+lBICf0gkLVv09+6FbyBSao0FMURVGqZiL8BjnR295uTL4FZIwZEJH7sELwSOCuUjcRkd2BY4GlwNMicgJwGFZtPAbcXXj/tsHJ4GZDt40sxqC0SzeqGKPKPnpe1W0EW3UL4AYEox8ahWYUY+QaF7sSIu2mxxR6aTedFY/FQrcdkY6iQi8WjhU8uyBHT0O3iqIoSpVMBKG3n7d9vsTxFViht4AxhB5weOD8JcAbCo4/KSLvNMa8UOU6Jy5uGjcbum10jl4J8VZYjFHVrNuM7aNnXCLGEPKEWLD1SdDRq7W9SlAo5vXSKxS1rkPKy33sifUA+aHb6567jkNmHsLs7tmj1jauo+emcq1V7MMDVbcaulUURVFqo+WhW2Cyt+0rcdzfP2Wc+8z0tu8G9gfe6d17X+DXwCLgFhGJFbtYRD4mIktFZOmWLVvKXfvEwM3kijGakaNXjHoUY3gzezPG8SZjjBZ6dc3Rq6AYI+ntK3T0/rb2b3zjwW9ww/M35ItQp3iOXqHQSzvp0Y6eb2C62l5FURRFqY2JIPTGw1cu4yV8hQPbc40xfzTG9BtjXgQ+iA3pLgDeVexiY8zVxpjFxpjFM2bMKHbKxMVJ4/o5eg2ruh3na6hHw+Rs1a1j++j5s28DgiwvdFvjrNvg9eMVY6S8fX4xRspNMZwe5rKHLgNgMDVY1G30Q7c90R46I51k3Myoti7RUqPjskJP26soiqIo1TERhJ7v2E0ucXxSwXml2OFtk8CtwQPGGINt2wK2bUvb8NuHVrNp5xCONKGPHjTY0QtOxjCEXS9Hzy0eum1ew2SHpJf72B21NUFJJ8nPnvoZm4Y20RvtZSA9wIiTc+t84eiHbruiXdkxZ8H3MCp0K4wWepqjpyiKolTJRBB6z3nbBSWOz/e2pXL4Cu8zUKLowheCnRWsbcJz/wvbGEkmcsUYLcvRK6y6lcodPSdphZ7reO1VRjt6wdBnUKhVQ1AoluvodYQ7iIVipJwUz25/lgVTFzB/6nyG0kNFHb3C0G3hexizGCObo6dCT1EURamOiSD07va2p4jkz7oSkV7gaGAEeHCc+zwBbAWmi8iuRY4f6G1XVb/UiUfacQmZTDZ0G21Y1e14jp6b30i5Gkcvk4RwHMc4RLyGyQAmWHVbT0ev3Bw91yHpvbdYOEY8HCflpNiR3MEuHbvQG+sdFbr1J2cMpYfoCHcQCUWKCr20my5djJEN3WoxhqIoilIdLRd6Xg7d7cBc4FMFhy8BuoFfGWOG/J0islBEFhbcJwP81Hv5H0HRKCKLgHOADHBDnd9CS8m4tt+cQ4MdvfH66I0K3VYzAi0FkVi2GEO8HLqgc5cnkmpsi5jn6LljOHrGIem9/Xg4TiwcI+kk2ZnYyZSOKfTEehhIDeTlDwaLMbqiXQB0Rew2TxAWc/SybV00R09RFEWpjYliFXwSOwLthyJyErAcOAI4ARuy/WrB+cu9baF99U3gJOADwCIR+RswA1uA0QF8vt3aq6QdlwgOGZpVdVtJMUZ1jl4maYsxwr7eCbZXqWcfvaCj54zdRy/o6MXCsayjNzU+Fcc4DKWHRjl1YIVeT9S2ZPFz9PJCt26KaDA0KwXtVUKR4iPnFEVRFKUMyrZ/RGQfEfmAiEwrcXy6d3xepYvwXL3FwC+wAu/zwD7AD4Gjyp1za4wZxgq9S4AurEP4VqyIPM0Y8/1K1zbRyTiGMA5J14qsaKOrboPi7YnrIeHNbq1LMUYKE46RMRnCJveXM6/qtp6TMQLX592rqKNn3388HCcejjOcGWYgNWAdvWiPLcYoIfT8Ag4/dDucGc6eN3bD5Izm5ymKoig1UYkq+DLwPaC/xPE+4LvAF6tZiDFmrTHmQ8aY2caYmDFmL2PM+caY7UXOFWNMUZvDGDNsjLnYGLPQGBM3xkw2xrzRGPOXatY10cm4LlEckq79OJrWR69/A/zhXFj+Z/u62Ai0Khw9x6swjWAIe4LHmAbl6JWcdVukYbJ3yA/dbh7eDMDU+FR6Yj1k3Ax9yVxheDB0Wyj08gRh0T56gRw9rbhVFEVRaqASofcG4M5S82K9/XeQG2mmNIGUY4jgMOLYr7JxkzEK+uj5kyEy3raYo1fprFsnhROxosfm6Hm7S1TdNm/WrZvN0YuFY8RCMTYNbwLIOnoAW0e2Zm8RLMYoFHq+WHWNS8ZkihRjBKpuNT9PURRFqYFKVMEcxq9YXQPsVvVqlIrJeDl6Cccqkab10fPdsODrYNi4mobJmSQZr8I0Yky2A3Zhjp4vmIJC75+b/8lF91+U5/6NR56jV7QYI5ejl/JEq+/obRm201N8Rw9gy4jdFwvFihZjFObo+S7i6IbJGrpVFEVR6kMlQi9FrnlxKXqp2MZRaiHjOXrDnqPXtD56vkjyxVaho1fprFtjwEmS8URPFEPIEzyFOXq90d5R++9bdx9/WPGHUSPGxiLP0StajJELoQYdvbjXAgZgasfUnKM3vJVoKEpHpCPP0fOPF4Zu/XPyGyYH26uktbWKoiiKUhOVqIKngNNFpKjF4M2QPQN4ph4LU8oj7ThExGUk4zl6zeqj54usoLMXbINYaXsVT2ilPQcraiju6GUSWQctGG5NeqHkYKHDuI90HcISRpB8R4+CHD3j4AWos46ez9T41LzQbUekg1g4VrQYw2+vUujolS7GcCCsQk9RFEWpnkqE3m+APYHrRGRW8ID3+jpgD+BX9VueMh7Gsa7UsGdONa2PXtDJg9rbq3hCLR32qoeNyfbOCQq9EWckK/SCoVs/782fLVsOGZMhEooQDUXHbZhcWIzhMyU+hd6YdRi3jGyhM9xp7+ekybgZEk4iK/Si4SgRiWTX6ruIeUIv+FwnraFbRVEUpSYqsQuuxvajextwsog8AazD5u4dhG1ncifwk3ovUhkDTywMZppcdesGBJ4xgKmtvUrGSqm0JxYjpkQfvUyC7kg3guQJvWodvYgXGh0zdGtckl5lSDQUJR6OA9AT7SEajmaFXH+qn6mTpgI2LBscf+bTEekYFbrN76MXzNHT0K2iKIpSG2XbP9782NOAbwFp4Eis8DsSm7/3TeD0EnNmlQZhPLEz7OmUxvXRGyN064u+UY5eBemavqPn3SOKEKJ4jl5HxI4Uy3P0vEbKvrgqh4ybISxhWzwxrqPnEg/HEZGs0JsSnwKQdfTAzsKNhqJk3EzWXfRDu2Dz9MYN3QbFtLZXURRFUWqgIrvAa6HyFRG5AFgITAF2As+qwGsRTXf0AvljYAWfL8SksOq2EkdvtNArlqM3khkpKvSS3vWVhG4dYx29iERKCD3vvRqHJCYryHwHbmqHde8KHbuUkyLl5By9zmhn9nhnpDPrOpYsxkDbqyiKoij1oaq4kCfqtOhiIuBmQGDIF3oNK8Yo6KPnjufoVVqMkR+6jUqoZNVtZ6STiETyqmb90O1QpnJHLxqOjtMwOUPSmKyTV+joRUKRrFPXEenAYEi76ayg84swwAq9MXP08hw9zdFTFEVRamNCjEBTqsd4TlSGMJGQII2ai1oofkygvUrW0SvSMLnc8K3v6HnPiRLK/uUsLMboCJcO3Y6ky2+v4jt65RVjuFmB5wsz39GDXHjWL8ZIOalsiDYo9PJy9IqFboNtaXQyhqIoilIjE2YEmlId4lXdZgg3LmwLRXvL2ddj5OhB+UIv6+h5Ez5Eigq9Ujl62dBtFe1VYuFYgaMXeK9eoUkwdOtvfUcP/n/2zjtMyvLe3/czfbaxLJ2lihUsqEQUJIIUhSgqtkU5gsZE8BiikWMiFtCDidEYxSTHYKIiSuAnSEQpRjCIkaDGEgVREURAlroLW6e/z++Pt0yf7bO75rmvi2t23/q8swvz4fNtWJXAHofHyvkzw8iJodt6F2NEwqoYQ6FQKBRNQo1Aa8dIKcEIX4alveUKMaCOqlst/phUx9dFkqMnrKpbM3Qb1sKEtJAeum2mYow6HT3jOYNSw2PXJ1uYzl4qR8/j8OC069dL5ejVXYwRk6OnKaGnUCgUiqahRqC1Y8KaPhUDsuDoJfbRi2k9EhV6CTl6+gH1u7xZdWuILD10q59rOnpmHp4l9FLk6KUrxpBScuWrV7Lm6zXWtrAMW45e2vYqhsisleEkR6+jO4XQs+uOXjAStNxFcyKG+bWZo5e6GCPGCVXtVRQKhULRRNQItHaMPv5MF0Eh7C3YLJn0oVstHBO6bYqjZ4RuDUHpEMk5eqYTlipHzwzdpnP0glqQ7Ue3s+PYDmub2UevLkdPAjuCx+hX0A+ICd16kkO3XofXaq9iOXrO1I5e6obJMbmQEZWjp1AoFIqmoUagtWNCmoYDXexEsLfc+DOoI3SbohgjcYxYXViOnv6tU9iSGiabTlimPnrpcvTMc2NdwIiMYLelqrqNd/RKHXbKNT+ndT4NiAndpnL0jNBtMBKM5ug50uTopR2BhuHoqdCtQqFQKJqGGoHWjglHJM640G0WHT0Z00cvYzFGAx09U+jFhG7NHL04oSccKSdjmCIqkcSWJmDk6IlMjp4utra4dWF3audTAeiR2wOnzUlxXrF1SqKjZ+boOW3OuGKLVKHb+GKMGIGsQrcKhUKhaCJqBFo7JhzRsGcrRy+pj15M6DZtexUa7OiFzWIMYceW6OgZrl1iMYaUMtpHL03o1jw3VhyGZVh39DKGbjU+c7lwChsndjwRgCHdhrDx2o1xEzHiHD2bk6Cm5+jFunnm2sMyTCgSylCMASBVexWFQqFQNBk1Aq0dE9IkThEVetmpujXVlxZ9bRZHzwjdGt/GtlcxHb10OXqmyIP0xRiWoxcj6MwcveT2KjGumoywxe3iFE9XnIboEkLEiTxIKMYwijtqQ7Vx+XmgC0HQQ8zmWlKHbjXVXkWhUCgUTaZBykBKGZJSzgY6AacC5xuvnaWU9wIRIcRlzb9MRSrCEc2qug3JLDl6qfropay6bWQfPcMxdAo7duNraVxjf81+ADp5O+GwRceWxQq9dJMxUjl6Ean30Ut29ARm4+JIJMg2t4tTvd3JhCn8EkO3qRw90EWrKS7jQrexuY1aWI1AUygUCkWTaJYRaEKIvkKIm4EbgR6A+nTKAqGItEK3kdbK0YvroyfSH18XlqMXFXq2hD56Ww5vIdeZS7+CfjiEg4DUz6mPo2dW5cYWY4S1MDnOHKvBcdLzSo2vK7/BZ7Nxak7mrkHmvFuPw3D0tBA14Zq4HnoQFXr+sJ9gJIjD5sCWsv+g1N9jof4qKRQKhaLxNFoZCCHsQohJQojXgZ3APegib31zLU6RmbCmWcUYoZauuk3so2dV3YbThG4Tjq8L09GTptCzWf9bMHP0th7ZysBOA7Hb7DhsDiLGfU0RF1vRmojp6KUsxrA7CUVC/O2bvzF93XRj/brQ21r+BQCn5hQnXTOWAneBtQbToasKViU5embo1hf2EdSC8T30IKEYI6IcPYVCoVA0iQYLPSHEcUKIXwJ7gWXAWKAMmAccJ6W8qHmXqEhHOCKxG330Iq05Aq05ijHCAUAQkhFswobd5ohW3WoRgpEgXxz9wqp8jc3RM0VckacofTFGOH3o1mVzEdSCbNq3iU2lm3QBaQi9PdXf4pCSPt7OGZd/drez+fn3fs5Z3c6ycu4qA5VJOXqJodv4HnrETBdRjp5CoVAomk69hJ4QwiGEuFoIsQ7Yjj73tghYgW71rJRS3i+l3N1yS1UkEopoOI0+eiHsOLMauq2rGKMRffQcbkIypDtiMaFbiWT70e2EtbDVyy5VMUZHd0cCkYC1XZMa+6r3ATE5erF99IxiDDPfr7S6NHo9Q+jtqz1E93AYe6IgS8BpczJl4BScNicOo4CiIlCR5OiZoVxf2EdIC6Vw9OIrfpWjp1AoFIqmkFEZCCFOEEI8gt5GZSkwGvg3MBPoKaW8uuWXqEhH0gi01miY3GyOXhDsbkIRQ+jZbHFVt1uObAHg1E4xjp4h2ky3rshbBESbJq/dtZZL/noJx/zHUvfRM9qruOwuNKmxt2ovYFT3GkKvtPYgxeFI/BzfOjBDtxXBijpz9JxJ7VNiQt6yYfdVKBQKhSKRuooxvkRvnHYIeBx4Tkr5WYuvSlEvQhENh8hWMUaGPnpmq5VU7VUaMuvWoRcx6I6ezaq61TSNrUe20snTie65evVrbMPkWEcP9IKMAlcBXx39irAWpjxQnnIyRmzDZIhW9fojfkPoSfb5DvH9cMOqX81wrCa1tDl6teHazKFbKVWOnkKhUCiaTH2UgQTWAMuVyGtb6LNuo+1VnNnM0Yuruo3wkdvNexU7Yk+IOz6khXhh2wuW4ErCcPTCWjgpdBuREbYe2cppnU9DGIIztr2KlaNnOnpG5a0ZivWFfJYYjHX0rBFohtCThrD0h3Wh59dCHAkco2c43KBcudh2KRlz9DIVY6gcPYVCoVA0A3UJvfuA3ehtUzYJIbYJIe4SQvRo+aUp6iKsafHFGFlpmJwcupWRMPd1KWL+ntVpj//k0Cc88q9HWPX1qtTXj3X07E6w2a1fTk1q7Kncw3GFx1mHx+XoGVW3Re740O2+mn3W92Y1blwxRkzD5Fh0oSfYb/Tk6xlqnKMHJDl6ZhuWmlANoUgohaOn+ugpFAqFovnIqAyklA9JKQcA44G/AgPQJ2PsEUKsFkJck4U1KtIQisi4YozsVN0mtleJsLNmH3ucTnyRNGPEiE61eHPPm6mvHw7E5+gJu1V164/4CcuwNX0CUhdjpHP0akI11jGJoVuzYXIsZo5eqaZfR8/Ra6Sjl5Cjl+PIwSEcVAYrCWrBpHtbTqhV4KImYygUCoWi8dTLApJS/k1KeRXQG5iN7vKNB5agh3YHCyHObrFVKlISG7pt8RFoiX30YkK3fz/ybwAC6ebFEhVj7+1/j+pgdfLlI0HL0XPYHGCzYzduZbZMiQ2DpmqvYubo1YRq8If9HPEdAXThl64YI6WjZ+To7QubQq+Bjp4tvaMnhKDAXUBFoCJzjp75XqrQrUKhUCiaQENHoB2SUj4spTwevX/ecvTxpEOA94UQHwsh/rsF1qlIQVjTcBih23CLO3rpR6C9eeQTAAIyldCLunKg5+q9s++d5Oubjp5VjBGddVsVrALiRZPD5rAmZpih244eoxgjXEtpTal1bG24Nm17lVSOnpmjty9Si0PY6RJpWFFEbCVtYo4eQAd3h3oIPWOdLSreFQqFQvFdp9GfIlLKN6WU1wK9gLvQ++udATzZTGtT1EEwrOEwQrfhbPfRM0KLB7Qg22r24pCSYEz+W6LQM8WY0+ZMHb6NBPU+emaOXkzo1nL0YsKgsVW3sQ2TzePNsK35fbqGyQ6bwxJmnTydADN0KyjVfPR0F+l/SRoZuk109AA6uDpQGaxM00fPENQRY53K0VMoFApFE2iyMpBSHpFS/kZKeQpwIXo4V5EFwprEIXThVeB1U+BNzPdqRtJU3W4WuoAa5vMTMMaY6cfHO4CmGBtRPIKN327kUO2h+OuHA2CPaa9ii1bdVof0UG+saHLanERkBCklgUgAu7BT4NLHkPnCvjihVxuuTV11m+DoDSgcYJ2PsFEa8dHTEI+NdvQcmR29pD56SY6eEnoKhUKhaDzNagFJKd+SUk5pzmsq0hOO6I6etDlYNmM4N4/o33I3S+qjpwu9GiN0XBwKE9BCSDOHL0HomUJrxuAZaFLj4fcfjr++6ejFFGOYEidVjp7dEEBhLYw/7Mdtd1tCsDZUy77qfThtTlw2l56jlxC6lVIm5egd10Gv6rVCt5qfYqOStzkdvQJXgVWMkeToWcUYKkdPoVAoFE1HJQC1Y0JmMYbNyfFd8yjwZNHRM4Re2BB2uVJDQ0ZDo4nFGEbo9oTCE7jl9FtYt3sdb+19K3r9REdPRCdjmMUbiTl6oOf8BSIBPA4Pdpsdr8NLTaiGfdX76JnXkzxXXspiDDO/L5Wj54/48QkbZTJEsVHg0dhijEbn6EWUo6dQKBSKpqOEXjvGKsbIhhhIE7oNG6+5mpGLZzh3icf7I34cNgd2m51pg6bRLacbK3esjF7fnHUbE7oFsCOs0G1ijp5+/zCBSAC33Q3oYrA2XEtpdSk9c3ta3ye2VzGFnsPmoDivmCJPEWd3OxuHcOAP+zlsFLZ0c+brN2xk6Dalo+cuoDpUjS/sS99HL1EwKxQKhULRCNSnSDtGd/TCkDQvtQVIcvTMIhBd4OVo8bl4iSPQApEAHrs+/stpd9K3oK/V/kS/UDDq6Nmd1vkCkdHRiw3dgi4GTUevOL+YXGduXDGGOU0jokWFXvfc7my8diMndDwBj8ODL+yjypgbnG+6cw0IocY5eqly9FwdAD0XMLkYI6G9inL0FAqFQtEElNBrx+h99LQsNdVN6KNnhW41BOA1tgfNgoxERy9GjIFe4VrmL4te3nT0IgmOnohx9BL66IEu9MzQLeiTJzaVbqLcX05xXjE5jpz49ipaGCmlJfjsCQLO4/Dgj/ipMZy1PFOINcTRi83Rc6aounV3iB6bJNITHT0l9BQKhULReJTQa8eENQ2HiCCSpiu0AImTMczQLRKHsONJ6JeXOOs2VowBdPJ2oswXI/SMWbfRHD1d4NgQlihLrLoFw9GLREXkVSdexSlFp3B+8fmM7DWSXGduXI4e6OFbK0cvQcB57B78YT9VRv+6PPO9bUgxhiHe7MKeotgiXujV2V5FOXoKhUKhaAJqvlI7JhSRuEUE7Fn4MSY1TI46eg5suOpw9GLz6EAXeuYMWq/DGz/r1hYN3dqFDaSekxfrlJmOXkSLEIwErbBwyckllJxcYh2X48zhQM2BaO4gujiMDd3G4nHoQq+6CY6eKd68Di9CJDexNkO3QN0Nk5Wjp1AoFIomoBy9dkw4ouEUkeyEbtNU3UaQOIQt6uiFE3L0ZDRHLzF0C+iunqbpwsYe0zDZEFbCcAYTRZNVdStDeljYEb12LDmOHGrCNfjCPitfLqSFosUYIv698zq8+CI+qo3l55n7G1AUYa4tVX4eJDh6aYsxVI6eQqFQKJqOEnrtmLAmyRUBcOW2/M0S++gZQilkCL06Hb1wcugW0PP0TLfN4SKsheNCt3bjvom5bok5erEiMpYcZw7H/McAyHPlWedYOXqJodtER8901BogpoUQOG3OlPl5gNXYGUgav5bs6Km/ogqFQqFoPOpTpB0Timjk4QNXfsvfLEPVrV0IPFpCjp6IL96IzaODGKHnK9N76IHu6FnFGPr9zG56ie6YWURhFWPYPaQix5FjrSnfaJUSioSs0G1SMYbdFHrgQWDJsAY6a06bM62jlx/z80py9FA5egqFQqFoPpTQa8eEI5I84Qd3awi9aMPk+uboxYoxK3TrL9OnYgCa3UVYpnH0HOkdvUyh21xn1O20HL2YYoz0OXqQi916zobmyrnsrpQ99EB3EU2xV2d7FZWjp1AoFIomoIReOyakaeTgA3dey9+sjhw9d2LVbeKs2wQxFpejZzh6YcO9iuujZ7xmEnoZHb2Ylix5zmjo1pzgkSpHzx/xUy0k+disEHVjHL10oVuIFmTU2TBZOXoKhUKhaAJK6LVjwhGph26z4egl9tGz2quAg2gxRn2rbp12JwWuAl3oGeeEjbYkiZMxIHmUWLr2KonEhk9NRy8UCVkTMlK1V/GFfVQJyMOmF4pAoxy9dKFbiBZkpB+Bphw9hUKhUDQd1V6lHRPWNLzSB67Wc/T0PnrCCt0mV92mDt2C0UvPH3X0QkZensPmiPbRE6lz9ExHLxAJENbCGYsxTCxHT4bT5+gZkzGqkeRia7SzdtmAy+hb0DftflPopS/GaJyTqFAoFApFLErotWMi4TBeAuAuqPvgppKhGMOBqNPRS5VH18ljNE2OmEJP/3WMn4yRJnRrhFxrwjUAGdurmJh5cWEtQ8NkK0fPRhdENFeugWPmZgyekXF/2tAtCe1VVNWtQqFQKJqA+hRpx9gNkZPdHD0zdKsLuLAQOIhx9Oox69akk7cT5f5yfSoGEDJz9GxOK1fNcvScqR29mqAh9NI4enHFGIajF9JCVo5eoqPmdXiRSMqR5CKiIdRmnj5SYIjz9MUYKkdPoVAoFE1HCb12jMsUelkJ3Sb00TNDtwIcCBzoLlsqR89sUJwoxpIdPf0eejFGdAQapC/GqAnp70G9ijFcycUYiaFb8z5HhUY+Iiq4Gujo1YXZSy85R894VTl6CoVCoWgGlNBrxzjC1foXWWmvkjgCzQzdCkwp4rK7Usy6lQSMHLzYhsmgO3pVoSoChisXEjGOns3M0dNfk/roGftNoZe2vYoj6uiZodtQJJRx1q1JnrTpgkvYY4Ru82Dl6CUKSOXoKRQKhaIZaTNCTwjRSwjxrBCiVAgREEJ8I4R4QgjRsQnX/L4QIiKEkEKIec253raAM1yrf9EaffTMqlshjERGmgF9AAAgAElEQVRPgcfhSenomeIvlaMH6OFbYhy9uD56qXP0nEIXSBXBCiD9uLHYFiepijES26vEitE8iV4R3MxuHkCf/D44bU46uhN/vc2GycrRUygUCkXTaRNCTwgxAPgQuBF4H3gc+Br4KbBZCNGpEdfMB54HaptxqW0Kl9aKQs/qowcOKUHYdEcvnNxHL2CEZpOEnjUGzRB6xq9jvKOXOUdvT+UeAHrk9ki57Lj2KrE5ekZ7lVQNk63jzdBtM+fnAYzsPZI3rnqDjp4EoZfYMFk5egqFQqFoAm1C6AH/B3QFZkopL5dS/kJKeSG64DsJeKgR15wPdAB+1XzLbFu4IlnM0Uvso6clOHo2Ox57akcvbejWbJoc0GfRxufoGSPQ6piMsadKF3o983qmXLXX4UUYa7dCt1r6EWhee/Q+uRLdWbM3f3G6EILO3s4pdiS0V1FVtwqFQqFoAq3+KSKEOA4YB3wD/CFh9xygBvgvIUQu9UQIcRm6OzgTKG2elbY9PJHWD92GBDgkIOzxOXoxVbrmtsTCg0J3IQAVwSr9WsZ2h3BY59vT5OiZQm9f1T7ynHlWcUPSsoUgx5mDTdgssRhXjJGivYpJvjTaq7SAo5cWkRC6VY6eQqFQKJpAqws94ELj9Q0pTRWhI6WsAjYBOcC59bmYEKIr8CfgFSnli8250LaGSzPbq7RG6DZajOGQMrOjZ4RuEytjzfw5UwiGRIyjlxC6TRwnZo1Ak2GK84oRGYolch25eOwe65zYYoy6c/TCLZKjl5akYgzV6lKhUCgUjactCL2TjNftafZ/ZbyeWM/rPY3+XNObsqj2gNfM0WvFyRgRAQ6ppXD0ojl6Zt5eYo6eKfx8xn7T0YstxqhrMgakD9ua5Dhz8Dg80bFpMmbWbaYcPSkNRy+bYiuxYbJy9BQKhULReNqC0OtgvFak2W9uL6zrQkKIm4DLgFullAcbsgghxI+FEB8IIT44fPhwQ05tNTxaLSHhAkfidIUWILGPXkzVrV1KsNlwO9yZHb2EHD2zJUrU0dO360LPzNFLMxkjRnwV5xVnXLrX4Y1z9DJNxojN0cvTMHL0WsPRUyPQFAqFQtF02oLQq4sEhZHmICH6AU8Ay6SULzX0JlLKp6WUQ6SUQ7p06dLgRbYGHukjYEvdVqTZSZyMoUVA2AgDTilB2HHb3Cly9NK3V3HanDhsDvyGOAwZP+L4EWhGjl5i1a2ov6OX68zF7XBbPetCkVDahsnxOXqGo5c0pqwFSczRU8UYCoVCoWgCbSEByHTsOqTZX5BwXDqeBXzArc2xqPaAV6sl6Mq20IsJ3drdetWt1MBmT+PoxTRMTjG9wmv34teCIOyEDJctbjJGGkdPCIFd2InISJ1Cr4u3CxJpicOwDGOX+vXThW5tgFdKPUcvm6Fb1V5FoVAoFM1IWxB6Xxqv6XLwTjBe0+XwmZyFLhYPp0nMv0cIcQ+wUkp5eYNX2QbJkT6C9noXIzeNxMkYMgJ2l16MoWkgbLjt7pg+etFZt1YfvRTTKzwOD75IEBxuQoa40R09M3SbuuoWdJEWiUToldcr49JnD51NSAtFc/S0sOXkJQo903XMxY6wHL0shm6tHD2jGEPl6CkUCoWiCbQFobfBeB0nhLDFVt4aTY+Hozt179ZxnUXo1bmJnAB8H/g3elPmj5u84jZCjqwl6MiS0ANAxIRuw+BwERZg1/RiDLc91tGLLcbQtyWGbkEXev6gLqZCkRihZ07GsNkRiJTnOmwOApEAPfJSN0s2KfTo6Z3SWHtIC1kCLzF0a7ZhyQuHdVEbaa32KmoEmkKhUCiaTqsLPSnlTiHEG+i99P4b+F3M7geAXGCBlLLG3CiEONk494uY68xMdX0hxDR0obdaSnlvsz9AK5KDj5C9zhqV5kPY4kO3Lm9M6NZw9JJm3aYvxjC3+f3HwJ7g6MUUY+Q4c1K2T3HYHOS78tP20EtavhA4hCNjHz3QQ8x5EZ/+rFort1dRjp5CoVAomkCrCz2DW4F/Ak8KIUYDnwNDgVHoIdt7Eo7/3Hht3knz7QgpJbn4CDqyMRXDIFboSQ3sTsKEcWiRFI5efDGGQOCyJRc1eO1e/PII2F1RoRfXR8+elJ9n4hAOuuQ2rHDGYXMQioSstST20QNdfOaHQiBDuqPnyqJrKhLaqyhHT6FQKBRNoE2U9EkpdwJDgIXoAu9OYADwJHCelLKs9VbXNglFJHnCTySbods4Ry+M5nAhhdCFnk0XehEZ0QVbwgg0t92d0pXzODz4DNfMFHr6ZIxo1W1aoWdz0DM3cyFGIk6bU++jJ/U8vXRryhV2w9Fr5fYqqupWoVAoFE2grTh6SCn3oo8tq8+x9XbypJQL0QXkd4qwppGLj/3ObLtN0fYqYbsT8FmOnhmaDUaCOBP66KUqxABdVB2RYb0YIxLCLux6OHXAhTBsJlcNHMchf2qd/+PTf0zfgr4NegSHTQ/dRrRIUn6eyc2n3UzHDxaB3GFU3bZCMYYagaZQKBSKZqDNCD1FwwiFwnQQAcKtFrqNEDb6y5mOnjnL1h/2kxvbXiUSSFlMAXo+nE9GwO4lrIWtyljyu8G4/2VYhuVcc9I1DX4Eh81BSNP76KXKzwOYOGAifLIa5HbD0WuN9ioqR0+hUCgUTUfFhdopEX8VAFpWHT1bXNVtyHDpLEfPHnX0EnP0UvXQA6MYA80qxnC2sHvmtDmtyRip8vMszGeNBFun6lbl6CkUCoWiGVBCr50S8Vfqr9mYc2tiOnqa7upFzBYlWhiEiDp6EX9ce5VAOH3o1uvw4peaVYzhbOF8uPo4ekD0WSOtVHUbUY6eQqFQKJqOEnrtlIhPF3rSmUWhZ/bRM8KKYUMAOSJhqxgDEh09PXSb1tGze/AjweGK62/XUsQ5epnuJUS0GKNVJmOoPnoKhUKhaDpK6LVTpBm6deVn76am+DFGlYUdutBzymjDZDAdvfjQbbocPa/Ti09IpE1vmNzSoVvT0QtEApnvZTl6rTUZQ826VSgUCkXTUZ8i7RQtoAs92SqhW13omaFbBzKFo1e/0K3p9AWM9irZEnqVwUo6uNONVybmWVtpMoYW1teQepyfQqFQKBT1Qgm9dor0V+tfuLPp6JniRw8rhgwBZJfojp4h5vzhGEcPWWcxBoDf7shKjp4Zuq0MVNLBVQ+hl/UcvZgRaCo/T6FQKBRNRAm99or/GADSXb/xX82C2UfPaLESNtqOOGQqRy+hj1660K3RDNlvs2fN0QtrYSoCFRRkeu/iHL3WyNELqfw8hUKhUDQZJfTaKcKnNxHWcjpl8abxoduwIUQcxr64HD3iQ7ep5txCNHTrs9vj++i1EGbotiJYkXlGbmvn6EVCytFTKBQKRZNRQq+dYvOV45MuHO5WGIFmhG6tHL0Mjl5NJEBVqCrtGDMrdGuz6RM1stRHryJQUXeOnhYGZJZz9GJGoClHT6FQKBRNRAm9dordV0Y5+ThsWUzWN4WerIejZwiW35V9gD/sZ3z/8SkvaQk9YacyWEl+C1cRO2wOqoJVhLRQ3UIvHNC/bpXJGDHzghUKhUKhaCTqk6SdYvcfpVzm47Bn80co9FG3Zug2NkdPJDt6W10u/lL5OdecdA1ndDkj5RVNp88n4Jj/GEWeohZ9AofNQZkR9s5cjCFiplO0RjGGytFTKBQKRdNRQq+d4vCXc1Tm47S3gqNnVt0ajpNDAjY7HocHh83BwdqDIGwsKCygo83DT8/6adpLemy6OPTZbJQHyil0F7boIzhtTmrDtQB1O3omrTEZQ0ZUjp5CoVAomowSeu0UR+CoEbrN4o/QaphsjkDThYgdCcKGw+ZgaPehvLX3LWrDfv7p9TI+t2/GcKzHyPMrk0HCWpiOno4t+gix0zDqLfSyWXVLjHBXjp5CoVAomogSeu0UV/Boqzt6Vo6eUYwBcGGfC9lbtZfnP19E0CYY7e2V8ZJe9PP2a36ArDh6JnVW3Zq0hqMHytFTKBQKRZNRQq89EgnhDFVRLvPJcWezUMDoo2fm6MWGbg1RMqr3KASCP235M4WRCGe6Mrd/8RjX2B/2AbRRR68VcvRAOXoKhUKhaDJK6LVHassBOEo+XfNTNyJuERKrbg1R4iTq6HXJ6cLpXU4npIW4oNZPXTLUEnqRGkA5evECUwk9hUKhUDQNJfTaI7V61WjIXYQzm1W3SaFb/d72GEcPYHSf0fqrz2/l86XDrUkADoT02b0d3S3r6JlCz2lzpu3tB7Rijl7sGpTQUygUCkXTaKVPMEWTqD0CgC0vi1MxIEboGSPQzNBtjKMHcNWJV2EXdkasuBO9H0uGS2phvJrGQUPoFXpa1tEzQ7cd3B0QIkN+o3L0FAqFQvEdQDl67RHD0XPld87yjQVImRS61XP0oqIp35XPDYNuwGEKw0xEAnilJILEYXOQ58xrqcXrazWFXqYeetCKOXqqGEOhUCgUzYcSeu0RQ+h5OnTL7n0TQ7eW0JNpRImoh9AL4jHCtx3dHTO7bM2AGbrNWIgBcV1OsjsZI7YYQ/31VCgUCkXTUJ8k7ZBwtR66ze/YGkIvWnUbMTY7IHWY0Tw+E5EQHuOYlg7bQtTRK3BnKMQA5egpFAqF4juBEnpthUgY3n0qOl81lqoD8OFCSzT5jh2iUubQtbBlw5xJWA2TjdCtkX+X1tGrT+g2HMBjHNPShRjQyNBtNnP0VMNkhUKhUDQjSui1FfZ9AK//Ana9nbxvy3J47aew/98ABKuOUC7z6dbBk901JvbRM4RnYjFG9Pj65OgFo45eC7dWgYaEbpWjp1AoFIr2jxJ6bYVgdfxrqn1frAYgUn2Eo+TTvSDbQs/M0dOFXghdxCW2V4keL+oRug3iNXP0WrhZMsSEbjP10INWrLpVjp5CoVAomg8l9NoKIX0yBMHa5H2m0Pt8FQA2XxnlshWFnjRz9HSBZofUhQPfFUdPjUBTKBQKRTtFCb22gin0QqmEnrHt8OdQthNn4CiVIp8Cb5bbICZW3UoNh5R6VllaR6+uHL2o0Mumo9cuQreq6lahUCgUTUR9krQVTIGXUujVgDNX//rjF/GGjhF0tXwrkmREXNVtGIklgUQTHD0te45e44oxsimoY36mytFTKBQKRRNRkzHaCplCt6EaKOwDDje881tcgN+b5dYqkJSjF5aanp8HTSjGCODNYtWtmZvXNadr5gNbzdFTOXoKhUKhaD6U0GsrWI5eTfK+YA24cuGqZ2DfR9z76hf4u1+Q3fVBUo5eWGrRX6DGtlfJch+9oT2G8pcJf+H4jsdnPjBWcLVWMYZy9BQKhULRRFTotq2Q6Oh9+yEc+Sq6zZUDHfshB13BS7VnUtSxKPtrTGiYHCZG6KVz9OqYdRtbjFHkaflnsgkbp3U5re4DW8vRi723cvQUCoVC0USUo9dWsIoxjNeV/w2dT4BrX9AdvRxdBFX4QgTDGl3z3dlfo1lcEVuMYeaUpc3Rq0PohQOcGAzRN78PnTydmnnBTaDVcvSATO+pos1SWVnJoUOHCIVCrb0UhULxHcLpdNK1a1cKCupoC5YGJfTaCkEjZGuGbn1H9T/mNmcOAEeq9ckZXVpL6GmxodsIlueU0n2qz6zbEKNqfYy6/LW2VWXa2o6ejChHrx1RWVnJwYMHKS4uxuv1tkKhlEKh+C4ipcTn87Fv3z6ARom9NvTJ+h9OYug2WA2BSuNrI0cPOFwVBKBzXmsIvfhijIiUOC33qZHtVSIBXUi1JZEHrTgCLebeKkev3XDo0CGKi4vJyclRIk+hUDQbQghycnIoLi7m0KFDjbpGG/t0/Q8mtr2KphlCr0rfFqy1hJ7p6LWe0IsdgRaJhm4bXXUbArurmRfaDMQ5etnuV5jhPVW0SUKhEF6vt7WXoVAovqN4vd5Gp4UooddWiG2YbE7C8FfqwipYnULotYY4EglVt5GYqttG9tELB8DRhoWezRFfCZvNeytHr12hnDyFQtFSNOXfFyX02gqxoVtT6AUqje0yLkfPbhN0zGkFcZQ461ZGsGcM3dajGCMSbNuOXrbz8/SbG/dWQk+hUCgUTUMJvbZCbOjWDNlGgtGCDFceAEeqghTlurDZWsE9SBiBFomtum3CrFvsrRCGrgtT6GU7Py/23qrqVqFQKBRNRH2StBUsR68GAtXR7VUH9FdX1NHL7/AttalGpbU0Rl+8XYFy9tvthLWYHL1GF2MEW0dM1YWVJ9cKhekqR0/RCsydOxchhPWne/fuXHLJJXz66actcj/zPps3b47bvnXrVoQQvPXWWw263ksvvcTChQuTtn/wwQdMmzaNk046CZvNxrRp01Ke/9VXX3HllVfSrVs3CgoKGDZsGK+//nra+x08eBCHw8Fjjz2Wcn8oFKKoqIhbb7213s9w77330r17d+v79evXI4Tgiy++yHje7bffzvHH19EEPoEDBw4wd+5c9uzZE7e9vvdsCa6//nqEEDz//PNZv/d3GSX02gqWo+eLVtsCVO3XX40cvUPVlRzOe5xF2xZleYFYwm324Xf4daeOeo5epua+9c3Ra8uh21Zx9DKIZ4WiBenQoQObN29m8+bNPPHEE2zfvp2xY8dSXl7eYvecN29es1wnndDbtGkT77zzDt/73vfiRFQsVVVVjB07lq+//pqnnnqK5cuX07NnTy699FLef//9lOd069aNUaNGsXTp0pT7//a3v3H06FEmT57c6Gc655xz2Lx5M/369Wv0NdJx4MABHnjggSSh15L3zERtbS2vvvoqAEuWLMnqvb/rKKHXVjAdvXAaoec0ijFqj4LQ+OTwJ1leIFbOXXnEx36HnYisy9GrT45eqI0XY6gcPcV/Dg6Hg3PPPZdzzz2XkpISFi1axKFDhzI6W01h5MiRrFmzho8//rhFrg/wk5/8hB07dvDiiy/Ss2fPlMds2rSJ3bt3s2jRIiZNmsS4ceNYunQpRUVFvPzyy2mvPXnyZD744AN27tyZtG/p0qX06tWL888/v9FrLygo4Nxzz8Xj8TT6Gu3hngCvvfYa1dXVjB49mvXr13P48OGs3j8TPp+vtZfQJJTQayuEYn6RqmN65cQ4elJKyv26CPzsyGfIukRUc2M4elVaiDK7g7AWxpEpzFjv0G0bFnpZn4oRc+/WCBsrFDGcccYZAOzduzdue3l5ObfccgvdunXD4/EwbNgw3nvvvbhjnnnmGQYNGoTX66Vz585ccMEFfPbZZ3HHTJo0iYEDB/LQQw/VuZY///nPDBo0CLfbTd++fXnkkUesfdOmTePll19m48aNVkh47ty5ANjq0aPTbFvRoUMHa5vD4SA3Nzfjv7OTJk3C7XYnuXp+v59XX32VkpISq1rytddeY8yYMXTp0oWCggLOO+881q9fn3FdqcKo5eXllJSUkJubS8+ePXn44YeTztu3bx833ngj/fv3x+v1cuKJJzJnzhzrOXfs2MGZZ54JwIgRIxBC4HA40t6zpqaG2267zfp5n3POOUlrP//88ykpKeGFF15gwIABFBQUMGHCBEpLSzM+o8mSJUvo06cP8+fPJxKJsGzZsqRjamtrmTVrFn369MHtdtO/f3/uvffeuGMWLFjAqaeeisfjoVu3blxzzTVUVVXFrTHTe7xjxw6EECxdupQpU6ZQWFjIFVdcAcBzzz3H8OHDKSoqoqioiNGjR/PRRx8lrfOtt95i5MiR5ObmUlhYyKhRo/jkk084fPgwbrebxYsXxx2vaRp9+vThrrvuqtd71VCU0GsLSKmHbj3GPzJxQi+ao1cVCBOW+uSMo4Gj7Kvel911ChtSatTIEOV2GyEthIMMrUDqOeu2TRdjtIajp4oxFG0EM6zXv39/a1sgEGDMmDGsW7eORx99lFdeeYUuXbowZswYDhzQ/716++23mT59OlOmTGHt2rU8++yzDBs2jIqKirjrCyGYPXs2K1asYNu2bWnX8eijjzJjxgwuv/xyVq1axYwZM7jvvvv4/e9/D8B9993HqFGjOPPMM63Q880331zv5xw9ejT9+vVj1qxZ7N27l/Lycn75y19y6NChtDl9AIWFhVx88cVJQm/VqlVUVVXFhW137drFZZddxuLFi3n55ZcZOnQoF110UZJAroupU6eybt065s+fz4IFC1i9ejXLly+PO+bw4cN07tyZJ554gtdff50777yTP/3pT9x+++0A9O7dm0WL9PSfBQsWsHnzZjZt2pT2njfddBOLFi3i/vvvZ8WKFfTo0YPx48cn5Vdu2rSJBQsW8Pjjj/PHP/6RDz74gOnTp9f5TMeOHeP111/n2muvZdCgQZx++ulJ4VtN07jkkkt4+umnmTlzJmvWrGHu3LkcOXLEOmbu3LnMmDGDCy+8kFdeeYWnnnqK3NxcamsbntP+s5/9jI4dO7J8+XJ+/vOfA7B7926mTZvGsmXLWLx4Md27d2fEiBHs3r3bOm/9+vWMGTMGr9fLokWLWLJkCcOGDaO0tJQuXbowceJEnnvuubh7vfnmm+zduzfj71pTUJZBWyAS0nvT5XQGfwVUH4zusxy9PI5UBcDut3ZtPbKVXvm9srhQgU9qaIAmBEf9RznOEiUpqoDrW3VrVBS3KdpCjp4K3bZrHnjtM7aVVtZ9YAswsGcBcy4d1Khzw2G9qn737t3cdtttDB48mMsuu8za/+KLL7J161Y+++wzTjjhBADGjBnDSSedxGOPPcajjz7K+++/z+mnn87dd99tnTdx4sSU9yspKWHOnDn86le/4oUXXkjaX1lZyQMPPMC9997LnDlzABg7diy1tbXMmzePGTNmMGDAAIqKitA0jXPPPbfBz5yTk8Nbb73FhAkT6NOnD6CHMFeuXMnAgQMznjt58mRKSkrYtm2bdezSpUs58cQTOeuss6zjZs6caX2taRqjRo1iy5YtPPPMMwwdOrRe6/z0009ZtWoVy5cv58orrwTgggsuoHfv3nGh1sGDBzN48GDr++HDh+P1epk+fTrz58/H7XZz2mmnATBw4MCM79mWLVt46aWXePHFF7n++usBuPjiixk4cCDz5s1j9erV1rHV1dWsXr3ackZLS0u56667CAaDuFzpIzcrVqwgEAhYbltJSQn33HMPe/bssX4ea9euZcOGDaxevZoJEyZY506dOhWAsrIyHn74YWbNmhXn9k6aNCnTW5qW4cOH87vf/S5um+kSg/4zHDt2LCeffDKLFy9m9uzZANx9990MGTKENWvWWG7u+PHjrfN++MMfMmHChLhne+655xg6dGidv2uNRVkGbQGzECO3s/6a0tHL5Uh1EGGPhni3HtmapQUaCBtVRIVbmb8s82SM+sy6DQfA0ZYdvVYM3apiDEWWKSsrw+l04nQ6Of744/n4449ZsWIFbnf07+j69es5++yz6d+/P+Fw2BKGF1xwAR988AGgC42PP/6YO+64g7fffptgMJj2nna7nV/84hcsWbIkZa7b5s2bqamp4eqrr7buFw6HufDCCzl48CDffvttk5/bvH7Hjh1ZuXIl69atY9KkSVx55ZV15g9eeuml5OXlWa5edXU1a9asSSrC2Lt3L//1X/9FcXExDocDp9PJ3//+d7Zv317vdb7//vsIIbj00kutbQUFBYwZMybuOE3TeOyxxzjllFPwer04nU6mTp2Kz+dr8Pv1r3/9CyEEV111lbXNZrNx9dVX884778QdO3To0Ljw98CBA5FS1hm+XbJkCSeccIIljEtKSpBS8v/+3/+zjvn73/9O165d40ReLP/85z8JBALceOONDXq+dPzgBz9I2vbZZ59x+eWX061bN+x2O06nk507d1o/w8rKSj788EOmTp2atsHxuHHj6NWrl1VZXFFRwSuvvNJs606FcvTaAmZ+Xk4n/bX6ILjyIVgVU4yRw5FqH8KmO3r9Cvqx5ciW7K5T2KghXrg5TDGSthijPiPQ2mJ7lVZ09FQxxneCxjpqrUmHDh1Yv349kUiETz75hFmzZnHdddexadMmK9ftyJEjvPvuuzidyX83BgwYAOgO33PPPceTTz7J/PnzycvLY8qUKTz66KPk5uYmnXfDDTfw4IMP8utf/zrO+TLvBzBoUOr3c+/evfTt27dJz/3MM8+wbds2vv32WwoLC61n+PLLL5kzZ45VDZqKnJwcJk6cyNKlS3nwwQdZuXIlPp8vLhcsEolwySWX4Pf7mTdvHscddxy5ubnMnj2bysr6u74HDhygsLAwyR3r2rVr3PePPfYYd999N7Nnz2bEiBEUFhby7rvvMnPmTPx+Pw1h//79dOjQIU7sg151XFlZSSQSwW7X/60y3zsTc52Z7nnw4EE2bNjA7bffzrFjxwDo2LEjZ511FkuWLOF//ud/AP0/IT169Eh7nbKyMoCMxzSEbt26xX1fUVHBuHHjKC4u5vHHH6dPnz54PB5uvPFG6/nKy8uRUmZcg9niZ+HChdx7773WfxAScwebEyX02gKmoxcr9HI6ghaKaZicy5HqY5ajd26Pc1m5c6VeEJEt10nYqBLxOXfOjMUY9RF6gTaao2c+V2vm6Cmhp8guDoeDIUOGALo74/V6ueGGG1i2bBnXXnstAEVFRQwZMoSnnnoq6fxYMTB16lSmTp3K4cOHWbFiBXfccQcFBQUpiwdcLhf/8z//w6xZs5JCbUVFRYCe95b44Qtw0kknNf6BDb744gv69u2bJFQGDx7Mxo0b6zx/8uTJ/OUvf+Gjjz5i6dKlnHnmmZx88snW/i+//JJPP/2UdevWxblvDa3m7N69OxUVFUmh0MRh98uWLaOkpIQHH3zQ2tbYfog9evSgoqKCQCAQ9/M9ePAgBQUFlshrLC+99BKRSITHHnssZU/CL774gpNPPplOnTqxf//+tNfp1En//Ny/f3/Sz9HE4/EkucvpWgclOnKbNm2itLSUjRs3xvUsNMUp6L+rQoiM6wQ953HevHn84x//YOHChUyaNCnOCW1uVLm5iUYAACAASURBVOi2LZAqdOvKB3eB/r3dBXYnR6oCCLufXGcup3U5DV/Yx7dVTQ9b1BshqE5w9OyZREl926u06apblaOn+M9lypQpDBo0iF//+tfWttGjR7Njxw769OnDkCFD4v6YeV+xdOnShVtuuYURI0ZkLLj40Y9+RMeOHePyqwDOO+88vF4vpaWlSfcbMmQI+fn5gC4WG+pWmfTt25dvvvmGo0ePxm3/8MMP69VP7qKLLqKoqIg//vGPvPHGG0lhW1PQxQqlr7/+mnfffbdB6/ze976Hpmm89tpr1raqqqqkClifz5fkwCVWetbHbQO9r56UMq7NjKZpvPzyy01qHWOyZMkSTj31VDZs2BD3Z+3atTgcDsvxGj16dMZWP8OHD8fj8WRsttyrV6+kRtDr1q2r1zpT/QzffvvtuFB4QUEBQ4YMYdGiRRmrtfv168fo0aO55557ePfdd1s0bAvK0WsbWKFbQ+hpIXDn6W5XzSGrWfLh6iAeV5B8Vz5FHv1/uRXBilRXbBmEjeqEKlordNukhsltMHRrhU9bM0dP/T9M0bqYVbHXX389b775JqNHj+aGG27gj3/8IyNHjmTWrFkcd9xxlJWV8f7779O9e3fuuOMO5syZQ3l5OSNHjqRz5858/PHHbNy4MaWbZ+LxePjZz35mVTiaFBYWMnfuXH7605+ye/duvv/976NpGtu3b2fDhg389a9/BeDkk09m5cqVvPLKK/Tq1YuePXvSs2dPDh8+bLlyR48eZffu3VaVqpl3dt111/HLX/6SCRMmcNddd5GTk8OLL77I+++/z6pVq+p8n5xOJ1deeSV//vOfASz302TQoEH07NmTO+64gwcffJCKigruv/9+evVqWDHdGWecwYQJE/jxj3/MsWPH6Nq1K4888ogldk3Gjh3LU089xZAhQzjuuONYtGgR33zzTdwx/fr1w+12s3DhQnJzc3G5XJx99tlJ9zz11FO55pprmDFjBseOHaN///48/fTTfPXVVzzzzDMNWn8iu3fv5t133+XRRx9l5MiRSfvHjh3LkiVLmDt3LuPHj2fMmDFce+213H///Zx55pns37+fd955h6eeeoqioiJmz57NnDlz8Pv9jB8/Hp/Px6pVq3jooYfo1q0bV1xxBc8//zyzZs3i4osv5s0336yzxY3JsGHDyMnJ4eabb2bWrFns2bOHBx54IKk3469//WvGjRvHD37wA370ox+Rk5PDpk2bOO+885KKMiZPnkzfvn258MILm/Q+1oX6JGkLJDp6AO4YR89sllwdwOUKkO/KJ9+l/8WuClZlb53CRrURurUZei/aXiXdrNt6tFdpy8UYytFT/Idz7bXXcsIJJ1hOm8fjYcOGDYwdO5Y5c+Ywbtw4fvrTn/LVV19xzjnnALrztG3bNqZPn85FF13EU089ZYm1TNx6661WqDaWu+66i6effpq1a9dy2WWXMXnyZBYvXsyIESPizh03bhw33XQT3/ve93j66acBPYH+6quv5uqrr+brr7/mrbfesr436d27Nxs2bKCwsJBbbrmFq6++mi+//JLly5enTMpPxeTJk5FSMmzYMKua0sTj8bBixQoArrzySubMmcP999/P8OHD63XtWBYtWsTo0aP5yU9+wo9+9CMuvvjiuEIJgAceeIBrrrmG2bNnM3nyZHJzc3n88cfjjsnJyeHpp5/mvffe44ILLshY+fvss88yZcoU5s6dy+WXX863337L2rVrOe+88xq8/liWLFmCzWbjuuuuS7l/ypQpbN++nQ8//BAhBCtXruSHP/whv/3tbxk/fjz33XcfXbp0sY6/7777+MMf/sDf/vY3Jk6cyPTp06mqqrLyQidOnMi8efNYunQpV1xxBaWlpfz2t7+t11p79OjBsmXL2Lt3L5deeilPPvkkTz/9dFzrIYBRo0bxxhtvUFlZyXXXXce1117LO++8Q3Fxcdxxl156qZWvl65wo7kQWW+6mwYhRC/gQeBioBOwH3gFeEBKeTTTucb5ucDlwA+As4DegAZ8CSwBfielTF/6FcOQIUOkWT2WFb5cC0tK4PrlsNj4Czvwcj0/b9dG6HwS3PY+l/9hEwdyHuek7rnMGTaHy165jEe+/wjj+4/PfP3mYuV/8/y+t/hNro3iCOyzw42uXvzsy3/CDSvhuJHxxy+8BLQI3LQ2/TUf6glDboSL6m6YmlV2/QOevwRO+gFM/kt27/3E6XBsN4z9Xxg+s+7jFa3O559/zimnnNLay1AoFO2EV199lcsvv5ydO3cmicV0ZPp3RgjxoZRySKp9bSJ0K4QYAPwT6AqsBL4AzgF+ClwshBgupSyr4zIjgBeBcmADukgsAi4FfgNMEkKMllI2LomjJUksxgDd0ZMR/WtXDgAHK/3Y8v0UuHpQ4NLdvspANvt0CaqFHtTsrQn22WV01m1KR68+kzHa+qzb1pyMoRw9hUKh+C5RWlrK9u3bmT17NhMnTqy3yGsKbSV0+3/oIm+mlPJyKeUvpJQXAo8DJwH1sXsOAFOAHlLKq4xr/Bg4EfgIGAb8d8ssv4lYOXpFWLlh7nxwG1U4rjwimuRQVQBpq6XAXRAN3YayHLpFkouNLsavjtPWhPYqmgZauG0LvVapus0wP1ihUCgU7Zb/+7//Y8yYMeTl5fHkk09m5Z6tLvSEEMcB44BvgD8k7J4D1AD/ZYRm0yKl/LeUcnFieFZKWQWYNdsjm2PNzY4p9Jy54NTdO13oGQm2zhzKqgNENElI1pLvysdtd+OyuagMZtHRM3L0crHTSeq/OnUWY2QagRYxflSONiz0WiVHTzl6CoVC8V1k3rx5hMNh3n333aRczpai1YUeYJabvCFlvP1jiLRNQA7Q8Lk2UULGa7gJ12g5zNCt04t0evWvXXngMYoxXLkcqPQDGkGt1nLz8l35zVKMURuqTXudikAFgUhA/0bYqBaCfGGjE7oIsTelYbIp9Nq0o9ca2Q2mo9cW/noqFAqFoj3TFj5JzG6X6ebAfGW8ntiEe9xkvKZuwNPaWI6el6qILnq02D56rhwOVgbAFkQiyXc2r9D733f/l5/8/Scp901ZM4X5H83XvxGCKgF5Mir0oo5euqrb+gg9VXWb8t7K0VMoFApFE2kLxRhmO+h0DeHM7albXdeBEOI29ErefwPPZjjux8CPgazZqRahWnB4wGanSnNRAJSHXXR2Gx/4rjwOVPoRNl0Qmo5egaugWYTe1iNbU14nEAnwTeU3FB0x2h0IGzUCOiDoJPRfHUemHL26Zt1aQq8N9tEz8+Raw21UOXoKhUKhaCbagqNXF2aDmQb3gRFCTAKeQC/UuFJKGUp3rJTyaSnlECnlkNi+PC3B9oNVPLR6G5V+YzkhHxgh28qILnpKa+3R0K0zh4MVfuxOvWDYrLjNd0cdvQ17NjRqSkYoEmJv1V6OBo4S0SJx+0qr9UHUuyp26RuEjWob5CPoJPR1NqlhctgICavQbZp7K6GnUCgUiqbRFoSe6dilG/RWkHBcvRBCXA4sBQ4BI6WUXzduec3PgQo/f/rHLrZ8azxSqBacOQTDGsfCuoDaXW2PCd3qOXod8/QUQ8vRcxZQGaxEkxp3bryTBZ8uaPBa9lbtJSIjaFLjWOBY3D5T6B0NHOWo/6gx61aQJwW9hYsBHQZwgqujfnBjRqDVHNZfvY0ya1uWthC6VY6eQqFQKJpIWxB6Xxqv6XLwTjBe0+XwJSGEuBpYBhwELpBSflnHKVnl9F66pv3kW0NYGY7envIaaqWer7arSsQJvYOVfjrk6o5bgbHdzNEr95cT0kJsPbK1wWux3DqgzB/fqnBf9b6E4wQ1QpAnIcfm5JXLX+FMb3f9gLR99DIIvX0f6a89z2zwuluc1myvgpqMoVAoFIrmoS0IvQ3G6zgh4tWCECIfGA74gHpNfxZCXIc+CaMUXeR9VccpWacwx0W/Tjl8sjde6O04VIMPXehtP0Zc6PZAhZ/8HD3UG1t1Wxms5GDtQQB2HttJTaimQWv5uiJqdJb5yjhQc4Db3ryNY/5jlqMHutALIfHbdKFHYm5eY0K3+z6E/B5Q0DP9Ma1FWxiBpqpuFVlk7ty5CCGsP927d+eSSy7h008/bZH7mffZvHlz3PatW7cihOCtt95q0PVeeuklFi5cmLQ98bnMP6+/Hl+bFwgEuPPOO+natSu5ubn84Ac/SJoPG8vBgwdxOBw89thjKfeHQiGKioq49dZb6/0M9957L927d7e+X79+PUIIvvjii4zn3X777Rx//PH1vg/AgQMHmDt3Lnv27InbXt97tgTXX389Qgief/75rN/7u0yrf5JIKXcCbwD9SG5o/ACQCyySUloKRghxshDi5MRrCSGmAi8Ae4Dvt6VwbSJn9C7kk71G6DZYA84cdh6uthy9z8uBTsfDsJlw4kUcqPST49aLF2KFXlgLs7dyLwASybaybQ1ax66KXdgMQVHmL+O9/e+x8duNbN6/mdLqUorzinHb3eyq2EU1uqOYJ4kReHXNus0g9Eo/guLkIdptglbN0VOOnqJ16NChA5s3b2bz5s088cQTbN++nbFjx1JeXt5i95w3b16zXCed0IP45zL/JM5pnTlzJgsXLuQ3v/kNy5cv58iRI4wdOxa/P/UwpW7dujFq1CiWLl2acv/f/vY3jh49yuTJkxv9TOeccw6bN2+mX79+jb5GOg4cOMADDzyQJPRa8p6ZqK2t5dVXXwX0GbiK5qMtVN0C3Io+Au1JIcRo4HNgKDAKPWR7T8Lxnxuv1iRgIcQo9KpaG7pLeGOKQcHHpJRPNPvqG8EZvQpZ+e9SDlb66RbygSuHnYeq6erKAQ2+rbVT7otQNO5/qQ2GqfKHcbmDCL8gz5kHRAXfjmM7rOtuPbKV73X/Xr3XsatiF6cUncJnZZ9R5iuj1ujpt+XIFvZV76N3fm9ynbnsqtxFtdQLRvI0GRUhphBK6ehlqLr1HYWyHXBG4/8RbFFUjp7iPxCHw8G55+otS88991z69evHeeedx+uvv5528HxTGDlyJGvWrOHjjz/mzDNbLoUj9rlS8e233/LMM8/w7LPPcsMNNwBw+umn079/f1588UVuvvnmlOdNnjyZH/7wh+zcuZMBAwbE7Vu6dCm9evXi/PPPb/S6CwoKMq67JWiNewK89tprVFdXM3r0aNavX8/hw4dp6cLI+uLz+fB6va29jEbT6o4eWK7eEGAhusC7ExgAPAmcV485twB9iT7PTehTNRL/3N6sC28CZ/Q28vT2HjNCt7qjZ8/tRNiRSwAnOw9XA+g99ACHw0+eM89y4Mzq253HdmITNnrk9mhQnp6Ukl2Vuzi9y+k4bU7K/GVWXt7WI1vZV72P4rxijutwHF8f+5pqqReD5PkrwWH0vmtsw+TSj/XXNu/oqRw9xX8uZ5xxBgB79+6N215eXs4tt9xCt27d8Hg8DBs2jPfeey/umGeeeYZBgwbh9Xrp3LkzF1xwAZ999lncMZMmTWLgwIE89FDdUy7//Oc/M2jQINxuN3379uWRRx6x9k2bNo2XX36ZjRs3WqHZuXPn1vs533jjDWs9JsXFxZx//vmsXbs27XmTJk3C7XYnuXp+v59XX32VkpISTMPhtddeY8yYMXTp0oWCggLOO+881q9fn3FdqcKo5eXllJSUkJubS8+ePXn44YeTztu3bx833ngj/fv3x+v1cuKJJzJnzhxCIT39Z8eOHZawHjFiBEIIHA5H2nvW1NRw2223WT/vc845J2nt559/PiUlJbzwwgsMGDCAgoICJkyYQGlpKfVhyZIl9OnTh/nz5xOJRFi2bFnSMbW1tcyaNYs+ffrgdrvp378/9957b9wxCxYs4NRTT8Xj8dCtWzeuueYaqqqq4taY6T3esWMHQgiWLl3KlClTKCws5IorrgDgueeeY/jw4RQVFVFUVMTo0aP56KOPktb51ltvMXLkSHJzcyksLGTUqFF88sknHD58GLfbzeLFi+OO1zSNPn36cNddd9XrvWoobULoAUgp90opb5RS9pBSuqSUfaWUP5VSJsUMpJRCSikSti00t2f40y9rD1QHg3p2wG4TekFGqBbp9LLzcA1f9rueI1f9FRDsPKQLvQMVeuhA2P2Wiwfxjl5nT2fO6HJGg4TeYd9hakI1HNfhOIo8RZT5okJvW9k2yvxlFOcV079Df/ZV76PMmC6XV3MEzpqmX8TWyBy9tlyIATF99FqxvYpy9BStjBnWix28HggEGDNmDOvWrePRRx/llVdeoUuXLowZM4YDBw4A8PbbbzN9+nSmTJnC2rVrefbZZxk2bBgVFfHNE4QQzJ49mxUrVrBtW/q0k0cffZQZM2Zw+eWXs2rVKmbMmMF9993H73//ewDuu+8+Ro0axZlnnmmFZmNduGPHjtG5c2ecTidnnnkmK1asiLv+F198Qa9evcjLy4vbfsopp2TMVSssLOTiiy9OEnqrVq2iqqoqLmy7a9cuLrvsMhYvXszLL7/M0KFDueiii5IEcl1MnTqVdevWMX/+fBYsWMDq1atZvnx53DGHDx+mc+fOPPHEE7z++uvceeed/OlPf+L223Wvo3fv3ixatAjQhdHmzZvZtGlT2nvedNNNLFq0iPvvv58VK1bQo0cPxo8fn5RfuWnTJhYsWMDjjz/OH//4Rz744AOmT59e5zMdO3aM119/nWuvvZZBgwZx+umnJ4VvNU3jkksu4emnn2bmzJmsWbOGuXPncuTIEeuYuXPnMmPGDC688EJeeeUVnnrqKXJzc6mtra1zDYn87Gc/o2PHjixfvpyf//znAOzevZtp06axbNkyFi9eTPfu3RkxYgS7d++2zlu/fj1jxozB6/WyaNEilixZwrBhwygtLaVLly5MnDiR5557Lu5eb775Jnv37mXatGkNXmd9aCuh2/8o9u7dxNtbF3NRjyN8vR2e1yooO7qfQM5GDnp7sF4rwtvpcxZu/YR/Hs7nQIUfZ8ej7PftTCn09lbt5ZSiUzi186m8/s3rLNy6EGc9Qo5m373+HfrTyduJMn8ZpdWleB1efGG9OXPPvJ7YhE3P/wvpxSN5/S+EE8boF6nL0fNXwHsp2r58/hp0OqFttlaB1nX0rHu3mf+HKf6DCId153737t3cdtttDB48mMsuu8za/+KLL7J161Y+++wzTjhBb4owZswYTjrpJB577DEeffRR3n//fU4//XTuvvtu67yJEyemvF9JSQlz5szhV7/6FS+88ELS/srKSh544AHuvfde5syZA8DYsWOpra1l3rx5zJgxgwEDBlBUVISmaUlhx+OPP55HHnmEwYMHU11dzYIFC7jyyit5+eWXLQfv6NGjFBYm/1vUsWNHjh49mvH9mjx5MiUlJWzbto2BAwcCetj2xBNP5KyzzrKOmzlzpvW1pmmMGjWKLVu28MwzzzB06NCM9zD59NNPWbVqFcuXL+fKK68E4IILLqB37954PB7ruMGDBzN48GDr++HDh+P1epk+fTrz58/H7XZz2mmnATBw4MCModotW7bw0ksv8eKLL3L99dcDcPHFFzNw4EDmzZvH6tWrrWOrq6tZvXo1HTroEavS0lLuuusugsEgLlf6fqkrVqwgEAhYbltJSQn33HMPe/bssQYYrF27lg0bNrB69WomTJhgnTt16lQAysrKePjhh5k1a1ac2xvr0jaE4cOH87vf/S5uW6xLrGkaY8eO5eSTT2bx4sXMnj0bgLvvvpshQ4awZs0ay80dP368dd4Pf/hDJkyYEPdszz33HEOHDrV+f5obJfRage173+HhQ/+wOgS+Qw7wLZ7u3/LmYXjzMDi6wj5gn/GfFU93+KYKxveL/sKYQi8iI3TN6crQHkOxCRuPfZi6CiwVXoeXkzqeRCdPJw7UHuBg7UHG9R3H2m/0cEVxXjGFbv0fwL/69P/d51/w8+gFOg2AvG7RCuFYOvQGXzmsTWNHn/Pjeq8z6+QUgbdIf75soyZjfDdY+ws4sKV17t39NBifHNKri7KyMpzO6H9uOnXqxL/+9S/c7uiYwvXr13P22WfTv39/SxSCLjg++OADQBcad911F3fccQdXXHEF5557btoPervdzi/+f3t3HpdVmT/+/3WJwC0gCKi44EapqPxyVHJDE/ccl8ydj9uY2tTUqJT9NLTELbNlHG1zJivUFJUsU0crJ7TMpcYsnXE3MzHNXTQVU3h//7iX7htuVhEQ38/H4zxu7utc55zrXFzcvO/rnOs6Eyfy2GOPub3cum3bNq5cuUL//v1djtehQwemT5/O8ePHqVWrVrbnNGTIEJf3PXv2pHXr1kybNs0lCHBzTzci4jY98/78/PxYtmwZ06ZN49dff2XdunVZLsOlpKQQFxdHcnIyJ0+eRGxTT6Wnp7vbrVvffPMNxhh69uzpSPP396dTp07s2rXLkZaRkcGcOXNYsGABR48edRlQcvz48XwNtPjPf/6DMYZ+/fo50sqUKUP//v2ZN2+eS94WLVo4gjywBpEiwokTJ3I8ZmJiInXr1nUExoMGDSIuLo7ly5fzzDPPAJCcnEzlypVdgjxnW7du5fr164wYMSLP55aT7t27Z0nbs2cPkyZNYtu2bZw+fdqRfvCgdfa3S5cu8e233/LGG29k2266dOlCaGgoCxcu5LnnniM1NZVVq1YxZ86cQim3OxroFYMHmj3J5gYDEBEupVk/uMQ7AC9PD3y8rL+SmxkZ/Jr2+4eaxcsDS1kPxxx6gEvvXmWfyoQHhbMtZhu/2R8tlgeWshYsZS0Elwtm64mtpEs6Laq2YMuJLVz67RLV/KpR2acyfev2ZeWhlQD4Bjg9Iq5uZxifzRSHHSZDq8wDqZ2UC8xzOYucJQAm/Jh7vttBn4yhiklAQAD//ve/SU9PZ9euXYwfP57/+7//Y8uWLZSx9TCfPXuW7du3uwSEdvYBCZ06deK9995j3rx5zJ07Fz8/P4YMGcLLL7+Mr69vlu2GDRvGtGnTmD17tkvPl/14AI0aNXJb5pSUlBwDvcyMMfTp04cJEyaQnp6Oh4cHgYGBXLx4MUveixcvuu3pc+bj40OvXr0cgd7HH3/MtWvXXO4FS09Pp0ePHqSlpTFjxgzCwsLw9fUlLi6OS5cu5bnsv/zyCxUqVMgSNFeuXNnl/auvvsqzzz5LXFwcbdu2pUKFCmzfvp0xY8ZkO4o4OydPniQgIMAl2AfrqONLly456hDIUlf2cuZ0zFOnTrFx40bGjRvn+B0EBgbStGlTEhMTHYHeuXPnqFq1arb7OXfOeit/TnnyIyQkxOV9amoqXbp0oXr16syZM4eaNWtisVgYMWKE4/zOnz+PiORYhjJlyvCnP/2JhIQEJk+e7Ljsn/newcKkgV4x8PT2pYK39Z6XnEKdij4578c+GAMgxNfaKH08ffDxzGVDN4ItwaSL9Ztl9fLViagYwY5fdlCxXEUAYpvFsillE+fSzrkEmDkyxtozpvLJPhhD/zzvaAXoUStuZcuWJTIyErD2zpQrV45hw4aRlJTEwIEDAQgKCiIyMpK33nory/bOwcDw4cMZPnw4Z86c4cMPPyQ2NhZ/f3+3gwe8vLx45plnGD9+fJZLbUFB1s+QtWvXZvnnC1C/fv0Cnatzj0t4eDgpKSlcuXLFJRDdv38/4eFZZvLKIiYmhqVLl7Jz506WLVtGkyZNXLY7cOAAu3fvZsOGDXTq1MmRfu3atXyVuUqVKqSmpma5FOrcuwSQlJTEoEGDmDZtmiOtoPMhVq1aldTUVK5fv+7y+z116hT+/v6OIK+gVqxYQXp6Oq+++qrbOQntv4Pg4GBOnjyZ7X6Cg4MBa2CaXXBusVj47TfXjpDspg7K3CO3ZcsWTpw4wRdffOEyZ6HzF4SgoCCMMTmWE6z3PM6YMYPNmzeTkJBAnz59XHpCC5veBHQH8/LwwuJhvS+jUrlbG4YeXC7Y8XN13+rEhMcwImKEY4RvgHcAL7R9gYH1B+JVEp9NW5roYAxVQgwZMoRGjRoxe/ZsR1rHjh05fPgwNWvWJDIy0mWx3/flrFKlSvz5z3+mbdu2OQ64GD16NIGBgS73VwG0atWKcuXKceLEiSzHi4yMpHx56xdPLy+vPPVWiQgfffQRjRs3dgQpXbp0AeCjjz5y5Dtx4gSbN292ub8qO127diUoKIj58+fz2WefZZk7zx7QOQdKR44cYfv2PD0HwOH+++8nIyODNWvWONIuX76cZQTstWvXsvTAZR7pmZfeNrDOqycirFy50pGWkZHBypUrb2nqGLvExEQiIiLYuHGjy7J+/XrKli3r6PHq2LEjp0+fzjLRtV1UVBQWiyXHyZZDQ0OzDK7ZsGFDnsrp7nf45Zdfcvz478+Y9/f3JzIykkWLFjkuzbtTu3ZtOnbsyKRJk9i+fXuhXW7OjnYZ3OHKe5Un7VoalX0q5545B/aeO4Ohim8VavjXILpGtEue1tVa07pa61s6jsoDnTBZlRD2UbGDBw/m888/p2PHjgwbNoz58+cTHR3N+PHjCQsL49y5c3zzzTdUqVKF2NhYpkyZwvnz54mOjqZixYp89913fPHFF2578+wsFgtPPfWUY4SjXYUKFYiPj2fs2LH89NNPPPDAA2RkZHDw4EE2btzoCM7Cw8P5+OOPWbVqFaGhoVSrVo1q1arRrl07+vbtS3h4OFeuXOHtt99m+/btrFq1ynGM0NBQRo4cybhx4xARKlWqRHx8PLVq1cpyj587np6e9O3blwULFgA4ej/tGjVqRLVq1YiNjWXatGmkpqby/PPPExoamuffBVinu/njH//Io48+ysWLF6lcuTIvvfSSI9i169y5M2+99RaRkZGEhYWxaNGiLE/5qF27Nt7e3iQkJODr64uXlxfNmmWd7ioiIoIBAwbw+OOPc/HiRerUqcM///lPDh06xDvvvJOv8mf2008/sX37dl5++WWio6OzrO/cuTOJiYnED60uzwAAIABJREFUx8fTrVs3OnXqxMCBA3n++edp0qQJJ0+e5KuvvuKtt94iKCiIuLg4pkyZQlpaGt26dePatWusXbuWmTNnEhISwsMPP8zChQsZP348Dz74IJ9//nmuU9zYtW7dGh8fH0aNGsX48eM5duwYU6dOpVo11yc7zZ49my5dutC9e3dGjx6Nj48PW7ZsoVWrVlkGZcTExFCrVi06dOhwS/WYKxHRJdPSrFkzuVP0+qiXRCREyA8Xfril/Xx94muJSIiQTkmdCqlkqsASeohM8Rc5/m1xl0Tl0d69e4u7CLdsypQpEhwcnCX95s2bUrduXenSpYsj7eLFizJmzBgJDQ0VT09PqV69ujz88MPy1VdfiYjImjVrpEOHDlKxYkXx9vaWevXqyaxZsyQjI8OxD0Bee+01l2NdvnxZgoKCBJCNGze6rFu8eLE0bdpULBaLVKhQQZo3by6vvvqqY/2ZM2ekd+/eEhgYKIBMmTJFREQeeeQRqVOnjlgsFvHx8ZE2bdrIunXrspxnWlqaxMbGSsWKFcXHx0e6desmR44cyXP9JScnCyBRUVFu12/fvl2aNWsmFotF6tatK4sWLZLBgwdLixYtHHkmTZokISEhjvcbNmwQQPbt2+dIO3v2rPTv31/KlSsnISEhMmPGDBk7dqzcc889jjyXLl2SYcOGSYUKFSQwMFBGjx4tq1atyrKvhQsXyr333iuenp7i4eGR7TF//fVX+ctf/iKVKlUSLy8viYyMlA0bNricX1RUlAwcONAlzd2+nM2aNUs8PDzkxIkTbtcvWbJEANmxY4eIiFy5ckViY2OlWrVq4uXlJXXq1JHnnnvOZZs333xTwsPDxcvLS0JCQmTAgAFy+fJlERHJyMiQGTNmSPXq1cXPz0+GDRsmH374oUsZDx06JICsX78+S3n+9a9/SYMGDcTb21vuu+8+Wb9+vdvzTk5OlqioKLFYLBIQECDt27eXXbt2ueT59ddfpUyZMo52mhc5fc4AOySbmMZITg+dv0tFRkaKffRYSTdk3RB2ndnFtpht+Hn55b5BNn64+AO9P+5N08pNWdhNnzNYrBY9BEc2wZ+/hKqNi7s0Kg/27dtHgwYNirsYSqk7xOrVq+nduzc//PCDyzyVOcnpc8YY862IRLpbp5du73DlvcrjU9bnloI8sA7GAOt0Kqq46fQqSilVGp04cYKDBw8SFxdHr1698hzk3QodjHGHq+Vfi7qBdW95P/7e/gRbgmkQrL0SxU6nV1FKqVLpzTffpFOnTvj5+WWZh/B20R69O9zTkU+TnpH3CTezU8aUYV2fdXh7eOeeWd1eOmGyUkqVSjNmzGDGjBlFekwN9O5wnmU88Sykx3QVZP49dRtoj55SSqlCopdulSpx7D16+ueplFLq1uh/EqVKGu3RU0opVUg00FOqpNF79JRSShUSDfSUKmm0R08ppVQh0UBPqZJGe/SUUkoVEg30lCpx9Fm3SimlCocGekqVNPZLtzrqVhWDDz/8kA4dOlChQgW8vb2pV68ekydP5uzZs448xhiMMWzbts1l2//9738YY9i0aZMjLTo6GmMMs2bNynKsihUrEh8f73ifkJDg2LfzMn/+fJftRIQXXniBGjVqUK5cOR544AG+//77bM/pxo0bBAUF8de//jXbPBEREfzxj3/Mdn1mCxYswBhDWloaAIcPH8YYwyeffJLjdn//+98pWzZ/M5ulpaURHx/P7t27XdLzeszbYdKkSRhjmDp1apEfW+WP/idRqqQx2qOnisfTTz9N//79CQsLY/HixXz22WfExsayZs0aRo8enSV/fiZ+nTNnDlevXs1T3uTkZLZt2+ZY+vTp47L+xRdfZPr06UyYMIE1a9bg5+dHp06d+OWXX9zuz9PTk759+5KUlER6etYJ5vfs2cOePXuIiYnJ8/lkVqNGDbZt20arVq0KvI/spKWlMXXq1CyB3u08Zm6WLVsGQGJiYpEfW+WPBnpKlTSOHj0N9FTRWbNmDX/72994++23WbBgAT179qRdu3Y8/vjj7Ny5k0cffdQlf3R0NOvWreO7777Ldd+tWrXi0qVL/POf/8xTWe6//35atmzpWCpXruxYl5aWxosvvsizzz7Lk08+SadOnUhKSsIYw+uvv57tPmNiYjh16pRLb6NdYmIiFouF3r1756l87nh7e9OyZUsCAgIKvI874ZgAX3/9NUeOHKFjx44cOHAgT22gqFy7dq24i1DiaKCnVImjPXqq6M2ZM4emTZvyyCOPZFnn4eFBt27dXNL69OlDw4YNmTlzZq77rlatGiNGjOCVV17h+vXrt1TOrVu3cunSJQYMGOBI8/X1pWfPnqxfvz7b7aKjo6lataqjJ8rZ8uXL6dGjB+XLlwdgy5Yt9OzZk6pVq+Ln50eTJk3cbufM3WXUtLQ0Hn/8cQICAggODubpp5/mxo0bLttdvnyZJ554gvr16+Pj40OdOnV48sknuXz5MgA3b94kMDAQgKFDhzouZx8/ftztMdPT03nuueeoUaMG3t7eREREZCn7kCFDaNmyJZ9++ikRERH4+fnRtm1b9u3bl+M52iUmJlKuXDnee+89vLy83PbqpaenM3PmTOrWrYu3tzehoaGMHDnSJc/KlSu5//77KVeuHBUrVqR79+6kpKS4lDGnOr558ybGGObOncuYMWOoVKkSTZo0AaxfXDp16kSlSpXw9/enVatW/Pvf/85Szl27dtG9e3cCAgIoX748LVu2JDk5mRs3bhASEuK2fUdFRbm0v5JOAz2lShrt0VNF7MaNG2zdupUHH3wwz9sYY4iLi+PDDz9k7969ueafMGECp06d4r333ss17z333EPZsmWpX78+//jHP1zW7d+/Hw8PD+rWreuS3qBBA/bv35/tPsuUKcOAAQP48MMPXYKtHTt2cPjwYZfLtkePHqVt27a88847rF69mt69ezN06FCSkpJyLbuzZ555hoSEBOLj43n//ff54YcfmDt3rkueK1euOO45XL9+PdOmTWPDhg0MGjQIgLJly7JhwwYA4uPjHZeznXs5ncXFxTF79mwef/xxVq9eTYsWLYiJiclS9h9//JGJEyfy/PPPs2TJEk6ePOk4Zk4yMjJYsWIFPXr0oEaNGjz44IMsW7YMEXHJN3LkSKZOnUpMTAxr167llVde4cqVK471CQkJ9OvXj/r165OUlMS7777Lvffe63IvaF69+OKLnD17lsWLFzNnzhzH+T300EMsWbKElStX0qJFC7p27crXX3/t2G7Pnj1ERUVx5swZ/vGPf7By5Up69erFsWPH8PT0ZNiwYSQkJLgc69ChQ2zdupURI0bku5zFRZ91q1RJo/PolQqzv5nN/vPZBx63U3hQOBOaT8hz/nPnznH9+nVq1qyZr+MMGjSIKVOmMGvWLBYvXpxj3tq1azN48GBmz57NqFGj3A5IqFq1KtOnT6d58+akp6eTmJjIY489xtWrV4mNjQXgwoUL+Pn54eHh+vcRGBjI1atX+e233/Dy8nJbhpiYGObOnctnn31G9+7dAeu9Zv7+/i4DMQYPHuz4WUR44IEHOHbsGG+//Tb9+/fPU92cOXOGt99+m5kzZzrK3qVLF+rXr++Sr0qVKrz55puO9zdv3qRmzZpER0fz888/U716dSIjIwFrAJy5l8vZ2bNnmTdvHlOmTCEuLg6Arl27kpKSQnx8vEvZz58/z7Zt2wgLCwOswX7//v05fPgw9957b7bH2LRpk0tQOGjQIFavXs2WLVto06YNYB2Us3DhQt544w3+8pe/OLa1b5Oens7EiRPp378/77//vmN9r169sj1uTkJDQ1m6dKlL2pgxYxw/Z2Rk0L59e/773//yzjvv0KJFC8AaOAcFBfHll19isVgA6+/IbuTIkbzyyits3ryZtm3bAvDee+9RrVo1l3wlnfboKVXSGAOY3wdlKFVETD7bnIeHBxMnTiQxMZEffvgh1/xxcXEcO3aMJUuWuF3ftWtXJk+eTJcuXejWrRuLFi1iwIABzJgxg4yMjBzLae9RyukcWrRoQVhYGMuXL3dss2LFCh5++GHHP3qwBkFPPvkktWrVwtPTE09PT959910OHjyY6zna7dq1i+vXr/PQQw850jw8PFze2y1cuJA//OEP+Pn54enpSXR0NGDtPcqP3bt3k5aWliUYHThwIHv37uX8+fOOtHvuuccR5AE0bNgQgOPHj+d4jMTERMqXL+8IjHv16oWvr6/L5duNGzcCMHz4cLf72Lt3L6dOnSq0XjF70O4sJSWFoUOHUr16dcqWLYunpyfJyckuv8Pk5GRiYmJcfvfOwsPDad26taNXLyMjg8WLFzNs2LAsXzRKMu3RU6qkMWW0N68UyE+PWnELDg7G29ubY8eO5XvbYcOGMW3aNGbPnu3Si+JOvXr16NevH7NmzWLo0KF52n+/fv1YsWIFR48eJSwsjMDAQC5fvkx6errLP9uLFy/i4+ODp6dnjvsbNGgQr7/+OmlpaXz77bekpKRkGW07dOhQdu7cyeTJk2nYsCHly5fn9ddfz9c0JvYRwJkvsWZ+n5SUxJ/+9CeeeOIJZs2aRXBwMMeOHaN///6OqVvy6uTJkwCEhIS4pNvfX7hwgaCgIAAqVKjgksfeC5rTMW/cuMHKlSt58MEHSUtLc+Tt3LkzSUlJzJ07l7Jly3Lu3DkCAgLw9fV1u59z584B1h7cwpD5fNPT0+nRowdpaWnMmDGDsLAwfH19iYuL49KlS458Fy5cyLUMI0eOZOzYscybN4+vvvqK48eP31GXbUF79JQqgYzen6eKlKenJ1FRUXz66af53tbLy4tnnnmGhQsX5tobBNb51w4ePMgHH3yQr+PYe+rCw8NJT0/n8OHDLuv3799PeHh4rvuJiYnh0qVLrFu3jmXLllGpUiU6duzoWH/lyhXWr1/P9OnTeeKJJ2jfvj2RkZFup2XJSZUqVQA4ffq0S3rm90lJSURFRfH666/TrVs3mjdv7hh8kV/2oCXzMU6dOgVQ4P3affLJJ1y4cIGkpCQCAwMdy6pVqzhz5gyff/45YP3ikJqamu10OsHBwcDvgak7FouF3377zSXNuUfSWeZe3AMHDrB7927eeOMNRowYQbt27YiMjMwyIjcwMDDHMoC1N9QYw8qVK0lISCAqKop69erluE1Jo4GeUiWNMdqjp4rcuHHj2LFjBwsXLsyyLiMjI8ferNGjRxMYGMhLL72U63Huu+8+evbsyQsvvJDlBn53Vq5cScWKFalVqxYArVu3xt/f32VwwdWrV1mzZk2WkcHuREREEBERwdKlS/nggw/o37+/y/2CaWlpiAje3t6OtNTUVNauXZvrvp01btwYLy8vPv74Y0daeno6q1evdsl37do1l2MBWS5t56W3Dax1a7FYsgy8WLFiBQ0bNnT05hVUYmIiFStWZOPGjVmW4OBgx+Vbe+C8aNEit/tp2LAhVapUcdvW7EJDQ/nxxx9dRmnbB6Xkxh7QOdfrkSNH2L59u0u+jh07smzZshxHgvv6+jJw4EBee+01Vq1adcf15oFeulWq5DHao6eKXs+ePXnqqacYOXIkW7Zs4aGHHsLPz4/9+/czf/58ateune2oXIvFwlNPPcWECXm7XD1p0iTHDfHO+vbtS/PmzbnvvvtIT09n+fLlLF++nHnz5lGmTBnHsSZOnMj06dMJDAwkPDycv/3tb2RkZOT45AtnMTExTJ48GRHJctk2ODiYJk2aEB8f77j0OGvWLMdgj7yqVKkSo0aNYvLkyZQpU4YGDRowf/78LL1KnTt3Zty4ccyaNYvIyEjWrl3LF1984ZLHx8eHGjVqsHz5cho0aIC3tzeNGzfOcsyKFSsyZswYpk6dSpkyZWjatClJSUl89tlnrFixIs9ld+fq1ausXr2aESNGOO4hdDZgwACWLFnC/PnzadiwIY888ghjx47ll19+oU2bNly4cIGPPvqIpUuX4uHhwezZsxk+fDheXl4MHDgQgM8//5yhQ4fSpEkTHn74YaZOncro0aMZNmwY3377bbaBY2aNGjWiWrVqxMbGMm3aNFJTU3n++ecJDQ11yTd16lSaN29Ou3btiI2NJTg4mJ07dxISEuJyf+HIkSNp1aoVvr6+d9S0Kg4iokumpVmzZqJUsVk9RmRWzeIuhcqHvXv3FncRCs0HH3wg0dHR4u/vL56enlK3bl15+umn5eTJk448gLz22msu212+fFmCgoIEkI0bNzrS27VrJ3379s1ynM6dOwsgU6ZMcaQ9++yzUq9ePSlXrpxYLBZp2rSpLFq0KMu2GRkZMmPGDKlevbpYLBZp06aN7Ny5M8/neOTIEQGkRo0akpGRkWX9gQMHJDo6Wnx8fKRmzZryyiuvyKRJkyQkJMSR5+233xZArl27JiIihw4dEkDWr1/vyHP16lV59NFHxd/fXwIDA2Xs2LHy0ksviYeHhyPPjRs3ZNy4cVKpUiUpX7689OvXT7Zs2ZJlX+vWrZOIiAjx9vYWQFJSUtwe88aNGzJ58mSpXr26eHp6SqNGjWTp0qUu5zd48GBp0aKFS5q7fTlLTEwUQL7++mu36+1l/uCDD0RE5ObNmzJt2jSpU6eOeHp6SmhoqIwaNcplm6SkJGnSpIl4eXlJcHCw9OjRQ1JSUhzrFyxYIGFhYeLj4yM9e/aUzZs3u5Txxo0bAshbb72VpTzbt2+XZs2aicVikbp168qiRYvcnvd3330nXbt2FV9fXylfvry0bNlSkpOTs+wvJCREhg8f7vbci0pOnzPADskmpjGSh67zu01kZKTs2LGjuIuh7lZrY2Hvx/D/Hynukqg82rdvHw0aNCjuYiilboPdu3fTuHFjNm3aRLt27YqtHDl9zhhjvhWRSHfr9NKtUiVNoz4QFJZ7PqWUUrfN2bNnOXDgAJMmTaJx48bFGuTdCh2MoVRJU6cttM7bvUZKKaVuj1WrVtG2bVvOnDmTpye6lFQa6CmllFJKZTJq1CgyMjLYs2eP4xm6dyIN9JRSSimlSikN9JRSqhDowDal1O1yK58vGugppdQt8vT0zDI/mlJKFZZr167l+ni/7Gigp5RSt6hy5cr8/PPPXL16VXv2lFKFRkS4evUqP//8c5bnJOeVTq+ilFK3yN/fH4ATJ05w48aNYi6NUqo08fT0JCQkxPE5k18a6CmlVCHw9/cv8AexUkrdLnrpVimllFKqlNJATymllFKqlNJATymllFKqlNJATymllFKqlNJATymllFKqlNJATymllFKqlDI6uWdWxpgzwE+3+TAVgbO3+Rh3G63Twqd1Wvi0Tguf1mnh0zotXLe7PmuJSCV3KzTQKybGmB0iElnc5ShNtE4Ln9Zp4dM6LXxap4VP67RwFWd96qVbpZRSSqlSSgM9pZRSSqlSSgO94vPP4i5AKaR1Wvi0Tguf1mnh0zotfFqnhavY6lPv0VNKKaWUKqW0R08ppZRSqpTSQE8ppZRSqpTSQK8IGWNCjTHvGmNOGGOuG2OOGmP+bowJLO6ylWS2epJsll+y2aa1MWadMea8MeaqMWa3MWacMcajqMtfXIwx/YwxrxljNhtjLtnq6/1ctsl3vRljehhjNhljUo0xvxpjvjbGDC/8Myp++alTY0ztHNqtGGOW5XCc4caYb2z1mWqr3x6378yKhzEm2BgzyhjzkTHmsDHmmu18vzLGjDTGuP0fpe00e/mtU22neWOMmW2M+dwYk2Kr0/PGmO+MMVOMMcHZbFMi2qneo1dEjDH3AFuBysDHwH6gOdAeOABEici54ithyWWMOQpUAP7uZvWvIvJKpvwPASuBNGA5cB7oCdQHPhCR/re1wCWEMeZ7oDHwK3AcCAeWiMiQbPLnu96MMU8CrwHnbNv8BvQDQoFXRWR8IZ9WscpPnRpjagM/AruAVW529z8R+cDNdq8AT9v2/wHgBQwCgoC/isjrhXEuJYEx5jHgLeAksBE4BoQAfYAArO2xvzj9o9J2mrP81qm207wxxvwG7AT2AqcBX6AlEAmcAFqKSIpT/pLTTkVElyJYgE8BwfoH4Jz+N1v6/OIuY0ldgKPA0Tzm9bf9EV4HIp3SLVgDbQEGFfc5FVG9tQfqAgaItp37+4VVb0Bt24fYOaC2U3ogcNi2TavirodirNPatvUJ+dh/a9s2h4HATPs6Z6vv2rdyDiVpATpg/edXJlN6FawBigB9ndK1nRZ+nWo7zds5W7JJn2mrized0kpUO9VLt0XAGBMGdMEasLyRafUU4Aow1BjjW8RFK436AZWAZSKyw54oImnAZNvbx4ujYEVNRDaKyCGxfVrkoiD19gjgDbwuIkedtrkAvGB7+1gBi18i5bNOC8JeXzNt9Wg/7lGsnx3ewIjbdOwiJyLJIrJGRDIypf8CzLe9jXZape00FwWo04K4q9opONqYOytsr3Wd0kpUO9VAr2h0sL1+5uaP7zKwBfDB2g2s3PM2xgwxxsQZY8YaY9pnc5+Dva4/cbPuS+Aq0NoY433bSnpnKki95bTN+kx57mbVjDF/trXdPxtj7sshr9bp727YXm86pWk7vTXu6tRO22nB9LS97nZKK1HttGxBNlL5Vt/2ejCb9Yew9vjVAz4vkhLdeaoAizOl/WiMGSEiXzilZVvXInLTGPMj0AgIA/bdlpLemQpSbzltc9IYcwUINcb4iMjV21DmO0Vn2+JgjNkEDBeRY05pvkB1rPednnSzn0O213q3qZwlhjGmLDDM9tb5H5+20wLKoU7ttJ3mgTFmPOCH9X7HSKAN1iDvRadsJaqdao9e0QiwvaZms96eXqEIynIneg/oiDXY8wX+P+AfWO9pWG+MaeyUV+u6YApSb3ndJiCb9aXdVWA60AzrfTaBQDusN8hHA59nul1D2+7vXgQigHUi8qlTurbTgsuuTrWd5s94rLdcjcMa5H0CdBGRM055SlQ71UCvZDC2Vx0C7YaITLXdd3JKRK6KyP9E5DGsA1nKAfH52J3WdcEUpN7u6roWkdMi8ryI7BSRi7blS6y9918D9wKjCrLrQi1oCWOMGYN1NOd+YGh+N7e9ajt1klOdajvNHxGpIiIGa8dDH6y9ct8ZY5rmYzdF2k410CsauUXi/pnyqbyx31j8gFOa1nXBFKTe8rrNpVsoV6kjIjeBBba3+Wm7uX3jv+MZY54A5mKdwqK9iJzPlEXbaT7loU7d0naaM1vHw0dYA+JgYJHT6hLVTjXQKxoHbK/Z3bNgH62T3T18yr3TtlfnywrZ1rXtHpU6WG9EPnJ7i3bHKUi95bRNVay/l+Ol+b6nW2C/zONouyJyBfgZ8LPVX2al+nPCGDMOeB34H9aAxN1k6NpO8yGPdZoTbae5EJGfsAbRjYwxFW3JJaqdaqBXNDbaXru4mZW8PBAFXAO2F3XB7nCtbK/OfyzJttcH3eR/AOvo5q0icv12FuwOVJB6y2mbbpnyKFf2EfaZv3DclXVqjJkAzAG+xxqQnM4mq7bTPMpHneZE22neVLO9ptteS1Y7Lcjke7oUaLJFnTC5YPXWCAhyk14L6+guAeKc0v2xfgu96ydMzlRf0eQ+YXK+6g3rt9K7ZiLaAtRpC8DLTXoHW70J0DrTurtxItrnbOe8w93feqa82k4Lv061neZen+FAFTfpZfh9wuQtTuklqp3qI9CKiJtHoO3D+gfWHmsXd2vRR6BlYYyJByZi7RX9EbgM3AN0x/pHsw54WER+c9qmN9ZH8qQBy7A+eqYXtkfPAAPkLmj4tnrobXtbBeiK9Zv5ZlvaWXF6pE5B6s0Y81dgHnfBo6Ugf3Vqm5qiEbAJ62OiAO7j97mwnhORGW6O8SrwFK6PlhqI9T6gUvVoKdszPBOw9oS8hvv7uo6KSILTNtpOc5DfOtV2mjvbJfCXsc6B9wPWdhSCdXRyGPAL0FFE9jptU3LaaXFHynfTAtTAOlXISdsv8CesN8nm+I3rbl5sf0iJWEeLXcQ64ecZYAPWOaFMNttFYQ0CL2C9LP5fIBbwKO5zKsK6i8f6LTC75Whh1BvWCUO/wBqEXwH+g3XurWKvg+KsU2AksBbrE3F+xfrt/pjtA7xtLscZbqvHK7Z6/QLoUdznXwz1KcAmbae3r061neapTiOwPvHje+As1vvrUm3nHk82/8NLSjvVHj2llFJKqVJKB2MopZRSSpVSGugppZRSSpVSGugppZRSSpVSGugppZRSSpVSGugppZRSSpVSGugppZRSSpVSGugppZRSSpVSGugppdQdyBgTb4wRY0x0cZdFKVVyaaCnlLor2YKk3Jbo4i6nUkrdirLFXQCllCpmU3NYd7SoCqGUUreDBnpKqbuaiMQXdxmUUup20Uu3SimVB873xBljhhtjvjPGXDPGnDbGvGuMqZLNdnWNMYuMMT8bY34zxpywva+bTX4PY8xjxpgtxphU2zEOG2MW5LBNP2PMN8aYq8aY88aYZcaY6oV5/kqpO5P26CmlVP7EAl2A5cAnQBtgBBBtjGkhImfsGY0x9wP/BsoDq4G9QDgwGHjIGNNRRHY45fcC/gV0AlKApcAloDbwMPAVcChTef4C9LLt/wugBTAQaGyM+YOIXC/Mk1dK3Vk00FNK3dWMMfHZrEoTkRfdpHcDWojId077mAOMA14ERtrSDLAI8AeGiMgSp/wDgWXA+8aYhiKSYVsVjzXIWwP0dw7SjDHetn1l9iBwv4j81ynvUiAGeAhYke3JK6VKPSMixV0GpZQqcsaY3D78UkWkglP+eGAK8K6IjMy0rwDgJ8AbqCAi140xUVh74LaJSGs3x9+MtTewnYh8aYzxAM4BXsC9InIil/LbyzNTRCZnWtceSAZeFZHxuZynUqoU03v0lFJ3NREx2SwVstnkCzf7SAW+ByxAA1tyU9trcjb7sac3sb2GAwHA7tyCvEx2uElLsb0G5mM/SqlSSAM9pZTKn1PZpP9DcWk9AAABx0lEQVRiew3I9Hoym/z29AqZXn/OZ3kuukm7aXv1yOe+lFKljAZ6SimVPyHZpNtH3aZmenU7GheomimfPWDT0bJKqUKjgZ5SSuVPu8wJtnv0/gCkAftsyfbBGtHZ7MeevtP2uh9rsHefMaZaYRRUKaU00FNKqfwZaoxpkiktHuul2kSnkbJbgANAG2NMP+fMtvcPAAexDthARNKBN4FywHzbKFvnbbyMMZUK+VyUUqWcTq+ilLqr5TC9CsAqEfk+U9p6YIsxZgXW++za2JajwER7JhERY8xwYAOw3BjzMdZeu/pAb+AyMMxpahWwPo6tBdATOGiMWWvLVwPr3H3PAAkFOlGl1F1JAz2l1N1uSg7rjmIdTetsDvAR1nnzBgK/Yg2+4kTktHNGEfnaNmnyZKzz4/UEzgKJwHQROZAp/2/GmAeBx4BhwHDAACdsx/wq/6enlLqb6Tx6SimVB07z1rUXkU3FWxqllMobvUdPKaWUUqqU0kBPKaWUUqqU0kBPKaWUUqqU0nv0lFJKKaVKKe3RU0oppZQqpTTQU0oppZQqpTTQU0oppZQqpTTQU0oppZQqpTTQU0oppZQqpTTQU0oppZQqpf4fnzrQ0Fj5biYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig4,ax4 = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "#ax4.plot(acc2, label='ResNet18 Training Accuracy')\n",
    "ax4.plot(val_acc2, label='ResNet18 Validation Accuracy')\n",
    "#ax4.plot(acc1, label='ResNet50 Training Accuracy')\n",
    "ax4.plot(val_acc1, label='ResNet50 Validation Accuracy')\n",
    "#ax4.plot(acc3, label='CNN50 Training Accuracy')\n",
    "ax4.plot(val_acc3, label='CNN50 Validation Accuracy')\n",
    "\n",
    "ax4.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax4.set_ylabel(r'Acc', fontsize=20)\n",
    "ax4.set_title('comparasion', fontsize=24)\n",
    "\n",
    "ax4.tick_params(labelsize=20)\n",
    "ax4.legend(loc=4, fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 15, 15, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_1 (Conv2D)       (None, 15, 15, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_1 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15, 64)   0           conv2_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_2 (Conv2D)       (None, 15, 15, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_2 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 64)   0           conv2_1_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_3 (Conv2D)       (None, 15, 15, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_shortcut (Conv2D (None, 15, 15, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_3 (BatchNormalizatio (None, 15, 15, 256)  1024        conv2_1_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_shortcut (BatchNorma (None, 15, 15, 256)  1024        conv2_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 15, 15, 256)  0           conv2_1_bn_3[0][0]               \n",
      "                                                                 conv2_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 15, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_1 (Conv2D)       (None, 15, 15, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_1 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 64)   0           conv2_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_2 (Conv2D)       (None, 15, 15, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_2 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 64)   0           conv2_2_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_3 (Conv2D)       (None, 15, 15, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_3 (BatchNormalizatio (None, 15, 15, 256)  1024        conv2_2_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 256)  0           conv2_2_bn_3[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_conv2d_1 (Conv2D)       (None, 15, 15, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_bn_1 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 64)   0           conv2_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_conv2d_2 (Conv2D)       (None, 15, 15, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_bn_2 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 64)   0           conv2_3_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_conv2d_3 (Conv2D)       (None, 15, 15, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_bn_3 (BatchNormalizatio (None, 15, 15, 256)  1024        conv2_3_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 15, 15, 256)  0           conv2_3_bn_3[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_1 (Conv2D)       (None, 8, 8, 128)    32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_1 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 128)    0           conv3_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_2 (Conv2D)       (None, 8, 8, 128)    147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_2 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 128)    0           conv3_1_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_3 (Conv2D)       (None, 8, 8, 512)    66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_shortcut (Conv2D (None, 8, 8, 512)    131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_3 (BatchNormalizatio (None, 8, 8, 512)    2048        conv3_1_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_shortcut (BatchNorma (None, 8, 8, 512)    2048        conv3_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 512)    0           conv3_1_bn_3[0][0]               \n",
      "                                                                 conv3_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_1 (Conv2D)       (None, 8, 8, 128)    65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_1 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 128)    0           conv3_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_2 (Conv2D)       (None, 8, 8, 128)    147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_2 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 128)    0           conv3_2_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_3 (Conv2D)       (None, 8, 8, 512)    66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_3 (BatchNormalizatio (None, 8, 8, 512)    2048        conv3_2_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 512)    0           conv3_2_bn_3[0][0]               \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_conv2d_1 (Conv2D)       (None, 8, 8, 128)    65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_bn_1 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 128)    0           conv3_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_conv2d_2 (Conv2D)       (None, 8, 8, 128)    147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_bn_2 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           conv3_3_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_conv2d_3 (Conv2D)       (None, 8, 8, 512)    66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_bn_3 (BatchNormalizatio (None, 8, 8, 512)    2048        conv3_3_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 512)    0           conv3_3_bn_3[0][0]               \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_conv2d_1 (Conv2D)       (None, 8, 8, 128)    65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_bn_1 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_4_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 128)    0           conv3_4_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_conv2d_2 (Conv2D)       (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_bn_2 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_4_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           conv3_4_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_conv2d_3 (Conv2D)       (None, 8, 8, 512)    66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_bn_3 (BatchNormalizatio (None, 8, 8, 512)    2048        conv3_4_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           conv3_4_bn_3[0][0]               \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_1 (Conv2D)       (None, 4, 4, 256)    131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 256)    0           conv4_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 256)    0           conv4_1_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_3 (Conv2D)       (None, 4, 4, 1024)   263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_shortcut (Conv2D (None, 4, 4, 1024)   525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_3 (BatchNormalizatio (None, 4, 4, 1024)   4096        conv4_1_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_shortcut (BatchNorma (None, 4, 4, 1024)   4096        conv4_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 1024)   0           conv4_1_bn_3[0][0]               \n",
      "                                                                 conv4_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 1024)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_1 (Conv2D)       (None, 4, 4, 256)    262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 256)    0           conv4_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 256)    0           conv4_2_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_3 (Conv2D)       (None, 4, 4, 1024)   263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_3 (BatchNormalizatio (None, 4, 4, 1024)   4096        conv4_2_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 1024)   0           conv4_2_bn_3[0][0]               \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_conv2d_1 (Conv2D)       (None, 4, 4, 256)    262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 256)    0           conv4_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 256)    0           conv4_3_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_conv2d_3 (Conv2D)       (None, 4, 4, 1024)   263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_bn_3 (BatchNormalizatio (None, 4, 4, 1024)   4096        conv4_3_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 1024)   0           conv4_3_bn_3[0][0]               \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_conv2d_1 (Conv2D)       (None, 4, 4, 256)    262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_4_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 256)    0           conv4_4_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_4_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 256)    0           conv4_4_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_conv2d_3 (Conv2D)       (None, 4, 4, 1024)   263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_bn_3 (BatchNormalizatio (None, 4, 4, 1024)   4096        conv4_4_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 1024)   0           conv4_4_bn_3[0][0]               \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_conv2d_1 (Conv2D)       (None, 4, 4, 256)    262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_5_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 256)    0           conv4_5_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_5_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 256)    0           conv4_5_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_conv2d_3 (Conv2D)       (None, 4, 4, 1024)   263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_bn_3 (BatchNormalizatio (None, 4, 4, 1024)   4096        conv4_5_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 1024)   0           conv4_5_bn_3[0][0]               \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_conv2d_1 (Conv2D)       (None, 4, 4, 256)    262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_6_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 256)    0           conv4_6_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_6_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 256)    0           conv4_6_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_conv2d_3 (Conv2D)       (None, 4, 4, 1024)   263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_bn_3 (BatchNormalizatio (None, 4, 4, 1024)   4096        conv4_6_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 1024)   0           conv4_6_bn_3[0][0]               \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_1 (Conv2D)       (None, 2, 2, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_1 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 512)    0           conv5_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_2 (Conv2D)       (None, 2, 2, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_2 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2, 2, 512)    0           conv5_1_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_3 (Conv2D)       (None, 2, 2, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_shortcut (Conv2D (None, 2, 2, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_3 (BatchNormalizatio (None, 2, 2, 2048)   8192        conv5_1_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_shortcut (BatchNorma (None, 2, 2, 2048)   8192        conv5_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 2, 2, 2048)   0           conv5_1_bn_3[0][0]               \n",
      "                                                                 conv5_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 2, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_conv2d_1 (Conv2D)       (None, 2, 2, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_bn_1 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 2, 512)    0           conv5_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_conv2d_2 (Conv2D)       (None, 2, 2, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_bn_2 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 2, 2, 512)    0           conv5_2_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_conv2d_3 (Conv2D)       (None, 2, 2, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_bn_3 (BatchNormalizatio (None, 2, 2, 2048)   8192        conv5_2_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 2, 2, 2048)   0           conv5_2_bn_3[0][0]               \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 2, 2, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_conv2d_1 (Conv2D)       (None, 2, 2, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_bn_1 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 2, 2, 512)    0           conv5_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_conv2d_2 (Conv2D)       (None, 2, 2, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_bn_2 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 2, 2, 512)    0           conv5_3_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_conv2d_3 (Conv2D)       (None, 2, 2, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_bn_3 (BatchNormalizatio (None, 2, 2, 2048)   8192        conv5_3_conv2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 2, 2, 2048)   0           conv5_3_bn_3[0][0]               \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2, 2, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Dense)                     (None, 6)            12294       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,600,006\n",
      "Trainable params: 23,546,886\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 70, 70, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_1 (Conv2D)       (None, 15, 15, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_1 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 15, 15, 64)   0           conv2_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_2 (Conv2D)       (None, 15, 15, 64)   36928       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_shortcut (Conv2D (None, 15, 15, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_2 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_shortcut (BatchNorma (None, 15, 15, 64)   256         conv2_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 15, 15, 64)   0           conv2_1_bn_2[0][0]               \n",
      "                                                                 conv2_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 15, 15, 64)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_1 (Conv2D)       (None, 15, 15, 64)   36928       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_1 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 15, 15, 64)   0           conv2_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_2 (Conv2D)       (None, 15, 15, 64)   36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_2 (BatchNormalizatio (None, 15, 15, 64)   256         conv2_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 15, 15, 64)   0           conv2_2_bn_2[0][0]               \n",
      "                                                                 activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 15, 15, 64)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_1 (Conv2D)       (None, 8, 8, 128)    73856       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_1 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 8, 8, 128)    0           conv3_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_2 (Conv2D)       (None, 8, 8, 128)    147584      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_shortcut (Conv2D (None, 8, 8, 128)    8320        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_2 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_shortcut (BatchNorma (None, 8, 8, 128)    512         conv3_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 128)    0           conv3_1_bn_2[0][0]               \n",
      "                                                                 conv3_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 8, 8, 128)    0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_1 (Conv2D)       (None, 8, 8, 128)    147584      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_1 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 8, 8, 128)    0           conv3_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_2 (Conv2D)       (None, 8, 8, 128)    147584      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_2 (BatchNormalizatio (None, 8, 8, 128)    512         conv3_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 128)    0           conv3_2_bn_2[0][0]               \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 8, 8, 128)    0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_1 (Conv2D)       (None, 4, 4, 256)    295168      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 4, 4, 256)    0           conv4_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_shortcut (Conv2D (None, 4, 4, 256)    33024       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_shortcut (BatchNorma (None, 4, 4, 256)    1024        conv4_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 4, 4, 256)    0           conv4_1_bn_2[0][0]               \n",
      "                                                                 conv4_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 4, 4, 256)    0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_1 (Conv2D)       (None, 4, 4, 256)    590080      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_1 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 4, 4, 256)    0           conv4_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_2 (Conv2D)       (None, 4, 4, 256)    590080      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_2 (BatchNormalizatio (None, 4, 4, 256)    1024        conv4_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 4, 4, 256)    0           conv4_2_bn_2[0][0]               \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 4, 4, 256)    0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_1 (Conv2D)       (None, 2, 2, 512)    1180160     activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_1 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 2, 2, 512)    0           conv5_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_2 (Conv2D)       (None, 2, 2, 512)    2359808     activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_conv2d_shortcut (Conv2D (None, 2, 2, 512)    131584      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_2 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_bn_shortcut (BatchNorma (None, 2, 2, 512)    2048        conv5_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 2, 2, 512)    0           conv5_1_bn_2[0][0]               \n",
      "                                                                 conv5_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 2, 2, 512)    0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_conv2d_1 (Conv2D)       (None, 2, 2, 512)    2359808     activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_bn_1 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 2, 2, 512)    0           conv5_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_conv2d_2 (Conv2D)       (None, 2, 2, 512)    2359808     activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_bn_2 (BatchNormalizatio (None, 2, 2, 512)    2048        conv5_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 2, 2, 512)    0           conv5_2_bn_2[0][0]               \n",
      "                                                                 activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 2, 2, 512)    0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 512)    0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Dense)                     (None, 6)            3078        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,198,406\n",
      "Trainable params: 11,188,678\n",
      "Non-trainable params: 9,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 32, 32, 64)        9472      \n",
      "_________________________________________________________________\n",
      "bn_conv1 (BatchNormalization (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-1_conv2d_1 (Conv2D)   (None, 15, 15, 64)        4160      \n",
      "_________________________________________________________________\n",
      "stage2-1_bn_1 (BatchNormaliz (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-1_conv2d_2 (Conv2D)   (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "stage2-1_bn_2 (BatchNormaliz (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-1_conv2d_3 (Conv2D)   (None, 15, 15, 256)       16640     \n",
      "_________________________________________________________________\n",
      "stage2-1_bn_3 (BatchNormaliz (None, 15, 15, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "stage2-2_conv2d_1 (Conv2D)   (None, 15, 15, 64)        16448     \n",
      "_________________________________________________________________\n",
      "stage2-2_bn_1 (BatchNormaliz (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-2_conv2d_2 (Conv2D)   (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "stage2-2_bn_2 (BatchNormaliz (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-2_conv2d_3 (Conv2D)   (None, 15, 15, 256)       16640     \n",
      "_________________________________________________________________\n",
      "stage2-2_bn_3 (BatchNormaliz (None, 15, 15, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "stage2-3_conv2d_1 (Conv2D)   (None, 15, 15, 64)        16448     \n",
      "_________________________________________________________________\n",
      "stage2-3_bn_1 (BatchNormaliz (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-3_conv2d_2 (Conv2D)   (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "stage2-3_bn_2 (BatchNormaliz (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "stage2-3_conv2d_3 (Conv2D)   (None, 15, 15, 256)       16640     \n",
      "_________________________________________________________________\n",
      "stage2-3_bn_3 (BatchNormaliz (None, 15, 15, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "stage3-1_conv2d_1 (Conv2D)   (None, 8, 8, 128)         32896     \n",
      "_________________________________________________________________\n",
      "stage3-1_bn_1 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-1_conv2d_2 (Conv2D)   (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "stage3-1_bn_2 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-1_conv2d_3 (Conv2D)   (None, 8, 8, 512)         66048     \n",
      "_________________________________________________________________\n",
      "stage3-1_bn_3 (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage3-2_conv2d_1 (Conv2D)   (None, 8, 8, 128)         65664     \n",
      "_________________________________________________________________\n",
      "stage3-2_bn_1 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-2_conv2d_2 (Conv2D)   (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "stage3-2_bn_2 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-2_conv2d_3 (Conv2D)   (None, 8, 8, 512)         66048     \n",
      "_________________________________________________________________\n",
      "stage3-2_bn_3 (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage3-3_conv2d_1 (Conv2D)   (None, 8, 8, 128)         65664     \n",
      "_________________________________________________________________\n",
      "stage3-3_bn_1 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-3_conv2d_2 (Conv2D)   (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "stage3-3_bn_2 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-3_conv2d_3 (Conv2D)   (None, 8, 8, 512)         66048     \n",
      "_________________________________________________________________\n",
      "stage3-3_bn_3 (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage3-4_conv2d_1 (Conv2D)   (None, 8, 8, 128)         65664     \n",
      "_________________________________________________________________\n",
      "stage3-4_bn_1 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-4_conv2d_2 (Conv2D)   (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "stage3-4_bn_2 (BatchNormaliz (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "stage3-4_conv2d_3 (Conv2D)   (None, 8, 8, 512)         66048     \n",
      "_________________________________________________________________\n",
      "stage3-4_bn_3 (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage4-1_conv2d_1 (Conv2D)   (None, 4, 4, 256)         131328    \n",
      "_________________________________________________________________\n",
      "stage4-1_bn_1 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-1_conv2d_2 (Conv2D)   (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "stage4-1_bn_2 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-1_conv2d_3 (Conv2D)   (None, 4, 4, 1024)        263168    \n",
      "_________________________________________________________________\n",
      "stage4-1_bn_3 (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "stage4-2_conv2d_1 (Conv2D)   (None, 4, 4, 256)         262400    \n",
      "_________________________________________________________________\n",
      "stage4-2_bn_1 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-2_conv2d_2 (Conv2D)   (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "stage4-2_bn_2 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-2_conv2d_3 (Conv2D)   (None, 4, 4, 1024)        263168    \n",
      "_________________________________________________________________\n",
      "stage4-2_bn_3 (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "stage4-3_conv2d_1 (Conv2D)   (None, 4, 4, 256)         262400    \n",
      "_________________________________________________________________\n",
      "stage4-3_bn_1 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-3_conv2d_2 (Conv2D)   (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "stage4-3_bn_2 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-3_conv2d_3 (Conv2D)   (None, 4, 4, 1024)        263168    \n",
      "_________________________________________________________________\n",
      "stage4-3_bn_3 (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "stage4-4_conv2d_1 (Conv2D)   (None, 4, 4, 256)         262400    \n",
      "_________________________________________________________________\n",
      "stage4-4_bn_1 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-4_conv2d_2 (Conv2D)   (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "stage4-4_bn_2 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-4_conv2d_3 (Conv2D)   (None, 4, 4, 1024)        263168    \n",
      "_________________________________________________________________\n",
      "stage4-4_bn_3 (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "stage4-5_conv2d_1 (Conv2D)   (None, 4, 4, 256)         262400    \n",
      "_________________________________________________________________\n",
      "stage4-5_bn_1 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-5_conv2d_2 (Conv2D)   (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "stage4-5_bn_2 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-5_conv2d_3 (Conv2D)   (None, 4, 4, 1024)        263168    \n",
      "_________________________________________________________________\n",
      "stage4-5_bn_3 (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "stage4-6_conv2d_1 (Conv2D)   (None, 4, 4, 256)         262400    \n",
      "_________________________________________________________________\n",
      "stage4-6_bn_1 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-6_conv2d_2 (Conv2D)   (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "stage4-6_bn_2 (BatchNormaliz (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "stage4-6_conv2d_3 (Conv2D)   (None, 4, 4, 1024)        263168    \n",
      "_________________________________________________________________\n",
      "stage4-6_bn_3 (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "stage5-1_conv2d_1 (Conv2D)   (None, 2, 2, 512)         524800    \n",
      "_________________________________________________________________\n",
      "stage5-1_bn_1 (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage5-1_conv2d_2 (Conv2D)   (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "stage5-1_bn_2 (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage5-1_conv2d_3 (Conv2D)   (None, 2, 2, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "stage5-1_bn_3 (BatchNormaliz (None, 2, 2, 2048)        8192      \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 2, 2, 2048)        0         \n",
      "_________________________________________________________________\n",
      "stage5-2_conv2d_1 (Conv2D)   (None, 2, 2, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "stage5-2_bn_1 (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage5-2_conv2d_2 (Conv2D)   (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "stage5-2_bn_2 (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage5-2_conv2d_3 (Conv2D)   (None, 2, 2, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "stage5-2_bn_3 (BatchNormaliz (None, 2, 2, 2048)        8192      \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 2, 2, 2048)        0         \n",
      "_________________________________________________________________\n",
      "stage5-3_conv2d_1 (Conv2D)   (None, 2, 2, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "stage5-3_bn_1 (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage5-3_conv2d_2 (Conv2D)   (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "stage5-3_bn_2 (BatchNormaliz (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "stage5-3_conv2d_3 (Conv2D)   (None, 2, 2, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "stage5-3_bn_3 (BatchNormaliz (None, 2, 2, 2048)        8192      \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 2, 2, 2048)        0         \n",
      "_________________________________________________________________\n",
      "avg_pool (AveragePooling2D)  (None, 1, 1, 2048)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 20,811,910\n",
      "Trainable params: 20,766,470\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
