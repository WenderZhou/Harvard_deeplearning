{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras as keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 50000\n",
      "number of test examples = 10000\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000, 10)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = cifar10.load_data()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "X_train_mean = np.mean(X_train, axis=0)\n",
    "X_train -= X_train_mean\n",
    "X_test -= X_train_mean\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = keras.utils.to_categorical(Y_train_orig, 10)\n",
    "Y_test = keras.utils.to_categorical(Y_test_orig, 10)    \n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a learning rate scheduler to change the learning rate\n",
    "def my_schedule(epoch):\n",
    "    if epoch > 180:\n",
    "        learning_rate = 5e-7\n",
    "    elif epoch > 160:\n",
    "        learning_rate = 1e-6\n",
    "    elif epoch > 120:\n",
    "        learning_rate = 1e-5\n",
    "    elif epoch > 80:\n",
    "        learning_rate = 1e-4\n",
    "    else:\n",
    "        learning_rate = 1e-3\n",
    "    print('Learning rate: ', learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "scheduler = LearningRateScheduler(my_schedule)\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'Res_cifar10_l2_model.h5' \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\19244\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Learning rate:  0.001\n",
      "Model: \"ResNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_1 (Conv2D)       (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_1 (BatchNormalizatio (None, 32, 32, 16)   64          conv2_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           conv2_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_2 (Conv2D)       (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_conv2d_shortcut (Conv2D (None, 32, 32, 16)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_2 (BatchNormalizatio (None, 32, 32, 16)   64          conv2_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_bn_shortcut (BatchNorma (None, 32, 32, 16)   64          conv2_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           conv2_1_bn_2[0][0]               \n",
      "                                                                 conv2_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_1 (Conv2D)       (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_1 (BatchNormalizatio (None, 32, 32, 16)   64          conv2_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           conv2_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_conv2d_2 (Conv2D)       (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_bn_2 (BatchNormalizatio (None, 32, 32, 16)   64          conv2_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           conv2_2_bn_2[0][0]               \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_conv2d_1 (Conv2D)       (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_bn_1 (BatchNormalizatio (None, 32, 32, 16)   64          conv2_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           conv2_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_conv2d_2 (Conv2D)       (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_bn_2 (BatchNormalizatio (None, 32, 32, 16)   64          conv2_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           conv2_3_bn_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_1 (Conv2D)       (None, 16, 16, 32)   4640        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_1 (BatchNormalizatio (None, 16, 16, 32)   128         conv3_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 32)   0           conv3_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_2 (Conv2D)       (None, 16, 16, 32)   9248        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_conv2d_shortcut (Conv2D (None, 16, 16, 32)   544         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_2 (BatchNormalizatio (None, 16, 16, 32)   128         conv3_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_bn_shortcut (BatchNorma (None, 16, 16, 32)   128         conv3_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32)   0           conv3_1_bn_2[0][0]               \n",
      "                                                                 conv3_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_1 (Conv2D)       (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_1 (BatchNormalizatio (None, 16, 16, 32)   128         conv3_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           conv3_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_conv2d_2 (Conv2D)       (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_bn_2 (BatchNormalizatio (None, 16, 16, 32)   128         conv3_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           conv3_2_bn_2[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_conv2d_1 (Conv2D)       (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_bn_1 (BatchNormalizatio (None, 16, 16, 32)   128         conv3_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           conv3_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_conv2d_2 (Conv2D)       (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_bn_2 (BatchNormalizatio (None, 16, 16, 32)   128         conv3_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           conv3_3_bn_2[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_1 (Conv2D)       (None, 8, 8, 64)     18496       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_1 (BatchNormalizatio (None, 8, 8, 64)     256         conv4_1_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 64)     0           conv4_1_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_2 (Conv2D)       (None, 8, 8, 64)     36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_conv2d_shortcut (Conv2D (None, 8, 8, 64)     2112        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_2 (BatchNormalizatio (None, 8, 8, 64)     256         conv4_1_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_bn_shortcut (BatchNorma (None, 8, 8, 64)     256         conv4_1_conv2d_shortcut[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 64)     0           conv4_1_bn_2[0][0]               \n",
      "                                                                 conv4_1_bn_shortcut[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_1 (Conv2D)       (None, 8, 8, 64)     36928       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_1 (BatchNormalizatio (None, 8, 8, 64)     256         conv4_2_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           conv4_2_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_conv2d_2 (Conv2D)       (None, 8, 8, 64)     36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_bn_2 (BatchNormalizatio (None, 8, 8, 64)     256         conv4_2_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           conv4_2_bn_2[0][0]               \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_conv2d_1 (Conv2D)       (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_bn_1 (BatchNormalizatio (None, 8, 8, 64)     256         conv4_3_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           conv4_3_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_conv2d_2 (Conv2D)       (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_bn_2 (BatchNormalizatio (None, 8, 8, 64)     256         conv4_3_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           conv4_3_bn_2[0][0]               \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 64)     0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc10 (Dense)                    (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 272,570\n",
      "Trainable params: 271,002\n",
      "Non-trainable params: 1,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from cifar10_net import ResNet20\n",
    "model1 = ResNet20(input_shape = (32, 32, 3), classes = 10)\n",
    "model1.compile(optimizer=optimizers.Adam(learning_rate=my_schedule(0)), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    data_format=None,\n",
    "    validation_split = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.5373 - acc: 0.4678Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 278us/sample - loss: 1.3908 - acc: 0.5258\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52580, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 1.5370 - acc: 0.4679 - val_loss: 1.4150 - val_acc: 0.5258\n",
      "Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.1200 - acc: 0.6313Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 292us/sample - loss: 1.0592 - acc: 0.5990\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.52580 to 0.59900, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 1.1197 - acc: 0.6314 - val_loss: 1.3087 - val_acc: 0.5990\n",
      "Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.9577 - acc: 0.7010Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 297us/sample - loss: 1.2993 - acc: 0.5832\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.59900\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.9577 - acc: 0.7010 - val_loss: 1.3510 - val_acc: 0.5832\n",
      "Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.8740 - acc: 0.7342- ETA: 1Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 380us/sample - loss: 1.0170 - acc: 0.7199\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.59900 to 0.71990, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.8739 - acc: 0.7342 - val_loss: 0.9672 - val_acc: 0.7199\n",
      "Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.8100 - acc: 0.7636Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 289us/sample - loss: 1.0200 - acc: 0.6651\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.71990\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8101 - acc: 0.7636 - val_loss: 1.1182 - val_acc: 0.6651\n",
      "Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.7707 - acc: 0.7807Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 261us/sample - loss: 0.9048 - acc: 0.7659\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.71990 to 0.76590, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.7705 - acc: 0.7807 - val_loss: 0.8207 - val_acc: 0.7659\n",
      "Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.7414 - acc: 0.7924Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 288us/sample - loss: 0.7636 - acc: 0.7674\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76590 to 0.76740, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 48s 30ms/step - loss: 0.7413 - acc: 0.7924 - val_loss: 0.8317 - val_acc: 0.7674\n",
      "Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.7151 - acc: 0.8047Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 312us/sample - loss: 0.7649 - acc: 0.7776\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.76740 to 0.77760, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.7152 - acc: 0.8047 - val_loss: 0.8160 - val_acc: 0.7776\n",
      "Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.8111Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 251us/sample - loss: 0.6447 - acc: 0.8062\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.77760 to 0.80620, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6968 - acc: 0.8111 - val_loss: 0.7135 - val_acc: 0.8062\n",
      "Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.6740 - acc: 0.8215Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 259us/sample - loss: 0.8029 - acc: 0.7742\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80620\n",
      "1563/1563 [==============================] - 45s 29ms/step - loss: 0.6739 - acc: 0.8216 - val_loss: 0.8394 - val_acc: 0.7742\n",
      "Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.8277Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 284us/sample - loss: 0.7984 - acc: 0.7816\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80620\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.6600 - acc: 0.8277 - val_loss: 0.8199 - val_acc: 0.7816\n",
      "Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.8311Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 295us/sample - loss: 0.8965 - acc: 0.7828\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80620\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.6471 - acc: 0.8310 - val_loss: 0.8397 - val_acc: 0.7828\n",
      "Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.6381 - acc: 0.8379Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 253us/sample - loss: 0.8413 - acc: 0.8038\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80620\n",
      "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6380 - acc: 0.8380 - val_loss: 0.7357 - val_acc: 0.8038\n",
      "Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6259 - acc: 0.8431Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 287us/sample - loss: 0.8510 - acc: 0.7860\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80620\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6258 - acc: 0.8432 - val_loss: 0.8501 - val_acc: 0.7860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6123 - acc: 0.8475Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 331us/sample - loss: 0.7576 - acc: 0.8075\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.80620 to 0.80750, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.6123 - acc: 0.8475 - val_loss: 0.7619 - val_acc: 0.8075\n",
      "Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6072 - acc: 0.8491Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 339us/sample - loss: 0.6949 - acc: 0.8364\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.80750 to 0.83640, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.6074 - acc: 0.8490 - val_loss: 0.6629 - val_acc: 0.8364\n",
      "Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.8517Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 332us/sample - loss: 0.7402 - acc: 0.8060\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83640\n",
      "1563/1563 [==============================] - 54s 34ms/step - loss: 0.6035 - acc: 0.8517 - val_loss: 0.7600 - val_acc: 0.8060\n",
      "Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8578Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 334us/sample - loss: 0.7518 - acc: 0.8311\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83640\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5891 - acc: 0.8578 - val_loss: 0.6769 - val_acc: 0.8311\n",
      "Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.8590Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 326us/sample - loss: 0.7470 - acc: 0.8147\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83640\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5893 - acc: 0.8590 - val_loss: 0.7626 - val_acc: 0.8147\n",
      "Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5814 - acc: 0.8622Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 268us/sample - loss: 0.6645 - acc: 0.8395\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.83640 to 0.83950, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5815 - acc: 0.8621 - val_loss: 0.6649 - val_acc: 0.8395\n",
      "Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5745 - acc: 0.8640Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 298us/sample - loss: 0.8658 - acc: 0.8025\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83950\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5744 - acc: 0.8640 - val_loss: 0.7832 - val_acc: 0.8025\n",
      "Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.8656Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 319us/sample - loss: 0.8027 - acc: 0.8188\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83950\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5723 - acc: 0.8657 - val_loss: 0.7303 - val_acc: 0.8188\n",
      "Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5665 - acc: 0.8681Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 330us/sample - loss: 0.7861 - acc: 0.8179\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83950\n",
      "1563/1563 [==============================] - 52s 34ms/step - loss: 0.5668 - acc: 0.8680 - val_loss: 0.7422 - val_acc: 0.8179\n",
      "Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8684Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 323us/sample - loss: 0.6591 - acc: 0.8239\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83950\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5634 - acc: 0.8683 - val_loss: 0.7233 - val_acc: 0.8239\n",
      "Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.8697Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 363us/sample - loss: 0.8088 - acc: 0.8211\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83950\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5607 - acc: 0.8698 - val_loss: 0.7348 - val_acc: 0.8211\n",
      "Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.8716Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 303us/sample - loss: 0.6230 - acc: 0.8430\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.83950 to 0.84300, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5585 - acc: 0.8717 - val_loss: 0.6579 - val_acc: 0.8430\n",
      "Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8751Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 422us/sample - loss: 0.8713 - acc: 0.7989\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.5512 - acc: 0.8751 - val_loss: 0.8390 - val_acc: 0.7989\n",
      "Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.8747Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 318us/sample - loss: 0.7972 - acc: 0.8239\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.5498 - acc: 0.8747 - val_loss: 0.7092 - val_acc: 0.8239\n",
      "Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.8769Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 325us/sample - loss: 0.8639 - acc: 0.8233\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5434 - acc: 0.8769 - val_loss: 0.7393 - val_acc: 0.8233\n",
      "Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8791Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 378us/sample - loss: 0.8826 - acc: 0.8310\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5442 - acc: 0.8791 - val_loss: 0.7272 - val_acc: 0.8310\n",
      "Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.8791Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 288us/sample - loss: 0.7716 - acc: 0.8345\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 0.5407 - acc: 0.8792 - val_loss: 0.7028 - val_acc: 0.8345\n",
      "Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.8809Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 329us/sample - loss: 0.7766 - acc: 0.8333\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.5357 - acc: 0.8809 - val_loss: 0.7036 - val_acc: 0.8333\n",
      "Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8810- ETA: 1s - loss: Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 351us/sample - loss: 0.8641 - acc: 0.8306\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84300\n",
      "1563/1563 [==============================] - 54s 34ms/step - loss: 0.5359 - acc: 0.8810 - val_loss: 0.7128 - val_acc: 0.8306\n",
      "Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.8804Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 303us/sample - loss: 0.6120 - acc: 0.8544\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.84300 to 0.85440, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5372 - acc: 0.8803 - val_loss: 0.6398 - val_acc: 0.8544\n",
      "Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8834Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 332us/sample - loss: 0.7025 - acc: 0.8442\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85440\n",
      "1563/1563 [==============================] - 52s 34ms/step - loss: 0.5290 - acc: 0.8834 - val_loss: 0.6579 - val_acc: 0.8442\n",
      "Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8819Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 298us/sample - loss: 0.6376 - acc: 0.8419\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85440\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.5290 - acc: 0.8819 - val_loss: 0.6814 - val_acc: 0.8419\n",
      "Learning rate:  0.001\n",
      "Epoch 37/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.8853- ETA: 1s - loEpoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 333us/sample - loss: 0.7060 - acc: 0.8495\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85440\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5265 - acc: 0.8853 - val_loss: 0.6522 - val_acc: 0.8495\n",
      "Learning rate:  0.001\n",
      "Epoch 38/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8845Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 311us/sample - loss: 0.6112 - acc: 0.8434\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85440\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.5251 - acc: 0.8845 - val_loss: 0.6805 - val_acc: 0.8434\n",
      "Learning rate:  0.001\n",
      "Epoch 39/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8861Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 369us/sample - loss: 0.7594 - acc: 0.8369\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85440\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5239 - acc: 0.8861 - val_loss: 0.7054 - val_acc: 0.8369\n",
      "Learning rate:  0.001\n",
      "Epoch 40/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8865Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 323us/sample - loss: 0.8832 - acc: 0.8565\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.85440 to 0.85650, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5214 - acc: 0.8865 - val_loss: 0.6440 - val_acc: 0.8565\n",
      "Learning rate:  0.001\n",
      "Epoch 41/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.888 - ETA: 0s - loss: 0.5174 - acc: 0.8889Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 359us/sample - loss: 0.8026 - acc: 0.8403\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5176 - acc: 0.8889 - val_loss: 0.7044 - val_acc: 0.8403\n",
      "Learning rate:  0.001\n",
      "Epoch 42/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8888Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 347us/sample - loss: 0.8096 - acc: 0.8405\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.5168 - acc: 0.8887 - val_loss: 0.6912 - val_acc: 0.8405\n",
      "Learning rate:  0.001\n",
      "Epoch 43/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8892Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 324us/sample - loss: 0.9058 - acc: 0.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5169 - acc: 0.8891 - val_loss: 0.7111 - val_acc: 0.8435\n",
      "Learning rate:  0.001\n",
      "Epoch 44/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8880Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 361us/sample - loss: 0.9431 - acc: 0.8363\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5157 - acc: 0.8880 - val_loss: 0.6843 - val_acc: 0.8363\n",
      "Learning rate:  0.001\n",
      "Epoch 45/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8898- ETA: 4s - los - ETA: 2s - loss: 0.5103 - acEpoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 319us/sample - loss: 0.8151 - acc: 0.8490\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.5113 - acc: 0.8898 - val_loss: 0.6485 - val_acc: 0.8490\n",
      "Learning rate:  0.001\n",
      "Epoch 46/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.8893Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 351us/sample - loss: 0.6351 - acc: 0.8564\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 54s 34ms/step - loss: 0.5143 - acc: 0.8892 - val_loss: 0.6374 - val_acc: 0.8564\n",
      "Learning rate:  0.001\n",
      "Epoch 47/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8902- ETA: 5s - loss: 0.5103 - Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 338us/sample - loss: 0.7272 - acc: 0.8458\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.5097 - acc: 0.8903 - val_loss: 0.6601 - val_acc: 0.8458\n",
      "Learning rate:  0.001\n",
      "Epoch 48/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8923- ETA: 0s - loss: 0.5097 Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 315us/sample - loss: 0.8574 - acc: 0.8243\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.85650\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5099 - acc: 0.8923 - val_loss: 0.7438 - val_acc: 0.8243\n",
      "Learning rate:  0.001\n",
      "Epoch 49/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8933Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 369us/sample - loss: 0.6774 - acc: 0.8619\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.85650 to 0.86190, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 54s 34ms/step - loss: 0.5069 - acc: 0.8933 - val_loss: 0.6123 - val_acc: 0.8619\n",
      "Learning rate:  0.001\n",
      "Epoch 50/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8941Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 304us/sample - loss: 0.8740 - acc: 0.8468\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.5030 - acc: 0.8941 - val_loss: 0.6660 - val_acc: 0.8468\n",
      "Learning rate:  0.001\n",
      "Epoch 51/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.8921Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 340us/sample - loss: 0.8331 - acc: 0.8565\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.5068 - acc: 0.8920 - val_loss: 0.6318 - val_acc: 0.8565\n",
      "Learning rate:  0.001\n",
      "Epoch 52/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8915Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 356us/sample - loss: 0.6975 - acc: 0.8613\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.5063 - acc: 0.8915 - val_loss: 0.6215 - val_acc: 0.8613\n",
      "Learning rate:  0.001\n",
      "Epoch 53/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8937Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 297us/sample - loss: 0.9501 - acc: 0.8119\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4984 - acc: 0.8937 - val_loss: 0.7958 - val_acc: 0.8119\n",
      "Learning rate:  0.001\n",
      "Epoch 54/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8941Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 346us/sample - loss: 0.7435 - acc: 0.8464\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.5017 - acc: 0.8942 - val_loss: 0.6687 - val_acc: 0.8464\n",
      "Learning rate:  0.001\n",
      "Epoch 55/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8958Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 353us/sample - loss: 0.8717 - acc: 0.8143\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.4976 - acc: 0.8958 - val_loss: 0.8001 - val_acc: 0.8143\n",
      "Learning rate:  0.001\n",
      "Epoch 56/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4998 - acc: 0.8942Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 309us/sample - loss: 0.9198 - acc: 0.8417\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.4998 - acc: 0.8942 - val_loss: 0.6946 - val_acc: 0.8417\n",
      "Learning rate:  0.001\n",
      "Epoch 57/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4973 - acc: 0.8945Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 339us/sample - loss: 0.7864 - acc: 0.8479\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.4975 - acc: 0.8944 - val_loss: 0.6613 - val_acc: 0.8479\n",
      "Learning rate:  0.001\n",
      "Epoch 58/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8960- ETA: Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 314us/sample - loss: 0.8368 - acc: 0.8227\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4912 - acc: 0.8960 - val_loss: 0.7067 - val_acc: 0.8227\n",
      "Learning rate:  0.001\n",
      "Epoch 59/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8981Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 340us/sample - loss: 0.9265 - acc: 0.8178\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4924 - acc: 0.8981 - val_loss: 0.7772 - val_acc: 0.8178\n",
      "Learning rate:  0.001\n",
      "Epoch 60/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8975Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 313us/sample - loss: 0.8324 - acc: 0.8361\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 0.4931 - acc: 0.8974 - val_loss: 0.7105 - val_acc: 0.8361\n",
      "Learning rate:  0.001\n",
      "Epoch 61/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8963Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 315us/sample - loss: 0.7744 - acc: 0.8468\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.4925 - acc: 0.8963 - val_loss: 0.6745 - val_acc: 0.8468\n",
      "Learning rate:  0.001\n",
      "Epoch 62/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8964Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 341us/sample - loss: 0.7717 - acc: 0.8297\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.4926 - acc: 0.8964 - val_loss: 0.7420 - val_acc: 0.8297\n",
      "Learning rate:  0.001\n",
      "Epoch 63/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8986Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 306us/sample - loss: 0.7989 - acc: 0.8496\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.4894 - acc: 0.8986 - val_loss: 0.6702 - val_acc: 0.8496\n",
      "Learning rate:  0.001\n",
      "Epoch 64/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4914 - acc: 0.8965Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 335us/sample - loss: 0.7646 - acc: 0.8420\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4914 - acc: 0.8964 - val_loss: 0.6661 - val_acc: 0.8420\n",
      "Learning rate:  0.001\n",
      "Epoch 65/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8991Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 327us/sample - loss: 0.9867 - acc: 0.8251\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.4881 - acc: 0.8991 - val_loss: 0.7747 - val_acc: 0.8251\n",
      "Learning rate:  0.001\n",
      "Epoch 66/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8978Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 291us/sample - loss: 0.6203 - acc: 0.8535\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.4869 - acc: 0.8978 - val_loss: 0.6415 - val_acc: 0.8535\n",
      "Learning rate:  0.001\n",
      "Epoch 67/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8996Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 354us/sample - loss: 0.7773 - acc: 0.8529\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4871 - acc: 0.8996 - val_loss: 0.6688 - val_acc: 0.8529\n",
      "Learning rate:  0.001\n",
      "Epoch 68/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8995Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 305us/sample - loss: 0.8686 - acc: 0.8234\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 52s 34ms/step - loss: 0.4850 - acc: 0.8995 - val_loss: 0.7477 - val_acc: 0.8234\n",
      "Learning rate:  0.001\n",
      "Epoch 69/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8986Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 316us/sample - loss: 0.7167 - acc: 0.8575\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.86190\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.4858 - acc: 0.8985 - val_loss: 0.6244 - val_acc: 0.8575\n",
      "Learning rate:  0.001\n",
      "Epoch 70/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4844 - acc: 0.8991Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 336us/sample - loss: 0.6542 - acc: 0.8698\n",
      "\n",
      "Epoch 00070: val_acc improved from 0.86190 to 0.86980, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.4844 - acc: 0.8990 - val_loss: 0.5914 - val_acc: 0.8698\n",
      "Learning rate:  0.001\n",
      "Epoch 71/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.8996Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 296us/sample - loss: 0.6542 - acc: 0.8649\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.86980\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4807 - acc: 0.8996 - val_loss: 0.6051 - val_acc: 0.8649\n",
      "Learning rate:  0.001\n",
      "Epoch 72/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8995Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 348us/sample - loss: 0.8522 - acc: 0.8562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_acc did not improve from 0.86980\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.4848 - acc: 0.8995 - val_loss: 0.6566 - val_acc: 0.8562\n",
      "Learning rate:  0.001\n",
      "Epoch 73/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.9002Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 324us/sample - loss: 0.7457 - acc: 0.8650\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.86980\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4818 - acc: 0.9002 - val_loss: 0.6281 - val_acc: 0.8650\n",
      "Learning rate:  0.001\n",
      "Epoch 74/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.9007Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 310us/sample - loss: 0.5215 - acc: 0.8695\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.86980\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4783 - acc: 0.9007 - val_loss: 0.5934 - val_acc: 0.8695\n",
      "Learning rate:  0.001\n",
      "Epoch 75/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.9012Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 334us/sample - loss: 0.5778 - acc: 0.8728\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.86980 to 0.87280, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4812 - acc: 0.9011 - val_loss: 0.5942 - val_acc: 0.8728\n",
      "Learning rate:  0.001\n",
      "Epoch 76/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.8992Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 350us/sample - loss: 0.5250 - acc: 0.8480\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87280\n",
      "1563/1563 [==============================] - 54s 34ms/step - loss: 0.4808 - acc: 0.8992 - val_loss: 0.6663 - val_acc: 0.8480\n",
      "Learning rate:  0.001\n",
      "Epoch 77/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.9026Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 321us/sample - loss: 0.8043 - acc: 0.8032\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87280\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4756 - acc: 0.9025 - val_loss: 0.8414 - val_acc: 0.8032\n",
      "Learning rate:  0.001\n",
      "Epoch 78/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.9029Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 363us/sample - loss: 0.6557 - acc: 0.8652\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87280\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.4769 - acc: 0.9030 - val_loss: 0.6055 - val_acc: 0.8652\n",
      "Learning rate:  0.001\n",
      "Epoch 79/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.9014- ETAEpoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 335us/sample - loss: 0.5055 - acc: 0.8762\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.87280 to 0.87620, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 54s 35ms/step - loss: 0.4771 - acc: 0.9013 - val_loss: 0.5755 - val_acc: 0.8762\n",
      "Learning rate:  0.001\n",
      "Epoch 80/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4756 - acc: 0.9013Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 305us/sample - loss: 0.5390 - acc: 0.8585\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.87620\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.4756 - acc: 0.9013 - val_loss: 0.6317 - val_acc: 0.8585\n",
      "Learning rate:  0.001\n",
      "Epoch 81/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4755 - acc: 0.9019- ETA: 1Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 350us/sample - loss: 0.7073 - acc: 0.8365\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87620\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.4756 - acc: 0.9019 - val_loss: 0.7104 - val_acc: 0.8365\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.9295Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 301us/sample - loss: 0.5040 - acc: 0.9020\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.87620 to 0.90200, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.3984 - acc: 0.9295 - val_loss: 0.4871 - val_acc: 0.9020\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.9388Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 328us/sample - loss: 0.5269 - acc: 0.9054\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.90200 to 0.90540, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.3671 - acc: 0.9388 - val_loss: 0.4789 - val_acc: 0.9054\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.9451Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 304us/sample - loss: 0.5120 - acc: 0.9080\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.90540 to 0.90800, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.3473 - acc: 0.9451 - val_loss: 0.4708 - val_acc: 0.9080\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.9456Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 335us/sample - loss: 0.5129 - acc: 0.9093\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.90800 to 0.90930, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.3384 - acc: 0.9456 - val_loss: 0.4648 - val_acc: 0.9093\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9477Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 321us/sample - loss: 0.4975 - acc: 0.9071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90930\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.3298 - acc: 0.9476 - val_loss: 0.4663 - val_acc: 0.9071\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.9507- ETA:Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 325us/sample - loss: 0.5308 - acc: 0.9097\n",
      "\n",
      "Epoch 00087: val_acc improved from 0.90930 to 0.90970, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.3205 - acc: 0.9507 - val_loss: 0.4642 - val_acc: 0.9097\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9510Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 283us/sample - loss: 0.5092 - acc: 0.9102\n",
      "\n",
      "Epoch 00088: val_acc improved from 0.90970 to 0.91020, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.3142 - acc: 0.9510 - val_loss: 0.4554 - val_acc: 0.9102\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9539- Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 316us/sample - loss: 0.5186 - acc: 0.9112\n",
      "\n",
      "Epoch 00089: val_acc improved from 0.91020 to 0.91120, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.3052 - acc: 0.9539 - val_loss: 0.4580 - val_acc: 0.9112\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9544Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 340us/sample - loss: 0.4693 - acc: 0.9127\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.91120 to 0.91270, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.3010 - acc: 0.9545 - val_loss: 0.4499 - val_acc: 0.9127\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9554- ETA:Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 297us/sample - loss: 0.5000 - acc: 0.9089\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91270\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.2978 - acc: 0.9554 - val_loss: 0.4599 - val_acc: 0.9089\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9556Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 342us/sample - loss: 0.4862 - acc: 0.9086\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91270\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.2906 - acc: 0.9556 - val_loss: 0.4708 - val_acc: 0.9086\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9567Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 330us/sample - loss: 0.4860 - acc: 0.9128\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.91270 to 0.91280, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.2863 - acc: 0.9567 - val_loss: 0.4470 - val_acc: 0.9128\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9568Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 299us/sample - loss: 0.5055 - acc: 0.9131\n",
      "\n",
      "Epoch 00094: val_acc improved from 0.91280 to 0.91310, saving model to C:\\Users\\19244\\saved_models\\ResNet_20_l2_model.h5\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.2813 - acc: 0.9568 - val_loss: 0.4483 - val_acc: 0.9131\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9580Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 340us/sample - loss: 0.4785 - acc: 0.9116\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91310\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.2767 - acc: 0.9580 - val_loss: 0.4497 - val_acc: 0.9116\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9607Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 387us/sample - loss: 0.5095 - acc: 0.9096\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91310\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 0.2690 - acc: 0.9607 - val_loss: 0.4567 - val_acc: 0.9096\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/100\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9595- ETA: 6s - loss: Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 323us/sample - loss: 0.4824 - acc: 0.9130\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91310\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.2697 - acc: 0.9595 - val_loss: 0.4435 - val_acc: 0.9130\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9611Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 275us/sample - loss: 0.5194 - acc: 0.9110\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91310\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.2632 - acc: 0.9611 - val_loss: 0.4451 - val_acc: 0.9110\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9624Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 4s 409us/sample - loss: 0.4696 - acc: 0.9098\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91310\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.2604 - acc: 0.9623 - val_loss: 0.4525 - val_acc: 0.9098\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/100\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9634Epoch 1/100\n",
      "10000/1563 [===============================================================================================================================================================================================] - 3s 253us/sample - loss: 0.4805 - acc: 0.9085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91310\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.2547 - acc: 0.9634 - val_loss: 0.4564 - val_acc: 0.9085\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "                              epochs = 100, \n",
    "                              validation_data=(X_test, Y_test),\n",
    "                              callbacks=[scheduler, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc1 = history1.history['acc']\n",
    "val_acc1 = history1.history['val_acc']\n",
    "\n",
    "fig1,ax1 = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "ax1.plot(acc1, label='Training Accuracy')\n",
    "ax1.plot(val_acc1, label='Validation Accuracy')\n",
    "\n",
    "ax1.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax1.set_ylabel(r'Acc', fontsize=20)\n",
    "ax1.set_title('ResNet20', fontsize=24)\n",
    "\n",
    "ax1.tick_params(labelsize=20)\n",
    "\n",
    "ax1.legend(loc=4, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
